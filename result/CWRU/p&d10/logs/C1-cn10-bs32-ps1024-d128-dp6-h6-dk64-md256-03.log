[2022-08-08 02:35:08,475] ## start time: 2022-08-08 02:35:08.335891
[2022-08-08 02:35:08,476] Using cuda device
[2022-08-08 02:35:08,477] In train:p&d10.npy.
[2022-08-08 02:35:08,478] One Channel
[2022-08-08 02:35:08,479] With Normal data.
[2022-08-08 02:35:08,479] Nunber of classes:10.
[2022-08-08 02:35:08,479] Nunber of ViT channels:1.
[2022-08-08 02:35:08,729] Totol epochs: 15
[2022-08-08 02:35:08,733] Epoch 1---------------
[2022-08-08 02:35:08,733] lr: 2.000000e-03
[2022-08-08 02:35:08,775] loss: 2.190241  [    0/ 4740]
[2022-08-08 02:35:09,323] loss: 1.752781  [  480/ 4740]
[2022-08-08 02:35:09,895] loss: 1.295585  [  960/ 4740]
[2022-08-08 02:35:10,428] loss: 1.162388  [ 1440/ 4740]
[2022-08-08 02:35:10,961] loss: 0.908294  [ 1920/ 4740]
[2022-08-08 02:35:11,500] loss: 0.302737  [ 2400/ 4740]
[2022-08-08 02:35:12,037] loss: 0.353899  [ 2880/ 4740]
[2022-08-08 02:35:12,564] loss: 0.125346  [ 3360/ 4740]
[2022-08-08 02:35:13,096] loss: 0.022310  [ 3840/ 4740]
[2022-08-08 02:35:13,625] loss: 0.036909  [ 4320/ 4740]
[2022-08-08 02:35:15,495] Train Error: Accuracy: 99.916%, Avg loss: 0.011173
[2022-08-08 02:35:16,091] Test  Error: Accuracy: 100.000%, Avg loss: 0.010033
[2022-08-08 02:35:16,092] Epoch 2---------------
[2022-08-08 02:35:16,093] lr: 1.900000e-03
[2022-08-08 02:35:16,132] loss: 0.012055  [    0/ 4740]
[2022-08-08 02:35:16,666] loss: 0.007247  [  480/ 4740]
[2022-08-08 02:35:17,218] loss: 0.004857  [  960/ 4740]
[2022-08-08 02:35:17,747] loss: 0.005618  [ 1440/ 4740]
[2022-08-08 02:35:18,277] loss: 0.002296  [ 1920/ 4740]
[2022-08-08 02:35:18,807] loss: 0.462597  [ 2400/ 4740]
[2022-08-08 02:35:19,340] loss: 0.187870  [ 2880/ 4740]
[2022-08-08 02:35:19,869] loss: 0.013764  [ 3360/ 4740]
[2022-08-08 02:35:20,397] loss: 0.073802  [ 3840/ 4740]
[2022-08-08 02:35:20,934] loss: 0.007555  [ 4320/ 4740]
[2022-08-08 02:35:22,799] Train Error: Accuracy: 99.895%, Avg loss: 0.008876
[2022-08-08 02:35:23,402] Test  Error: Accuracy: 99.951%, Avg loss: 0.008736
[2022-08-08 02:35:23,402] Epoch 3---------------
[2022-08-08 02:35:23,403] lr: 1.805000e-03
[2022-08-08 02:35:23,441] loss: 0.190623  [    0/ 4740]
[2022-08-08 02:35:23,974] loss: 0.005470  [  480/ 4740]
[2022-08-08 02:35:24,505] loss: 0.004164  [  960/ 4740]
[2022-08-08 02:35:25,045] loss: 0.002716  [ 1440/ 4740]
[2022-08-08 02:35:25,578] loss: 0.002380  [ 1920/ 4740]
[2022-08-08 02:35:26,112] loss: 0.002405  [ 2400/ 4740]
[2022-08-08 02:35:26,642] loss: 0.003583  [ 2880/ 4740]
[2022-08-08 02:35:27,175] loss: 0.002026  [ 3360/ 4740]
[2022-08-08 02:35:27,703] loss: 0.002046  [ 3840/ 4740]
[2022-08-08 02:35:28,238] loss: 0.001809  [ 4320/ 4740]
[2022-08-08 02:35:30,101] Train Error: Accuracy: 100.000%, Avg loss: 0.002990
[2022-08-08 02:35:30,695] Test  Error: Accuracy: 99.951%, Avg loss: 0.004458
[2022-08-08 02:35:30,695] Epoch 4---------------
[2022-08-08 02:35:30,696] lr: 1.714750e-03
[2022-08-08 02:35:30,733] loss: 0.001778  [    0/ 4740]
[2022-08-08 02:35:31,273] loss: 0.002067  [  480/ 4740]
[2022-08-08 02:35:31,805] loss: 0.001224  [  960/ 4740]
[2022-08-08 02:35:32,336] loss: 0.001441  [ 1440/ 4740]
[2022-08-08 02:35:32,867] loss: 0.001838  [ 1920/ 4740]
[2022-08-08 02:35:33,402] loss: 0.001032  [ 2400/ 4740]
[2022-08-08 02:35:33,934] loss: 0.001687  [ 2880/ 4740]
[2022-08-08 02:35:34,466] loss: 0.000915  [ 3360/ 4740]
[2022-08-08 02:35:35,000] loss: 0.000902  [ 3840/ 4740]
[2022-08-08 02:35:35,534] loss: 0.001208  [ 4320/ 4740]
[2022-08-08 02:35:37,374] Train Error: Accuracy: 100.000%, Avg loss: 0.001102
[2022-08-08 02:35:37,986] Test  Error: Accuracy: 100.000%, Avg loss: 0.001239
[2022-08-08 02:35:37,986] Epoch 5---------------
[2022-08-08 02:35:37,987] lr: 1.629012e-03
[2022-08-08 02:35:38,026] loss: 0.001266  [    0/ 4740]
[2022-08-08 02:35:38,562] loss: 0.000793  [  480/ 4740]
[2022-08-08 02:35:39,098] loss: 0.000794  [  960/ 4740]
[2022-08-08 02:35:39,632] loss: 0.001132  [ 1440/ 4740]
[2022-08-08 02:35:40,158] loss: 0.000791  [ 1920/ 4740]
[2022-08-08 02:35:40,688] loss: 0.000691  [ 2400/ 4740]
[2022-08-08 02:35:41,217] loss: 0.000783  [ 2880/ 4740]
[2022-08-08 02:35:41,745] loss: 0.000786  [ 3360/ 4740]
[2022-08-08 02:35:42,277] loss: 0.000903  [ 3840/ 4740]
[2022-08-08 02:35:42,806] loss: 0.000797  [ 4320/ 4740]
[2022-08-08 02:35:44,649] Train Error: Accuracy: 100.000%, Avg loss: 0.000817
[2022-08-08 02:35:45,249] Test  Error: Accuracy: 100.000%, Avg loss: 0.000983
[2022-08-08 02:35:45,249] Epoch 6---------------
[2022-08-08 02:35:45,250] lr: 1.547562e-03
[2022-08-08 02:35:45,288] loss: 0.000568  [    0/ 4740]
[2022-08-08 02:35:45,821] loss: 0.000674  [  480/ 4740]
[2022-08-08 02:35:46,358] loss: 0.000638  [  960/ 4740]
[2022-08-08 02:35:46,893] loss: 0.000702  [ 1440/ 4740]
[2022-08-08 02:35:47,431] loss: 0.000680  [ 1920/ 4740]
[2022-08-08 02:35:47,960] loss: 0.000884  [ 2400/ 4740]
[2022-08-08 02:35:48,497] loss: 0.000651  [ 2880/ 4740]
[2022-08-08 02:35:49,031] loss: 0.000880  [ 3360/ 4740]
[2022-08-08 02:35:49,570] loss: 0.000644  [ 3840/ 4740]
[2022-08-08 02:35:50,101] loss: 0.000566  [ 4320/ 4740]
[2022-08-08 02:35:51,942] Train Error: Accuracy: 100.000%, Avg loss: 0.000691
[2022-08-08 02:35:52,540] Test  Error: Accuracy: 100.000%, Avg loss: 0.000770
[2022-08-08 02:35:52,540] Epoch 7---------------
[2022-08-08 02:35:52,542] lr: 1.470184e-03
[2022-08-08 02:35:52,578] loss: 0.000681  [    0/ 4740]
[2022-08-08 02:35:53,108] loss: 0.000656  [  480/ 4740]
[2022-08-08 02:35:53,644] loss: 0.001002  [  960/ 4740]
[2022-08-08 02:35:54,177] loss: 0.000621  [ 1440/ 4740]
[2022-08-08 02:35:54,715] loss: 0.000490  [ 1920/ 4740]
[2022-08-08 02:35:55,254] loss: 0.000678  [ 2400/ 4740]
[2022-08-08 02:35:55,783] loss: 0.000522  [ 2880/ 4740]
[2022-08-08 02:35:56,309] loss: 0.000524  [ 3360/ 4740]
[2022-08-08 02:35:56,844] loss: 0.000574  [ 3840/ 4740]
[2022-08-08 02:35:57,377] loss: 0.000485  [ 4320/ 4740]
[2022-08-08 02:35:59,236] Train Error: Accuracy: 100.000%, Avg loss: 0.000598
[2022-08-08 02:35:59,831] Test  Error: Accuracy: 100.000%, Avg loss: 0.000786
[2022-08-08 02:35:59,831] Epoch 8---------------
[2022-08-08 02:35:59,833] lr: 1.260499e-03
[2022-08-08 02:35:59,872] loss: 0.000551  [    0/ 4740]
[2022-08-08 02:36:00,404] loss: 0.000502  [  480/ 4740]
[2022-08-08 02:36:00,938] loss: 0.000587  [  960/ 4740]
[2022-08-08 02:36:01,473] loss: 0.000469  [ 1440/ 4740]
[2022-08-08 02:36:02,005] loss: 0.000538  [ 1920/ 4740]
[2022-08-08 02:36:02,542] loss: 0.000565  [ 2400/ 4740]
[2022-08-08 02:36:03,075] loss: 0.000480  [ 2880/ 4740]
[2022-08-08 02:36:03,609] loss: 0.000438  [ 3360/ 4740]
[2022-08-08 02:36:04,143] loss: 0.000456  [ 3840/ 4740]
[2022-08-08 02:36:04,675] loss: 0.000541  [ 4320/ 4740]
[2022-08-08 02:36:06,517] Train Error: Accuracy: 100.000%, Avg loss: 0.000796
[2022-08-08 02:36:07,113] Test  Error: Accuracy: 100.000%, Avg loss: 0.000759
[2022-08-08 02:36:07,114] Epoch 9---------------
[2022-08-08 02:36:07,115] lr: 1.197474e-03
[2022-08-08 02:36:07,152] loss: 0.000536  [    0/ 4740]
[2022-08-08 02:36:07,687] loss: 0.000450  [  480/ 4740]
[2022-08-08 02:36:08,223] loss: 0.000476  [  960/ 4740]
[2022-08-08 02:36:08,756] loss: 0.000457  [ 1440/ 4740]
[2022-08-08 02:36:09,287] loss: 0.000490  [ 1920/ 4740]
[2022-08-08 02:36:09,819] loss: 0.000458  [ 2400/ 4740]
[2022-08-08 02:36:10,347] loss: 0.000372  [ 2880/ 4740]
[2022-08-08 02:36:10,876] loss: 0.000474  [ 3360/ 4740]
[2022-08-08 02:36:11,410] loss: 0.000451  [ 3840/ 4740]
[2022-08-08 02:36:11,941] loss: 0.000474  [ 4320/ 4740]
[2022-08-08 02:36:13,778] Train Error: Accuracy: 100.000%, Avg loss: 0.000509
[2022-08-08 02:36:14,370] Test  Error: Accuracy: 100.000%, Avg loss: 0.000647
[2022-08-08 02:36:14,370] Epoch 10---------------
[2022-08-08 02:36:14,371] lr: 1.137600e-03
[2022-08-08 02:36:14,409] loss: 0.000615  [    0/ 4740]
[2022-08-08 02:36:14,947] loss: 0.000333  [  480/ 4740]
[2022-08-08 02:36:15,491] loss: 0.000403  [  960/ 4740]
[2022-08-08 02:36:16,023] loss: 0.000502  [ 1440/ 4740]
[2022-08-08 02:36:16,555] loss: 0.000343  [ 1920/ 4740]
[2022-08-08 02:36:17,088] loss: 0.000439  [ 2400/ 4740]
[2022-08-08 02:36:17,623] loss: 0.000690  [ 2880/ 4740]
[2022-08-08 02:36:18,151] loss: 0.000436  [ 3360/ 4740]
[2022-08-08 02:36:18,684] loss: 0.000328  [ 3840/ 4740]
[2022-08-08 02:36:19,224] loss: 0.000699  [ 4320/ 4740]
[2022-08-08 02:36:21,081] Train Error: Accuracy: 100.000%, Avg loss: 0.000456
[2022-08-08 02:36:21,684] Test  Error: Accuracy: 100.000%, Avg loss: 0.000576
[2022-08-08 02:36:21,684] Epoch 11---------------
[2022-08-08 02:36:21,685] lr: 1.080720e-03
[2022-08-08 02:36:21,723] loss: 0.000566  [    0/ 4740]
[2022-08-08 02:36:22,255] loss: 0.000392  [  480/ 4740]
[2022-08-08 02:36:22,784] loss: 0.000509  [  960/ 4740]
[2022-08-08 02:36:23,316] loss: 0.000351  [ 1440/ 4740]
[2022-08-08 02:36:23,847] loss: 0.000561  [ 1920/ 4740]
[2022-08-08 02:36:24,384] loss: 0.000401  [ 2400/ 4740]
[2022-08-08 02:36:24,916] loss: 0.000479  [ 2880/ 4740]
[2022-08-08 02:36:25,449] loss: 0.000403  [ 3360/ 4740]
[2022-08-08 02:36:25,975] loss: 0.000469  [ 3840/ 4740]
[2022-08-08 02:36:26,508] loss: 0.000532  [ 4320/ 4740]
[2022-08-08 02:36:28,361] Train Error: Accuracy: 100.000%, Avg loss: 0.000435
[2022-08-08 02:36:28,958] Test  Error: Accuracy: 100.000%, Avg loss: 0.000652
[2022-08-08 02:36:28,959] Epoch 12---------------
[2022-08-08 02:36:28,960] lr: 8.362407e-04
[2022-08-08 02:36:28,997] loss: 0.000508  [    0/ 4740]
[2022-08-08 02:36:29,538] loss: 0.000547  [  480/ 4740]
[2022-08-08 02:36:30,069] loss: 0.000370  [  960/ 4740]
[2022-08-08 02:36:30,602] loss: 0.000396  [ 1440/ 4740]
[2022-08-08 02:36:31,130] loss: 0.000291  [ 1920/ 4740]
[2022-08-08 02:36:31,660] loss: 0.000339  [ 2400/ 4740]
[2022-08-08 02:36:32,189] loss: 0.000378  [ 2880/ 4740]
[2022-08-08 02:36:32,715] loss: 0.000632  [ 3360/ 4740]
[2022-08-08 02:36:33,244] loss: 0.000565  [ 3840/ 4740]
[2022-08-08 02:36:33,781] loss: 0.000380  [ 4320/ 4740]
[2022-08-08 02:36:35,621] Train Error: Accuracy: 100.000%, Avg loss: 0.000405
[2022-08-08 02:36:36,215] Test  Error: Accuracy: 100.000%, Avg loss: 0.000730
[2022-08-08 02:36:36,216] Epoch 13---------------
[2022-08-08 02:36:36,217] lr: 6.470671e-04
[2022-08-08 02:36:36,255] loss: 0.000394  [    0/ 4740]
[2022-08-08 02:36:36,787] loss: 0.000420  [  480/ 4740]
[2022-08-08 02:36:37,324] loss: 0.000315  [  960/ 4740]
[2022-08-08 02:36:37,861] loss: 0.000692  [ 1440/ 4740]
[2022-08-08 02:36:38,395] loss: 0.000258  [ 1920/ 4740]
[2022-08-08 02:36:38,926] loss: 0.000345  [ 2400/ 4740]
[2022-08-08 02:36:39,467] loss: 0.000362  [ 2880/ 4740]
[2022-08-08 02:36:39,996] loss: 0.000355  [ 3360/ 4740]
[2022-08-08 02:36:40,526] loss: 0.000334  [ 3840/ 4740]
[2022-08-08 02:36:41,054] loss: 0.000609  [ 4320/ 4740]
[2022-08-08 02:36:42,897] Train Error: Accuracy: 100.000%, Avg loss: 0.000395
[2022-08-08 02:36:43,489] Test  Error: Accuracy: 100.000%, Avg loss: 0.000548
[2022-08-08 02:36:43,490] Epoch 14---------------
[2022-08-08 02:36:43,490] lr: 6.147137e-04
[2022-08-08 02:36:43,528] loss: 0.000393  [    0/ 4740]
[2022-08-08 02:36:44,057] loss: 0.000268  [  480/ 4740]
[2022-08-08 02:36:44,589] loss: 0.000507  [  960/ 4740]
[2022-08-08 02:36:45,122] loss: 0.000347  [ 1440/ 4740]
[2022-08-08 02:36:45,657] loss: 0.000458  [ 1920/ 4740]
[2022-08-08 02:36:46,185] loss: 0.000376  [ 2400/ 4740]
[2022-08-08 02:36:46,714] loss: 0.000294  [ 2880/ 4740]
[2022-08-08 02:36:47,242] loss: 0.000420  [ 3360/ 4740]
[2022-08-08 02:36:47,774] loss: 0.000316  [ 3840/ 4740]
[2022-08-08 02:36:48,303] loss: 0.000378  [ 4320/ 4740]
[2022-08-08 02:36:50,139] Train Error: Accuracy: 100.000%, Avg loss: 0.000386
[2022-08-08 02:36:50,746] Test  Error: Accuracy: 100.000%, Avg loss: 0.000732
[2022-08-08 02:36:50,746] Epoch 15---------------
[2022-08-08 02:36:50,747] lr: 4.292775e-04
[2022-08-08 02:36:50,785] loss: 0.000333  [    0/ 4740]
[2022-08-08 02:36:51,322] loss: 0.000388  [  480/ 4740]
[2022-08-08 02:36:51,860] loss: 0.000335  [  960/ 4740]
[2022-08-08 02:36:52,396] loss: 0.000380  [ 1440/ 4740]
[2022-08-08 02:36:52,926] loss: 0.000346  [ 1920/ 4740]
[2022-08-08 02:36:53,455] loss: 0.000296  [ 2400/ 4740]
[2022-08-08 02:36:53,986] loss: 0.000468  [ 2880/ 4740]
[2022-08-08 02:36:54,517] loss: 0.000288  [ 3360/ 4740]
[2022-08-08 02:36:55,048] loss: 0.000945  [ 3840/ 4740]
[2022-08-08 02:36:55,585] loss: 0.000468  [ 4320/ 4740]
[2022-08-08 02:36:57,427] Train Error: Accuracy: 100.000%, Avg loss: 0.000365
[2022-08-08 02:36:58,020] Test  Error: Accuracy: 100.000%, Avg loss: 0.000524
[2022-08-08 02:36:58,020] Done!
[2022-08-08 02:36:58,024] Number of parameters:3424522
[2022-08-08 02:36:58,025] ## end time: 2022-08-08 02:36:58.020280
[2022-08-08 02:36:58,025] ## used time: 0:01:49.684389
