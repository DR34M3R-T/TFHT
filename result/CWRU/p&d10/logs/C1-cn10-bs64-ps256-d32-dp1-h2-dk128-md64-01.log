[2022-08-06 15:22:23,698] ## start time: 2022-08-06 15:22:23.576511
[2022-08-06 15:22:23,698] Using cuda device
[2022-08-06 15:22:23,701] In train:p&d10.npy.
[2022-08-06 15:22:23,701] One Channel
[2022-08-06 15:22:23,701] With Normal data.
[2022-08-06 15:22:23,702] Nunber of classes:10.
[2022-08-06 15:22:23,702] Nunber of ViT channels:1.
[2022-08-06 15:22:23,891] Totol epochs: 10
[2022-08-06 15:22:23,892] Epoch 1---------------
[2022-08-06 15:22:23,893] lr: 2.000000e-03
[2022-08-06 15:22:23,906] loss: 2.287033  [    0/ 4783]
[2022-08-06 15:22:24,066] loss: 1.809059  [  960/ 4783]
[2022-08-06 15:22:24,226] loss: 1.282065  [ 1920/ 4783]
[2022-08-06 15:22:24,382] loss: 1.036345  [ 2880/ 4783]
[2022-08-06 15:22:24,533] loss: 0.658716  [ 3840/ 4783]
[2022-08-06 15:22:24,998] Train Error: Accuracy: 84.717%, Avg loss: 0.508313
[2022-08-06 15:22:25,128] Test  Error: Accuracy: 84.400%, Avg loss: 0.517423
[2022-08-06 15:22:25,129] Epoch 2---------------
[2022-08-06 15:22:25,130] lr: 1.900000e-03
[2022-08-06 15:22:25,142] loss: 0.495249  [    0/ 4783]
[2022-08-06 15:22:25,301] loss: 0.251575  [  960/ 4783]
[2022-08-06 15:22:25,458] loss: 0.167058  [ 1920/ 4783]
[2022-08-06 15:22:25,611] loss: 0.202824  [ 2880/ 4783]
[2022-08-06 15:22:25,762] loss: 0.194586  [ 3840/ 4783]
[2022-08-06 15:22:26,185] Train Error: Accuracy: 98.934%, Avg loss: 0.089350
[2022-08-06 15:22:26,303] Test  Error: Accuracy: 98.550%, Avg loss: 0.097151
[2022-08-06 15:22:26,303] Epoch 3---------------
[2022-08-06 15:22:26,304] lr: 1.805000e-03
[2022-08-06 15:22:26,316] loss: 0.090860  [    0/ 4783]
[2022-08-06 15:22:26,465] loss: 0.098818  [  960/ 4783]
[2022-08-06 15:22:26,616] loss: 0.087842  [ 1920/ 4783]
[2022-08-06 15:22:26,766] loss: 0.175006  [ 2880/ 4783]
[2022-08-06 15:22:26,917] loss: 0.037542  [ 3840/ 4783]
[2022-08-06 15:22:27,337] Train Error: Accuracy: 99.289%, Avg loss: 0.042516
[2022-08-06 15:22:27,454] Test  Error: Accuracy: 99.500%, Avg loss: 0.041765
[2022-08-06 15:22:27,454] Epoch 4---------------
[2022-08-06 15:22:27,455] lr: 1.714750e-03
[2022-08-06 15:22:27,468] loss: 0.024887  [    0/ 4783]
[2022-08-06 15:22:27,620] loss: 0.035975  [  960/ 4783]
[2022-08-06 15:22:27,770] loss: 0.039531  [ 1920/ 4783]
[2022-08-06 15:22:27,917] loss: 0.016904  [ 2880/ 4783]
[2022-08-06 15:22:28,066] loss: 0.043356  [ 3840/ 4783]
[2022-08-06 15:22:28,490] Train Error: Accuracy: 99.289%, Avg loss: 0.037975
[2022-08-06 15:22:28,610] Test  Error: Accuracy: 99.250%, Avg loss: 0.042924
[2022-08-06 15:22:28,610] Epoch 5---------------
[2022-08-06 15:22:28,611] lr: 1.470184e-03
[2022-08-06 15:22:28,622] loss: 0.024485  [    0/ 4783]
[2022-08-06 15:22:28,772] loss: 0.018767  [  960/ 4783]
[2022-08-06 15:22:28,920] loss: 0.018221  [ 1920/ 4783]
[2022-08-06 15:22:29,069] loss: 0.012835  [ 2880/ 4783]
[2022-08-06 15:22:29,218] loss: 0.011347  [ 3840/ 4783]
[2022-08-06 15:22:29,637] Train Error: Accuracy: 97.972%, Avg loss: 0.065145
[2022-08-06 15:22:29,756] Test  Error: Accuracy: 98.100%, Avg loss: 0.063792
[2022-08-06 15:22:29,756] Epoch 6---------------
[2022-08-06 15:22:29,757] lr: 1.026684e-03
[2022-08-06 15:22:29,768] loss: 0.068186  [    0/ 4783]
[2022-08-06 15:22:29,919] loss: 0.116314  [  960/ 4783]
[2022-08-06 15:22:30,069] loss: 0.007884  [ 1920/ 4783]
[2022-08-06 15:22:30,218] loss: 0.013994  [ 2880/ 4783]
[2022-08-06 15:22:30,377] loss: 0.006272  [ 3840/ 4783]
[2022-08-06 15:22:30,799] Train Error: Accuracy: 99.875%, Avg loss: 0.013927
[2022-08-06 15:22:30,917] Test  Error: Accuracy: 99.750%, Avg loss: 0.016021
[2022-08-06 15:22:30,918] Epoch 7---------------
[2022-08-06 15:22:30,919] lr: 9.753500e-04
[2022-08-06 15:22:30,931] loss: 0.021832  [    0/ 4783]
[2022-08-06 15:22:31,080] loss: 0.010554  [  960/ 4783]
[2022-08-06 15:22:31,229] loss: 0.007033  [ 1920/ 4783]
[2022-08-06 15:22:31,378] loss: 0.004531  [ 2880/ 4783]
[2022-08-06 15:22:31,529] loss: 0.013872  [ 3840/ 4783]
[2022-08-06 15:22:31,948] Train Error: Accuracy: 99.854%, Avg loss: 0.010673
[2022-08-06 15:22:32,069] Test  Error: Accuracy: 99.700%, Avg loss: 0.015236
[2022-08-06 15:22:32,070] Epoch 8---------------
[2022-08-06 15:22:32,071] lr: 9.265825e-04
[2022-08-06 15:22:32,082] loss: 0.007640  [    0/ 4783]
[2022-08-06 15:22:32,232] loss: 0.005288  [  960/ 4783]
[2022-08-06 15:22:32,382] loss: 0.005380  [ 1920/ 4783]
[2022-08-06 15:22:32,533] loss: 0.004722  [ 2880/ 4783]
[2022-08-06 15:22:32,681] loss: 0.005243  [ 3840/ 4783]
[2022-08-06 15:22:33,099] Train Error: Accuracy: 99.958%, Avg loss: 0.008188
[2022-08-06 15:22:33,217] Test  Error: Accuracy: 99.650%, Avg loss: 0.016484
[2022-08-06 15:22:33,217] Epoch 9---------------
[2022-08-06 15:22:33,218] lr: 7.944286e-04
[2022-08-06 15:22:33,231] loss: 0.046432  [    0/ 4783]
[2022-08-06 15:22:33,385] loss: 0.006575  [  960/ 4783]
[2022-08-06 15:22:33,542] loss: 0.004597  [ 1920/ 4783]
[2022-08-06 15:22:33,701] loss: 0.003846  [ 2880/ 4783]
[2022-08-06 15:22:33,850] loss: 0.004075  [ 3840/ 4783]
[2022-08-06 15:22:34,268] Train Error: Accuracy: 99.916%, Avg loss: 0.007346
[2022-08-06 15:22:34,385] Test  Error: Accuracy: 99.900%, Avg loss: 0.010122
[2022-08-06 15:22:34,385] Epoch 10---------------
[2022-08-06 15:22:34,386] lr: 7.547072e-04
[2022-08-06 15:22:34,398] loss: 0.003672  [    0/ 4783]
[2022-08-06 15:22:34,546] loss: 0.006437  [  960/ 4783]
[2022-08-06 15:22:34,698] loss: 0.004994  [ 1920/ 4783]
[2022-08-06 15:22:34,848] loss: 0.003600  [ 2880/ 4783]
[2022-08-06 15:22:34,995] loss: 0.004277  [ 3840/ 4783]
[2022-08-06 15:22:35,415] Train Error: Accuracy: 99.958%, Avg loss: 0.005825
[2022-08-06 15:22:35,536] Test  Error: Accuracy: 99.850%, Avg loss: 0.009868
[2022-08-06 15:22:35,536] Done!
[2022-08-06 15:22:35,538] Number of parameters:92106
[2022-08-06 15:22:35,538] ## end time: 2022-08-06 15:22:35.536975
[2022-08-06 15:22:35,538] ## used time: 0:00:11.960464
