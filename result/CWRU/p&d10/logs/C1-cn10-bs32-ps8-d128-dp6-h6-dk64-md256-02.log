[2022-08-07 18:50:07,939] ## start time: 2022-08-07 18:50:07.798758
[2022-08-07 18:50:07,939] Using cuda device
[2022-08-07 18:50:07,940] In train:p&d10.npy.
[2022-08-07 18:50:07,941] One Channel
[2022-08-07 18:50:07,942] With Normal data.
[2022-08-07 18:50:07,942] Nunber of classes:10.
[2022-08-07 18:50:07,943] Nunber of ViT channels:1.
[2022-08-07 18:50:08,212] Totol epochs: 15
[2022-08-07 18:50:08,216] Epoch 1---------------
[2022-08-07 18:50:08,216] lr: 2.000000e-03
[2022-08-07 18:50:10,047] loss: 2.461557  [    0/ 4683]
[2022-08-07 18:50:37,480] loss: 1.955569  [  480/ 4683]
[2022-08-07 18:51:04,914] loss: 1.903053  [  960/ 4683]
[2022-08-07 18:51:32,347] loss: 1.628707  [ 1440/ 4683]
[2022-08-07 18:51:59,779] loss: 1.206794  [ 1920/ 4683]
[2022-08-07 18:52:27,211] loss: 1.156089  [ 2400/ 4683]
[2022-08-07 18:52:54,643] loss: 0.960239  [ 2880/ 4683]
[2022-08-07 18:53:22,075] loss: 0.912061  [ 3360/ 4683]
[2022-08-07 18:53:49,508] loss: 0.515492  [ 3840/ 4683]
[2022-08-07 18:54:16,940] loss: 0.639471  [ 4320/ 4683]
[2022-08-07 18:56:14,110] Train Error: Accuracy: 74.995%, Avg loss: 0.708468
[2022-08-07 18:56:58,169] Test  Error: Accuracy: 74.857%, Avg loss: 0.718197
[2022-08-07 18:56:58,170] Epoch 2---------------
[2022-08-07 18:56:58,170] lr: 1.900000e-03
[2022-08-07 18:57:00,001] loss: 0.641532  [    0/ 4683]
[2022-08-07 18:57:27,432] loss: 0.314263  [  480/ 4683]
[2022-08-07 18:57:54,862] loss: 0.557528  [  960/ 4683]
[2022-08-07 18:58:22,292] loss: 0.195468  [ 1440/ 4683]
[2022-08-07 18:58:49,723] loss: 0.190394  [ 1920/ 4683]
[2022-08-07 18:59:17,154] loss: 0.253878  [ 2400/ 4683]
[2022-08-07 18:59:44,585] loss: 0.233085  [ 2880/ 4683]
[2022-08-07 19:00:12,015] loss: 0.148254  [ 3360/ 4683]
[2022-08-07 19:00:39,446] loss: 0.165068  [ 3840/ 4683]
[2022-08-07 19:01:06,877] loss: 0.249748  [ 4320/ 4683]
[2022-08-07 19:03:04,064] Train Error: Accuracy: 92.633%, Avg loss: 0.219442
[2022-08-07 19:03:48,127] Test  Error: Accuracy: 92.333%, Avg loss: 0.231430
[2022-08-07 19:03:48,128] Epoch 3---------------
[2022-08-07 19:03:48,128] lr: 1.805000e-03
[2022-08-07 19:03:49,958] loss: 0.249439  [    0/ 4683]
[2022-08-07 19:04:17,387] loss: 0.041163  [  480/ 4683]
[2022-08-07 19:04:44,817] loss: 0.427499  [  960/ 4683]
[2022-08-07 19:05:12,249] loss: 0.175949  [ 1440/ 4683]
[2022-08-07 19:05:39,680] loss: 0.488378  [ 1920/ 4683]
[2022-08-07 19:06:07,109] loss: 0.122741  [ 2400/ 4683]
[2022-08-07 19:06:34,540] loss: 0.235913  [ 2880/ 4683]
[2022-08-07 19:07:01,969] loss: 0.142673  [ 3360/ 4683]
[2022-08-07 19:07:29,399] loss: 0.338095  [ 3840/ 4683]
[2022-08-07 19:07:56,829] loss: 0.094340  [ 4320/ 4683]
[2022-08-07 19:09:54,018] Train Error: Accuracy: 86.504%, Avg loss: 0.512517
[2022-08-07 19:10:38,086] Test  Error: Accuracy: 85.667%, Avg loss: 0.507970
[2022-08-07 19:10:38,087] Epoch 4---------------
[2022-08-07 19:10:38,087] lr: 1.260499e-03
[2022-08-07 19:10:39,917] loss: 0.596322  [    0/ 4683]
[2022-08-07 19:11:07,349] loss: 0.206418  [  480/ 4683]
[2022-08-07 19:11:34,780] loss: 0.178158  [  960/ 4683]
[2022-08-07 19:12:02,211] loss: 0.085687  [ 1440/ 4683]
[2022-08-07 19:12:29,644] loss: 0.071001  [ 1920/ 4683]
[2022-08-07 19:12:57,074] loss: 0.222478  [ 2400/ 4683]
[2022-08-07 19:13:24,501] loss: 0.027778  [ 2880/ 4683]
[2022-08-07 19:13:51,924] loss: 0.008243  [ 3360/ 4683]
[2022-08-07 19:14:19,348] loss: 0.071266  [ 3840/ 4683]
[2022-08-07 19:14:46,774] loss: 0.144329  [ 4320/ 4683]
[2022-08-07 19:16:43,948] Train Error: Accuracy: 97.245%, Avg loss: 0.085354
[2022-08-07 19:17:28,007] Test  Error: Accuracy: 96.762%, Avg loss: 0.107885
[2022-08-07 19:17:28,008] Epoch 5---------------
[2022-08-07 19:17:28,008] lr: 1.197474e-03
[2022-08-07 19:17:29,839] loss: 0.157932  [    0/ 4683]
[2022-08-07 19:17:57,267] loss: 0.045841  [  480/ 4683]
[2022-08-07 19:18:24,695] loss: 0.110411  [  960/ 4683]
[2022-08-07 19:18:52,119] loss: 0.023762  [ 1440/ 4683]
[2022-08-07 19:19:19,546] loss: 0.010323  [ 1920/ 4683]
[2022-08-07 19:19:46,973] loss: 0.090141  [ 2400/ 4683]
[2022-08-07 19:20:14,398] loss: 0.179682  [ 2880/ 4683]
[2022-08-07 19:20:41,826] loss: 0.210796  [ 3360/ 4683]
[2022-08-07 19:21:09,254] loss: 0.148056  [ 3840/ 4683]
[2022-08-07 19:21:36,681] loss: 0.012247  [ 4320/ 4683]
[2022-08-07 19:23:33,859] Train Error: Accuracy: 95.003%, Avg loss: 0.147467
[2022-08-07 19:24:17,922] Test  Error: Accuracy: 94.524%, Avg loss: 0.146373
[2022-08-07 19:24:17,922] Epoch 6---------------
[2022-08-07 19:24:17,923] lr: 8.362407e-04
[2022-08-07 19:24:19,753] loss: 0.009786  [    0/ 4683]
[2022-08-07 19:24:47,176] loss: 0.005696  [  480/ 4683]
[2022-08-07 19:25:14,600] loss: 0.029371  [  960/ 4683]
[2022-08-07 19:25:42,023] loss: 0.009912  [ 1440/ 4683]
[2022-08-07 19:26:09,450] loss: 0.026279  [ 1920/ 4683]
[2022-08-07 19:26:36,876] loss: 0.052830  [ 2400/ 4683]
[2022-08-07 19:27:04,302] loss: 0.168370  [ 2880/ 4683]
[2022-08-07 19:27:31,727] loss: 0.026170  [ 3360/ 4683]
[2022-08-07 19:27:59,152] loss: 0.139336  [ 3840/ 4683]
[2022-08-07 19:28:26,577] loss: 0.105749  [ 4320/ 4683]
[2022-08-07 19:30:23,752] Train Error: Accuracy: 97.651%, Avg loss: 0.081418
[2022-08-07 19:31:07,817] Test  Error: Accuracy: 96.571%, Avg loss: 0.116519
[2022-08-07 19:31:07,818] Epoch 7---------------
[2022-08-07 19:31:07,819] lr: 7.944286e-04
[2022-08-07 19:31:09,648] loss: 0.039687  [    0/ 4683]
[2022-08-07 19:31:37,073] loss: 0.074737  [  480/ 4683]
[2022-08-07 19:32:04,499] loss: 0.007010  [  960/ 4683]
[2022-08-07 19:32:31,924] loss: 0.032496  [ 1440/ 4683]
[2022-08-07 19:32:59,350] loss: 0.024184  [ 1920/ 4683]
[2022-08-07 19:33:26,774] loss: 0.049267  [ 2400/ 4683]
[2022-08-07 19:33:54,199] loss: 0.010422  [ 2880/ 4683]
[2022-08-07 19:34:21,624] loss: 0.008815  [ 3360/ 4683]
[2022-08-07 19:34:49,048] loss: 0.049102  [ 3840/ 4683]
[2022-08-07 19:35:16,473] loss: 0.016599  [ 4320/ 4683]
[2022-08-07 19:37:13,654] Train Error: Accuracy: 99.103%, Avg loss: 0.030472
[2022-08-07 19:37:57,717] Test  Error: Accuracy: 98.571%, Avg loss: 0.048882
[2022-08-07 19:37:57,718] Epoch 8---------------
[2022-08-07 19:37:57,718] lr: 7.547072e-04
[2022-08-07 19:37:59,548] loss: 0.020801  [    0/ 4683]
[2022-08-07 19:38:26,972] loss: 0.009831  [  480/ 4683]
[2022-08-07 19:38:54,398] loss: 0.008149  [  960/ 4683]
[2022-08-07 19:39:21,820] loss: 0.009251  [ 1440/ 4683]
[2022-08-07 19:39:49,244] loss: 0.004725  [ 1920/ 4683]
[2022-08-07 19:40:16,670] loss: 0.008544  [ 2400/ 4683]
[2022-08-07 19:40:44,094] loss: 0.009556  [ 2880/ 4683]
[2022-08-07 19:41:11,520] loss: 0.011106  [ 3360/ 4683]
[2022-08-07 19:41:38,947] loss: 0.008731  [ 3840/ 4683]
[2022-08-07 19:42:06,373] loss: 0.023128  [ 4320/ 4683]
[2022-08-07 19:44:03,548] Train Error: Accuracy: 98.783%, Avg loss: 0.040106
[2022-08-07 19:44:47,612] Test  Error: Accuracy: 98.429%, Avg loss: 0.055684
[2022-08-07 19:44:47,613] Epoch 9---------------
[2022-08-07 19:44:47,613] lr: 5.839780e-04
[2022-08-07 19:44:49,442] loss: 0.005868  [    0/ 4683]
[2022-08-07 19:45:16,870] loss: 0.012147  [  480/ 4683]
[2022-08-07 19:45:44,297] loss: 0.011763  [  960/ 4683]
[2022-08-07 19:46:11,723] loss: 0.018307  [ 1440/ 4683]
[2022-08-07 19:46:39,148] loss: 0.078811  [ 1920/ 4683]
[2022-08-07 19:47:06,575] loss: 0.010829  [ 2400/ 4683]
[2022-08-07 19:47:34,000] loss: 0.005496  [ 2880/ 4683]
[2022-08-07 19:48:01,428] loss: 0.016242  [ 3360/ 4683]
[2022-08-07 19:48:28,854] loss: 0.045533  [ 3840/ 4683]
[2022-08-07 19:48:56,281] loss: 0.007299  [ 4320/ 4683]
[2022-08-07 19:50:53,456] Train Error: Accuracy: 99.744%, Avg loss: 0.012846
[2022-08-07 19:51:37,520] Test  Error: Accuracy: 99.333%, Avg loss: 0.028607
[2022-08-07 19:51:37,521] Epoch 10---------------
[2022-08-07 19:51:37,521] lr: 5.547791e-04
[2022-08-07 19:51:39,351] loss: 0.004679  [    0/ 4683]
[2022-08-07 19:52:06,776] loss: 0.045152  [  480/ 4683]
[2022-08-07 19:52:34,200] loss: 0.011586  [  960/ 4683]
[2022-08-07 19:53:01,627] loss: 0.001990  [ 1440/ 4683]
[2022-08-07 19:53:29,053] loss: 0.001769  [ 1920/ 4683]
[2022-08-07 19:53:56,479] loss: 0.002311  [ 2400/ 4683]
[2022-08-07 19:54:23,902] loss: 0.000620  [ 2880/ 4683]
[2022-08-07 19:54:51,329] loss: 0.111418  [ 3360/ 4683]
[2022-08-07 19:55:18,755] loss: 0.005094  [ 3840/ 4683]
[2022-08-07 19:55:46,181] loss: 0.145602  [ 4320/ 4683]
[2022-08-07 19:57:43,365] Train Error: Accuracy: 99.466%, Avg loss: 0.019189
[2022-08-07 19:58:27,430] Test  Error: Accuracy: 99.476%, Avg loss: 0.025034
[2022-08-07 19:58:27,431] Epoch 11---------------
[2022-08-07 19:58:27,431] lr: 5.270402e-04
[2022-08-07 19:58:29,262] loss: 0.010978  [    0/ 4683]
[2022-08-07 19:58:56,687] loss: 0.038077  [  480/ 4683]
[2022-08-07 19:59:24,110] loss: 0.011978  [  960/ 4683]
[2022-08-07 19:59:51,535] loss: 0.006033  [ 1440/ 4683]
[2022-08-07 20:00:18,960] loss: 0.006371  [ 1920/ 4683]
[2022-08-07 20:00:46,386] loss: 0.008779  [ 2400/ 4683]
[2022-08-07 20:01:13,810] loss: 0.028093  [ 2880/ 4683]
[2022-08-07 20:01:41,236] loss: 0.003512  [ 3360/ 4683]
[2022-08-07 20:02:08,662] loss: 0.165306  [ 3840/ 4683]
[2022-08-07 20:02:36,087] loss: 0.005517  [ 4320/ 4683]
[2022-08-07 20:04:33,276] Train Error: Accuracy: 99.509%, Avg loss: 0.019759
[2022-08-07 20:05:17,341] Test  Error: Accuracy: 98.571%, Avg loss: 0.046201
[2022-08-07 20:05:17,341] Epoch 12---------------
[2022-08-07 20:05:17,341] lr: 3.680518e-04
[2022-08-07 20:05:19,170] loss: 0.020837  [    0/ 4683]
[2022-08-07 20:05:46,596] loss: 0.001718  [  480/ 4683]
[2022-08-07 20:06:14,023] loss: 0.001784  [  960/ 4683]
[2022-08-07 20:06:41,450] loss: 0.009018  [ 1440/ 4683]
[2022-08-07 20:07:08,877] loss: 0.005651  [ 1920/ 4683]
[2022-08-07 20:07:36,300] loss: 0.004329  [ 2400/ 4683]
[2022-08-07 20:08:03,725] loss: 0.008664  [ 2880/ 4683]
[2022-08-07 20:08:31,151] loss: 0.038634  [ 3360/ 4683]
[2022-08-07 20:08:58,575] loss: 0.002646  [ 3840/ 4683]
[2022-08-07 20:09:26,001] loss: 0.001127  [ 4320/ 4683]
[2022-08-07 20:11:23,184] Train Error: Accuracy: 99.893%, Avg loss: 0.007721
[2022-08-07 20:12:07,250] Test  Error: Accuracy: 99.286%, Avg loss: 0.024040
[2022-08-07 20:12:07,251] Epoch 13---------------
[2022-08-07 20:12:07,252] lr: 3.496492e-04
[2022-08-07 20:12:09,083] loss: 0.000865  [    0/ 4683]
[2022-08-07 20:12:36,509] loss: 0.002091  [  480/ 4683]
[2022-08-07 20:13:03,934] loss: 0.000768  [  960/ 4683]
[2022-08-07 20:13:31,361] loss: 0.000880  [ 1440/ 4683]
[2022-08-07 20:13:58,787] loss: 0.023773  [ 1920/ 4683]
[2022-08-07 20:14:26,213] loss: 0.001064  [ 2400/ 4683]
[2022-08-07 20:14:53,639] loss: 0.024967  [ 2880/ 4683]
[2022-08-07 20:15:21,065] loss: 0.003881  [ 3360/ 4683]
[2022-08-07 20:15:48,492] loss: 0.003207  [ 3840/ 4683]
[2022-08-07 20:16:15,920] loss: 0.009213  [ 4320/ 4683]
[2022-08-07 20:18:13,104] Train Error: Accuracy: 99.915%, Avg loss: 0.006980
[2022-08-07 20:18:57,172] Test  Error: Accuracy: 99.286%, Avg loss: 0.027979
[2022-08-07 20:18:57,173] Epoch 14---------------
[2022-08-07 20:18:57,173] lr: 2.705519e-04
[2022-08-07 20:18:59,003] loss: 0.011235  [    0/ 4683]
[2022-08-07 20:19:26,429] loss: 0.002192  [  480/ 4683]
[2022-08-07 20:19:53,855] loss: 0.001035  [  960/ 4683]
[2022-08-07 20:20:21,280] loss: 0.000768  [ 1440/ 4683]
[2022-08-07 20:20:48,703] loss: 0.001501  [ 1920/ 4683]
[2022-08-07 20:21:16,127] loss: 0.002836  [ 2400/ 4683]
[2022-08-07 20:21:43,553] loss: 0.001983  [ 2880/ 4683]
[2022-08-07 20:22:10,979] loss: 0.000592  [ 3360/ 4683]
[2022-08-07 20:22:38,404] loss: 0.001881  [ 3840/ 4683]
[2022-08-07 20:23:05,829] loss: 0.001497  [ 4320/ 4683]
[2022-08-07 20:25:03,007] Train Error: Accuracy: 99.829%, Avg loss: 0.006101
[2022-08-07 20:25:47,074] Test  Error: Accuracy: 99.095%, Avg loss: 0.033207
[2022-08-07 20:25:47,074] Epoch 15---------------
[2022-08-07 20:25:47,075] lr: 2.093479e-04
[2022-08-07 20:25:48,907] loss: 0.006974  [    0/ 4683]
[2022-08-07 20:26:16,332] loss: 0.000536  [  480/ 4683]
[2022-08-07 20:26:43,759] loss: 0.003178  [  960/ 4683]
[2022-08-07 20:27:11,184] loss: 0.001152  [ 1440/ 4683]
[2022-08-07 20:27:38,609] loss: 0.002678  [ 1920/ 4683]
[2022-08-07 20:28:06,032] loss: 0.002184  [ 2400/ 4683]
[2022-08-07 20:28:33,457] loss: 0.001469  [ 2880/ 4683]
[2022-08-07 20:29:00,881] loss: 0.012786  [ 3360/ 4683]
[2022-08-07 20:29:28,307] loss: 0.002268  [ 3840/ 4683]
[2022-08-07 20:29:55,732] loss: 0.033119  [ 4320/ 4683]
[2022-08-07 20:31:52,914] Train Error: Accuracy: 99.915%, Avg loss: 0.004135
[2022-08-07 20:32:36,981] Test  Error: Accuracy: 99.429%, Avg loss: 0.022608
[2022-08-07 20:32:36,982] Done!
[2022-08-07 20:32:36,986] Number of parameters:3229450
[2022-08-07 20:32:36,986] ## end time: 2022-08-07 20:32:36.982152
[2022-08-07 20:32:36,987] ## used time: 1:42:29.183394
