[2022-08-08 02:36:58,234] ## start time: 2022-08-08 02:36:58.099299
[2022-08-08 02:36:58,235] Using cuda device
[2022-08-08 02:36:58,236] In train:p&d10.npy.
[2022-08-08 02:36:58,237] One Channel
[2022-08-08 02:36:58,237] With Normal data.
[2022-08-08 02:36:58,238] Nunber of classes:10.
[2022-08-08 02:36:58,238] Nunber of ViT channels:1.
[2022-08-08 02:36:58,485] Totol epochs: 15
[2022-08-08 02:36:58,488] Epoch 1---------------
[2022-08-08 02:36:58,488] lr: 2.000000e-03
[2022-08-08 02:36:58,533] loss: 2.387724  [    0/ 4702]
[2022-08-08 02:36:59,070] loss: 2.016899  [  480/ 4702]
[2022-08-08 02:36:59,629] loss: 1.373146  [  960/ 4702]
[2022-08-08 02:37:00,161] loss: 0.457189  [ 1440/ 4702]
[2022-08-08 02:37:00,681] loss: 0.283005  [ 1920/ 4702]
[2022-08-08 02:37:01,199] loss: 0.117940  [ 2400/ 4702]
[2022-08-08 02:37:01,720] loss: 0.056792  [ 2880/ 4702]
[2022-08-08 02:37:02,238] loss: 0.017504  [ 3360/ 4702]
[2022-08-08 02:37:02,759] loss: 0.010384  [ 3840/ 4702]
[2022-08-08 02:37:03,284] loss: 0.349448  [ 4320/ 4702]
[2022-08-08 02:37:05,005] Train Error: Accuracy: 93.854%, Avg loss: 0.228409
[2022-08-08 02:37:05,604] Test  Error: Accuracy: 92.792%, Avg loss: 0.239281
[2022-08-08 02:37:05,605] Epoch 2---------------
[2022-08-08 02:37:05,607] lr: 1.900000e-03
[2022-08-08 02:37:05,644] loss: 0.261280  [    0/ 4702]
[2022-08-08 02:37:06,168] loss: 0.076238  [  480/ 4702]
[2022-08-08 02:37:06,692] loss: 0.012440  [  960/ 4702]
[2022-08-08 02:37:07,218] loss: 0.013166  [ 1440/ 4702]
[2022-08-08 02:37:07,740] loss: 0.032323  [ 1920/ 4702]
[2022-08-08 02:37:08,263] loss: 0.008834  [ 2400/ 4702]
[2022-08-08 02:37:08,786] loss: 0.006544  [ 2880/ 4702]
[2022-08-08 02:37:09,310] loss: 0.004610  [ 3360/ 4702]
[2022-08-08 02:37:09,838] loss: 0.006482  [ 3840/ 4702]
[2022-08-08 02:37:10,362] loss: 0.006013  [ 4320/ 4702]
[2022-08-08 02:37:12,090] Train Error: Accuracy: 100.000%, Avg loss: 0.003641
[2022-08-08 02:37:12,691] Test  Error: Accuracy: 100.000%, Avg loss: 0.004332
[2022-08-08 02:37:12,691] Epoch 3---------------
[2022-08-08 02:37:12,692] lr: 1.805000e-03
[2022-08-08 02:37:12,731] loss: 0.003029  [    0/ 4702]
[2022-08-08 02:37:13,255] loss: 0.002739  [  480/ 4702]
[2022-08-08 02:37:13,778] loss: 0.003480  [  960/ 4702]
[2022-08-08 02:37:14,295] loss: 0.002542  [ 1440/ 4702]
[2022-08-08 02:37:14,821] loss: 0.002224  [ 1920/ 4702]
[2022-08-08 02:37:15,361] loss: 0.001872  [ 2400/ 4702]
[2022-08-08 02:37:15,885] loss: 0.002458  [ 2880/ 4702]
[2022-08-08 02:37:16,430] loss: 0.001924  [ 3360/ 4702]
[2022-08-08 02:37:16,983] loss: 0.002170  [ 3840/ 4702]
[2022-08-08 02:37:17,592] loss: 0.001686  [ 4320/ 4702]
[2022-08-08 02:37:19,442] Train Error: Accuracy: 100.000%, Avg loss: 0.001669
[2022-08-08 02:37:20,039] Test  Error: Accuracy: 100.000%, Avg loss: 0.002143
[2022-08-08 02:37:20,040] Epoch 4---------------
[2022-08-08 02:37:20,040] lr: 1.714750e-03
[2022-08-08 02:37:20,077] loss: 0.001940  [    0/ 4702]
[2022-08-08 02:37:20,604] loss: 0.001771  [  480/ 4702]
[2022-08-08 02:37:21,131] loss: 0.001255  [  960/ 4702]
[2022-08-08 02:37:21,652] loss: 0.001728  [ 1440/ 4702]
[2022-08-08 02:37:22,181] loss: 0.001401  [ 1920/ 4702]
[2022-08-08 02:37:22,702] loss: 0.001087  [ 2400/ 4702]
[2022-08-08 02:37:23,226] loss: 0.001411  [ 2880/ 4702]
[2022-08-08 02:37:23,744] loss: 0.001109  [ 3360/ 4702]
[2022-08-08 02:37:24,274] loss: 0.001080  [ 3840/ 4702]
[2022-08-08 02:37:24,798] loss: 0.001357  [ 4320/ 4702]
[2022-08-08 02:37:26,512] Train Error: Accuracy: 100.000%, Avg loss: 0.001113
[2022-08-08 02:37:27,106] Test  Error: Accuracy: 100.000%, Avg loss: 0.001307
[2022-08-08 02:37:27,106] Epoch 5---------------
[2022-08-08 02:37:27,107] lr: 1.629012e-03
[2022-08-08 02:37:27,145] loss: 0.001079  [    0/ 4702]
[2022-08-08 02:37:27,669] loss: 0.000993  [  480/ 4702]
[2022-08-08 02:37:28,193] loss: 0.000965  [  960/ 4702]
[2022-08-08 02:37:28,716] loss: 0.000778  [ 1440/ 4702]
[2022-08-08 02:37:29,237] loss: 0.000937  [ 1920/ 4702]
[2022-08-08 02:37:29,765] loss: 0.000884  [ 2400/ 4702]
[2022-08-08 02:37:30,289] loss: 0.000926  [ 2880/ 4702]
[2022-08-08 02:37:30,813] loss: 0.651317  [ 3360/ 4702]
[2022-08-08 02:37:31,334] loss: 0.380947  [ 3840/ 4702]
[2022-08-08 02:37:31,856] loss: 0.057421  [ 4320/ 4702]
[2022-08-08 02:37:33,562] Train Error: Accuracy: 99.851%, Avg loss: 0.029915
[2022-08-08 02:37:34,171] Test  Error: Accuracy: 99.904%, Avg loss: 0.034307
[2022-08-08 02:37:34,171] Epoch 6---------------
[2022-08-08 02:37:34,172] lr: 1.137600e-03
[2022-08-08 02:37:34,211] loss: 0.064231  [    0/ 4702]
[2022-08-08 02:37:34,728] loss: 0.014250  [  480/ 4702]
[2022-08-08 02:37:35,248] loss: 0.011044  [  960/ 4702]
[2022-08-08 02:37:35,766] loss: 0.031396  [ 1440/ 4702]
[2022-08-08 02:37:36,285] loss: 0.005594  [ 1920/ 4702]
[2022-08-08 02:37:36,802] loss: 0.009095  [ 2400/ 4702]
[2022-08-08 02:37:37,322] loss: 0.007164  [ 2880/ 4702]
[2022-08-08 02:37:37,840] loss: 0.004767  [ 3360/ 4702]
[2022-08-08 02:37:38,359] loss: 0.003514  [ 3840/ 4702]
[2022-08-08 02:37:38,878] loss: 0.004387  [ 4320/ 4702]
[2022-08-08 02:37:40,601] Train Error: Accuracy: 100.000%, Avg loss: 0.003792
[2022-08-08 02:37:41,195] Test  Error: Accuracy: 100.000%, Avg loss: 0.004805
[2022-08-08 02:37:41,195] Epoch 7---------------
[2022-08-08 02:37:41,196] lr: 1.080720e-03
[2022-08-08 02:37:41,233] loss: 0.002913  [    0/ 4702]
[2022-08-08 02:37:41,752] loss: 0.002876  [  480/ 4702]
[2022-08-08 02:37:42,276] loss: 0.002685  [  960/ 4702]
[2022-08-08 02:37:42,797] loss: 0.002331  [ 1440/ 4702]
[2022-08-08 02:37:43,319] loss: 0.002227  [ 1920/ 4702]
[2022-08-08 02:37:43,839] loss: 0.001840  [ 2400/ 4702]
[2022-08-08 02:37:44,362] loss: 0.002584  [ 2880/ 4702]
[2022-08-08 02:37:44,886] loss: 0.002999  [ 3360/ 4702]
[2022-08-08 02:37:45,406] loss: 0.001877  [ 3840/ 4702]
[2022-08-08 02:37:45,934] loss: 0.002171  [ 4320/ 4702]
[2022-08-08 02:37:47,654] Train Error: Accuracy: 100.000%, Avg loss: 0.002056
[2022-08-08 02:37:48,253] Test  Error: Accuracy: 100.000%, Avg loss: 0.002449
[2022-08-08 02:37:48,253] Epoch 8---------------
[2022-08-08 02:37:48,254] lr: 1.026684e-03
[2022-08-08 02:37:48,292] loss: 0.001532  [    0/ 4702]
[2022-08-08 02:37:48,818] loss: 0.002304  [  480/ 4702]
[2022-08-08 02:37:49,340] loss: 0.002737  [  960/ 4702]
[2022-08-08 02:37:49,865] loss: 0.001712  [ 1440/ 4702]
[2022-08-08 02:37:50,387] loss: 0.001210  [ 1920/ 4702]
[2022-08-08 02:37:50,912] loss: 0.001265  [ 2400/ 4702]
[2022-08-08 02:37:51,431] loss: 0.001617  [ 2880/ 4702]
[2022-08-08 02:37:51,957] loss: 0.001112  [ 3360/ 4702]
[2022-08-08 02:37:52,477] loss: 0.001133  [ 3840/ 4702]
[2022-08-08 02:37:53,000] loss: 0.001068  [ 4320/ 4702]
[2022-08-08 02:37:54,732] Train Error: Accuracy: 100.000%, Avg loss: 0.001263
[2022-08-08 02:37:55,330] Test  Error: Accuracy: 100.000%, Avg loss: 0.001573
[2022-08-08 02:37:55,331] Epoch 9---------------
[2022-08-08 02:37:55,332] lr: 9.753500e-04
[2022-08-08 02:37:55,368] loss: 0.001065  [    0/ 4702]
[2022-08-08 02:37:55,891] loss: 0.003321  [  480/ 4702]
[2022-08-08 02:37:56,414] loss: 0.001200  [  960/ 4702]
[2022-08-08 02:37:56,937] loss: 0.001253  [ 1440/ 4702]
[2022-08-08 02:37:57,457] loss: 0.000956  [ 1920/ 4702]
[2022-08-08 02:37:57,978] loss: 0.000799  [ 2400/ 4702]
[2022-08-08 02:37:58,502] loss: 0.001287  [ 2880/ 4702]
[2022-08-08 02:37:59,022] loss: 0.000854  [ 3360/ 4702]
[2022-08-08 02:37:59,548] loss: 0.001184  [ 3840/ 4702]
[2022-08-08 02:38:00,074] loss: 0.000877  [ 4320/ 4702]
[2022-08-08 02:38:01,807] Train Error: Accuracy: 100.000%, Avg loss: 0.000986
[2022-08-08 02:38:02,416] Test  Error: Accuracy: 100.000%, Avg loss: 0.001144
[2022-08-08 02:38:02,416] Epoch 10---------------
[2022-08-08 02:38:02,417] lr: 9.265825e-04
[2022-08-08 02:38:02,454] loss: 0.000847  [    0/ 4702]
[2022-08-08 02:38:02,979] loss: 0.000823  [  480/ 4702]
[2022-08-08 02:38:03,502] loss: 0.000789  [  960/ 4702]
[2022-08-08 02:38:04,031] loss: 0.000648  [ 1440/ 4702]
[2022-08-08 02:38:04,559] loss: 0.000719  [ 1920/ 4702]
[2022-08-08 02:38:05,082] loss: 0.000640  [ 2400/ 4702]
[2022-08-08 02:38:05,606] loss: 0.000762  [ 2880/ 4702]
[2022-08-08 02:38:06,132] loss: 0.000668  [ 3360/ 4702]
[2022-08-08 02:38:06,653] loss: 0.000888  [ 3840/ 4702]
[2022-08-08 02:38:07,175] loss: 0.000865  [ 4320/ 4702]
[2022-08-08 02:38:08,889] Train Error: Accuracy: 100.000%, Avg loss: 0.000780
[2022-08-08 02:38:09,487] Test  Error: Accuracy: 100.000%, Avg loss: 0.000955
[2022-08-08 02:38:09,487] Epoch 11---------------
[2022-08-08 02:38:09,488] lr: 8.802533e-04
[2022-08-08 02:38:09,527] loss: 0.000708  [    0/ 4702]
[2022-08-08 02:38:10,048] loss: 0.000737  [  480/ 4702]
[2022-08-08 02:38:10,570] loss: 0.001157  [  960/ 4702]
[2022-08-08 02:38:11,091] loss: 0.000962  [ 1440/ 4702]
[2022-08-08 02:38:11,615] loss: 0.000702  [ 1920/ 4702]
[2022-08-08 02:38:12,137] loss: 0.000725  [ 2400/ 4702]
[2022-08-08 02:38:12,660] loss: 0.000611  [ 2880/ 4702]
[2022-08-08 02:38:13,180] loss: 0.000661  [ 3360/ 4702]
[2022-08-08 02:38:13,703] loss: 0.000483  [ 3840/ 4702]
[2022-08-08 02:38:14,227] loss: 0.000553  [ 4320/ 4702]
[2022-08-08 02:38:15,952] Train Error: Accuracy: 99.638%, Avg loss: 0.010364
[2022-08-08 02:38:16,549] Test  Error: Accuracy: 99.616%, Avg loss: 0.012848
[2022-08-08 02:38:16,550] Epoch 12---------------
[2022-08-08 02:38:16,551] lr: 6.147137e-04
[2022-08-08 02:38:16,588] loss: 0.005043  [    0/ 4702]
[2022-08-08 02:38:17,120] loss: 0.001003  [  480/ 4702]
[2022-08-08 02:38:17,656] loss: 0.000864  [  960/ 4702]
[2022-08-08 02:38:18,177] loss: 0.004252  [ 1440/ 4702]
[2022-08-08 02:38:18,702] loss: 0.001163  [ 1920/ 4702]
[2022-08-08 02:38:19,223] loss: 0.000639  [ 2400/ 4702]
[2022-08-08 02:38:19,749] loss: 0.000799  [ 2880/ 4702]
[2022-08-08 02:38:20,275] loss: 0.000619  [ 3360/ 4702]
[2022-08-08 02:38:20,799] loss: 0.000625  [ 3840/ 4702]
[2022-08-08 02:38:21,320] loss: 0.000729  [ 4320/ 4702]
[2022-08-08 02:38:23,034] Train Error: Accuracy: 100.000%, Avg loss: 0.000667
[2022-08-08 02:38:23,627] Test  Error: Accuracy: 100.000%, Avg loss: 0.001070
[2022-08-08 02:38:23,627] Epoch 13---------------
[2022-08-08 02:38:23,630] lr: 5.839780e-04
[2022-08-08 02:38:23,666] loss: 0.000896  [    0/ 4702]
[2022-08-08 02:38:24,192] loss: 0.000821  [  480/ 4702]
[2022-08-08 02:38:24,717] loss: 0.000861  [  960/ 4702]
[2022-08-08 02:38:25,240] loss: 0.000610  [ 1440/ 4702]
[2022-08-08 02:38:25,762] loss: 0.001062  [ 1920/ 4702]
[2022-08-08 02:38:26,286] loss: 0.000412  [ 2400/ 4702]
[2022-08-08 02:38:26,807] loss: 0.000545  [ 2880/ 4702]
[2022-08-08 02:38:27,333] loss: 0.000586  [ 3360/ 4702]
[2022-08-08 02:38:27,864] loss: 0.000654  [ 3840/ 4702]
[2022-08-08 02:38:28,389] loss: 0.000690  [ 4320/ 4702]
[2022-08-08 02:38:30,120] Train Error: Accuracy: 100.000%, Avg loss: 0.000547
[2022-08-08 02:38:30,725] Test  Error: Accuracy: 100.000%, Avg loss: 0.000778
[2022-08-08 02:38:30,726] Epoch 14---------------
[2022-08-08 02:38:30,727] lr: 5.547791e-04
[2022-08-08 02:38:30,765] loss: 0.000574  [    0/ 4702]
[2022-08-08 02:38:31,293] loss: 0.000481  [  480/ 4702]
[2022-08-08 02:38:31,818] loss: 0.000582  [  960/ 4702]
[2022-08-08 02:38:32,342] loss: 0.000375  [ 1440/ 4702]
[2022-08-08 02:38:32,866] loss: 0.000576  [ 1920/ 4702]
[2022-08-08 02:38:33,391] loss: 0.000472  [ 2400/ 4702]
[2022-08-08 02:38:33,916] loss: 0.000445  [ 2880/ 4702]
[2022-08-08 02:38:34,440] loss: 0.000549  [ 3360/ 4702]
[2022-08-08 02:38:34,963] loss: 0.000442  [ 3840/ 4702]
[2022-08-08 02:38:35,485] loss: 0.000417  [ 4320/ 4702]
[2022-08-08 02:38:37,222] Train Error: Accuracy: 100.000%, Avg loss: 0.000517
[2022-08-08 02:38:37,819] Test  Error: Accuracy: 100.000%, Avg loss: 0.000592
[2022-08-08 02:38:37,820] Epoch 15---------------
[2022-08-08 02:38:37,821] lr: 5.270402e-04
[2022-08-08 02:38:37,858] loss: 0.000408  [    0/ 4702]
[2022-08-08 02:38:38,380] loss: 0.000773  [  480/ 4702]
[2022-08-08 02:38:38,901] loss: 0.000493  [  960/ 4702]
[2022-08-08 02:38:39,422] loss: 0.000671  [ 1440/ 4702]
[2022-08-08 02:38:39,948] loss: 0.000346  [ 1920/ 4702]
[2022-08-08 02:38:40,470] loss: 0.000478  [ 2400/ 4702]
[2022-08-08 02:38:40,994] loss: 0.000370  [ 2880/ 4702]
[2022-08-08 02:38:41,515] loss: 0.000421  [ 3360/ 4702]
[2022-08-08 02:38:42,037] loss: 0.000538  [ 3840/ 4702]
[2022-08-08 02:38:42,562] loss: 0.000396  [ 4320/ 4702]
[2022-08-08 02:38:44,285] Train Error: Accuracy: 100.000%, Avg loss: 0.000475
[2022-08-08 02:38:44,887] Test  Error: Accuracy: 100.000%, Avg loss: 0.001034
[2022-08-08 02:38:44,888] Done!
[2022-08-08 02:38:44,892] Number of parameters:3686410
[2022-08-08 02:38:44,892] ## end time: 2022-08-08 02:38:44.888045
[2022-08-08 02:38:44,892] ## used time: 0:01:46.788746
