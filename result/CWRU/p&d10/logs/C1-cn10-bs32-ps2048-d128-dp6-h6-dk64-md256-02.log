[2022-08-08 02:38:45,088] ## start time: 2022-08-08 02:38:44.956287
[2022-08-08 02:38:45,088] Using cuda device
[2022-08-08 02:38:45,090] In train:p&d10.npy.
[2022-08-08 02:38:45,090] One Channel
[2022-08-08 02:38:45,090] With Normal data.
[2022-08-08 02:38:45,091] Nunber of classes:10.
[2022-08-08 02:38:45,091] Nunber of ViT channels:1.
[2022-08-08 02:38:45,342] Totol epochs: 15
[2022-08-08 02:38:45,345] Epoch 1---------------
[2022-08-08 02:38:45,345] lr: 2.000000e-03
[2022-08-08 02:38:45,384] loss: 2.356853  [    0/ 4717]
[2022-08-08 02:38:45,922] loss: 1.628992  [  480/ 4717]
[2022-08-08 02:38:46,474] loss: 0.857590  [  960/ 4717]
[2022-08-08 02:38:47,001] loss: 0.328749  [ 1440/ 4717]
[2022-08-08 02:38:47,528] loss: 0.427111  [ 1920/ 4717]
[2022-08-08 02:38:48,047] loss: 0.045956  [ 2400/ 4717]
[2022-08-08 02:38:48,579] loss: 0.040679  [ 2880/ 4717]
[2022-08-08 02:38:49,099] loss: 0.015802  [ 3360/ 4717]
[2022-08-08 02:38:49,623] loss: 0.008429  [ 3840/ 4717]
[2022-08-08 02:38:50,142] loss: 0.009398  [ 4320/ 4717]
[2022-08-08 02:38:51,935] Train Error: Accuracy: 100.000%, Avg loss: 0.006706
[2022-08-08 02:38:52,526] Test  Error: Accuracy: 99.903%, Avg loss: 0.009481
[2022-08-08 02:38:52,526] Epoch 2---------------
[2022-08-08 02:38:52,527] lr: 1.900000e-03
[2022-08-08 02:38:52,563] loss: 0.005380  [    0/ 4717]
[2022-08-08 02:38:53,082] loss: 0.004858  [  480/ 4717]
[2022-08-08 02:38:53,602] loss: 0.005890  [  960/ 4717]
[2022-08-08 02:38:54,128] loss: 0.003793  [ 1440/ 4717]
[2022-08-08 02:38:54,653] loss: 0.605342  [ 1920/ 4717]
[2022-08-08 02:38:55,180] loss: 0.058059  [ 2400/ 4717]
[2022-08-08 02:38:55,703] loss: 0.013061  [ 2880/ 4717]
[2022-08-08 02:38:56,229] loss: 0.015243  [ 3360/ 4717]
[2022-08-08 02:38:56,749] loss: 0.010975  [ 3840/ 4717]
[2022-08-08 02:38:57,270] loss: 0.006978  [ 4320/ 4717]
[2022-08-08 02:38:59,031] Train Error: Accuracy: 99.979%, Avg loss: 0.005937
[2022-08-08 02:38:59,625] Test  Error: Accuracy: 99.952%, Avg loss: 0.007592
[2022-08-08 02:38:59,625] Epoch 3---------------
[2022-08-08 02:38:59,626] lr: 1.805000e-03
[2022-08-08 02:38:59,664] loss: 0.006768  [    0/ 4717]
[2022-08-08 02:39:00,189] loss: 0.009478  [  480/ 4717]
[2022-08-08 02:39:00,715] loss: 0.017425  [  960/ 4717]
[2022-08-08 02:39:01,235] loss: 0.003452  [ 1440/ 4717]
[2022-08-08 02:39:01,757] loss: 0.005727  [ 1920/ 4717]
[2022-08-08 02:39:02,278] loss: 0.003053  [ 2400/ 4717]
[2022-08-08 02:39:02,801] loss: 0.002451  [ 2880/ 4717]
[2022-08-08 02:39:03,327] loss: 0.002534  [ 3360/ 4717]
[2022-08-08 02:39:03,845] loss: 0.001600  [ 3840/ 4717]
[2022-08-08 02:39:04,365] loss: 0.002410  [ 4320/ 4717]
[2022-08-08 02:39:06,141] Train Error: Accuracy: 100.000%, Avg loss: 0.001881
[2022-08-08 02:39:06,736] Test  Error: Accuracy: 99.952%, Avg loss: 0.002794
[2022-08-08 02:39:06,736] Epoch 4---------------
[2022-08-08 02:39:06,738] lr: 1.714750e-03
[2022-08-08 02:39:06,775] loss: 0.001717  [    0/ 4717]
[2022-08-08 02:39:07,297] loss: 0.001679  [  480/ 4717]
[2022-08-08 02:39:07,819] loss: 0.001524  [  960/ 4717]
[2022-08-08 02:39:08,340] loss: 0.001391  [ 1440/ 4717]
[2022-08-08 02:39:08,870] loss: 0.001893  [ 1920/ 4717]
[2022-08-08 02:39:09,393] loss: 0.001372  [ 2400/ 4717]
[2022-08-08 02:39:09,919] loss: 0.001431  [ 2880/ 4717]
[2022-08-08 02:39:10,441] loss: 0.001312  [ 3360/ 4717]
[2022-08-08 02:39:10,967] loss: 0.000936  [ 3840/ 4717]
[2022-08-08 02:39:11,489] loss: 0.001320  [ 4320/ 4717]
[2022-08-08 02:39:13,257] Train Error: Accuracy: 100.000%, Avg loss: 0.001157
[2022-08-08 02:39:13,848] Test  Error: Accuracy: 99.952%, Avg loss: 0.002371
[2022-08-08 02:39:13,848] Epoch 5---------------
[2022-08-08 02:39:13,849] lr: 1.629012e-03
[2022-08-08 02:39:13,886] loss: 0.001588  [    0/ 4717]
[2022-08-08 02:39:14,413] loss: 0.001089  [  480/ 4717]
[2022-08-08 02:39:14,938] loss: 0.000884  [  960/ 4717]
[2022-08-08 02:39:15,466] loss: 0.000895  [ 1440/ 4717]
[2022-08-08 02:39:15,999] loss: 0.001244  [ 1920/ 4717]
[2022-08-08 02:39:16,524] loss: 0.000863  [ 2400/ 4717]
[2022-08-08 02:39:17,047] loss: 0.000871  [ 2880/ 4717]
[2022-08-08 02:39:17,574] loss: 0.000895  [ 3360/ 4717]
[2022-08-08 02:39:18,097] loss: 0.000761  [ 3840/ 4717]
[2022-08-08 02:39:18,628] loss: 0.000922  [ 4320/ 4717]
[2022-08-08 02:39:20,412] Train Error: Accuracy: 100.000%, Avg loss: 0.000870
[2022-08-08 02:39:21,009] Test  Error: Accuracy: 100.000%, Avg loss: 0.001307
[2022-08-08 02:39:21,010] Epoch 6---------------
[2022-08-08 02:39:21,011] lr: 1.547562e-03
[2022-08-08 02:39:21,047] loss: 0.000898  [    0/ 4717]
[2022-08-08 02:39:21,575] loss: 0.000948  [  480/ 4717]
[2022-08-08 02:39:22,094] loss: 0.000711  [  960/ 4717]
[2022-08-08 02:39:22,615] loss: 0.000844  [ 1440/ 4717]
[2022-08-08 02:39:23,136] loss: 0.000941  [ 1920/ 4717]
[2022-08-08 02:39:23,655] loss: 0.000775  [ 2400/ 4717]
[2022-08-08 02:39:24,179] loss: 0.000574  [ 2880/ 4717]
[2022-08-08 02:39:24,724] loss: 0.000866  [ 3360/ 4717]
[2022-08-08 02:39:25,253] loss: 0.000639  [ 3840/ 4717]
[2022-08-08 02:39:25,781] loss: 0.000722  [ 4320/ 4717]
[2022-08-08 02:39:27,551] Train Error: Accuracy: 100.000%, Avg loss: 0.000685
[2022-08-08 02:39:28,149] Test  Error: Accuracy: 100.000%, Avg loss: 0.000919
[2022-08-08 02:39:28,150] Epoch 7---------------
[2022-08-08 02:39:28,151] lr: 1.470184e-03
[2022-08-08 02:39:28,189] loss: 0.000855  [    0/ 4717]
[2022-08-08 02:39:28,715] loss: 0.000553  [  480/ 4717]
[2022-08-08 02:39:29,244] loss: 0.000644  [  960/ 4717]
[2022-08-08 02:39:29,774] loss: 0.000743  [ 1440/ 4717]
[2022-08-08 02:39:30,304] loss: 0.000766  [ 1920/ 4717]
[2022-08-08 02:39:30,828] loss: 0.000533  [ 2400/ 4717]
[2022-08-08 02:39:31,353] loss: 0.000771  [ 2880/ 4717]
[2022-08-08 02:39:31,873] loss: 0.000596  [ 3360/ 4717]
[2022-08-08 02:39:32,396] loss: 0.000567  [ 3840/ 4717]
[2022-08-08 02:39:32,920] loss: 0.000760  [ 4320/ 4717]
[2022-08-08 02:39:34,692] Train Error: Accuracy: 100.000%, Avg loss: 0.000590
[2022-08-08 02:39:35,284] Test  Error: Accuracy: 100.000%, Avg loss: 0.000867
[2022-08-08 02:39:35,284] Epoch 8---------------
[2022-08-08 02:39:35,285] lr: 1.396675e-03
[2022-08-08 02:39:35,322] loss: 0.000559  [    0/ 4717]
[2022-08-08 02:39:35,853] loss: 0.000613  [  480/ 4717]
[2022-08-08 02:39:36,374] loss: 0.000521  [  960/ 4717]
[2022-08-08 02:39:36,900] loss: 0.000555  [ 1440/ 4717]
[2022-08-08 02:39:37,421] loss: 0.000757  [ 1920/ 4717]
[2022-08-08 02:39:37,946] loss: 0.000474  [ 2400/ 4717]
[2022-08-08 02:39:38,468] loss: 0.000624  [ 2880/ 4717]
[2022-08-08 02:39:38,997] loss: 0.000581  [ 3360/ 4717]
[2022-08-08 02:39:39,521] loss: 0.000510  [ 3840/ 4717]
[2022-08-08 02:39:40,044] loss: 0.000489  [ 4320/ 4717]
[2022-08-08 02:39:41,835] Train Error: Accuracy: 100.000%, Avg loss: 0.000596
[2022-08-08 02:39:42,426] Test  Error: Accuracy: 100.000%, Avg loss: 0.000865
[2022-08-08 02:39:42,426] Epoch 9---------------
[2022-08-08 02:39:42,427] lr: 1.326841e-03
[2022-08-08 02:39:42,465] loss: 0.000415  [    0/ 4717]
[2022-08-08 02:39:42,993] loss: 0.000526  [  480/ 4717]
[2022-08-08 02:39:43,516] loss: 0.000683  [  960/ 4717]
[2022-08-08 02:39:44,039] loss: 0.000377  [ 1440/ 4717]
[2022-08-08 02:39:44,567] loss: 0.000421  [ 1920/ 4717]
[2022-08-08 02:39:45,095] loss: 0.000657  [ 2400/ 4717]
[2022-08-08 02:39:45,618] loss: 0.000842  [ 2880/ 4717]
[2022-08-08 02:39:46,145] loss: 0.000374  [ 3360/ 4717]
[2022-08-08 02:39:46,669] loss: 0.000572  [ 3840/ 4717]
[2022-08-08 02:39:47,193] loss: 0.000457  [ 4320/ 4717]
[2022-08-08 02:39:48,963] Train Error: Accuracy: 100.000%, Avg loss: 0.000503
[2022-08-08 02:39:49,564] Test  Error: Accuracy: 100.000%, Avg loss: 0.000735
[2022-08-08 02:39:49,564] Epoch 10---------------
[2022-08-08 02:39:49,565] lr: 1.260499e-03
[2022-08-08 02:39:49,603] loss: 0.000559  [    0/ 4717]
[2022-08-08 02:39:50,131] loss: 0.000452  [  480/ 4717]
[2022-08-08 02:39:50,676] loss: 0.000383  [  960/ 4717]
[2022-08-08 02:39:51,208] loss: 0.000428  [ 1440/ 4717]
[2022-08-08 02:39:51,744] loss: 0.000482  [ 1920/ 4717]
[2022-08-08 02:39:52,280] loss: 0.000617  [ 2400/ 4717]
[2022-08-08 02:39:52,805] loss: 0.000402  [ 2880/ 4717]
[2022-08-08 02:39:53,325] loss: 0.000322  [ 3360/ 4717]
[2022-08-08 02:39:53,844] loss: 0.000479  [ 3840/ 4717]
[2022-08-08 02:39:54,361] loss: 0.000277  [ 4320/ 4717]
[2022-08-08 02:39:56,133] Train Error: Accuracy: 100.000%, Avg loss: 0.000465
[2022-08-08 02:39:56,726] Test  Error: Accuracy: 99.952%, Avg loss: 0.000962
[2022-08-08 02:39:56,727] Epoch 11---------------
[2022-08-08 02:39:56,728] lr: 8.802533e-04
[2022-08-08 02:39:56,765] loss: 0.000442  [    0/ 4717]
[2022-08-08 02:39:57,288] loss: 0.000583  [  480/ 4717]
[2022-08-08 02:39:57,810] loss: 0.000330  [  960/ 4717]
[2022-08-08 02:39:58,331] loss: 0.000388  [ 1440/ 4717]
[2022-08-08 02:39:58,854] loss: 0.000342  [ 1920/ 4717]
[2022-08-08 02:39:59,371] loss: 2.026919  [ 2400/ 4717]
[2022-08-08 02:39:59,903] loss: 1.112097  [ 2880/ 4717]
[2022-08-08 02:40:00,424] loss: 0.094529  [ 3360/ 4717]
[2022-08-08 02:40:00,957] loss: 0.052156  [ 3840/ 4717]
[2022-08-08 02:40:01,493] loss: 0.012420  [ 4320/ 4717]
[2022-08-08 02:40:03,269] Train Error: Accuracy: 99.873%, Avg loss: 0.016952
[2022-08-08 02:40:03,865] Test  Error: Accuracy: 99.613%, Avg loss: 0.024739
[2022-08-08 02:40:03,866] Epoch 12---------------
[2022-08-08 02:40:03,867] lr: 6.147137e-04
[2022-08-08 02:40:03,905] loss: 0.025920  [    0/ 4717]
[2022-08-08 02:40:04,429] loss: 0.004834  [  480/ 4717]
[2022-08-08 02:40:04,954] loss: 0.007417  [  960/ 4717]
[2022-08-08 02:40:05,473] loss: 0.005202  [ 1440/ 4717]
[2022-08-08 02:40:05,994] loss: 0.005017  [ 1920/ 4717]
[2022-08-08 02:40:06,523] loss: 0.004889  [ 2400/ 4717]
[2022-08-08 02:40:07,047] loss: 0.004899  [ 2880/ 4717]
[2022-08-08 02:40:07,568] loss: 0.002807  [ 3360/ 4717]
[2022-08-08 02:40:08,091] loss: 0.015821  [ 3840/ 4717]
[2022-08-08 02:40:08,614] loss: 0.003660  [ 4320/ 4717]
[2022-08-08 02:40:10,389] Train Error: Accuracy: 100.000%, Avg loss: 0.002889
[2022-08-08 02:40:10,987] Test  Error: Accuracy: 99.903%, Avg loss: 0.005468
[2022-08-08 02:40:10,987] Epoch 13---------------
[2022-08-08 02:40:10,988] lr: 5.839780e-04
[2022-08-08 02:40:11,026] loss: 0.004078  [    0/ 4717]
[2022-08-08 02:40:11,553] loss: 0.002261  [  480/ 4717]
[2022-08-08 02:40:12,084] loss: 0.002223  [  960/ 4717]
[2022-08-08 02:40:12,609] loss: 0.002547  [ 1440/ 4717]
[2022-08-08 02:40:13,134] loss: 0.001676  [ 1920/ 4717]
[2022-08-08 02:40:13,655] loss: 0.005269  [ 2400/ 4717]
[2022-08-08 02:40:14,181] loss: 0.001301  [ 2880/ 4717]
[2022-08-08 02:40:14,699] loss: 0.001578  [ 3360/ 4717]
[2022-08-08 02:40:15,221] loss: 0.002046  [ 3840/ 4717]
[2022-08-08 02:40:15,749] loss: 0.001988  [ 4320/ 4717]
[2022-08-08 02:40:17,545] Train Error: Accuracy: 100.000%, Avg loss: 0.001612
[2022-08-08 02:40:18,139] Test  Error: Accuracy: 99.903%, Avg loss: 0.004646
[2022-08-08 02:40:18,139] Epoch 14---------------
[2022-08-08 02:40:18,140] lr: 5.547791e-04
[2022-08-08 02:40:18,179] loss: 0.001632  [    0/ 4717]
[2022-08-08 02:40:18,704] loss: 0.000889  [  480/ 4717]
[2022-08-08 02:40:19,233] loss: 0.001124  [  960/ 4717]
[2022-08-08 02:40:19,753] loss: 0.001468  [ 1440/ 4717]
[2022-08-08 02:40:20,276] loss: 0.000979  [ 1920/ 4717]
[2022-08-08 02:40:20,800] loss: 0.001288  [ 2400/ 4717]
[2022-08-08 02:40:21,324] loss: 0.000967  [ 2880/ 4717]
[2022-08-08 02:40:21,845] loss: 0.000831  [ 3360/ 4717]
[2022-08-08 02:40:22,369] loss: 0.001198  [ 3840/ 4717]
[2022-08-08 02:40:22,893] loss: 0.001312  [ 4320/ 4717]
[2022-08-08 02:40:24,671] Train Error: Accuracy: 100.000%, Avg loss: 0.001131
[2022-08-08 02:40:25,263] Test  Error: Accuracy: 99.903%, Avg loss: 0.003921
[2022-08-08 02:40:25,263] Epoch 15---------------
[2022-08-08 02:40:25,264] lr: 5.270402e-04
[2022-08-08 02:40:25,302] loss: 0.006312  [    0/ 4717]
[2022-08-08 02:40:25,828] loss: 0.001632  [  480/ 4717]
[2022-08-08 02:40:26,350] loss: 0.003491  [  960/ 4717]
[2022-08-08 02:40:26,876] loss: 0.001052  [ 1440/ 4717]
[2022-08-08 02:40:27,401] loss: 0.001088  [ 1920/ 4717]
[2022-08-08 02:40:27,928] loss: 0.001121  [ 2400/ 4717]
[2022-08-08 02:40:28,449] loss: 0.001926  [ 2880/ 4717]
[2022-08-08 02:40:28,975] loss: 0.001420  [ 3360/ 4717]
[2022-08-08 02:40:29,498] loss: 0.002475  [ 3840/ 4717]
[2022-08-08 02:40:30,022] loss: 0.001259  [ 4320/ 4717]
[2022-08-08 02:40:31,795] Train Error: Accuracy: 100.000%, Avg loss: 0.001018
[2022-08-08 02:40:32,402] Test  Error: Accuracy: 99.952%, Avg loss: 0.002848
[2022-08-08 02:40:32,402] Done!
[2022-08-08 02:40:32,406] Number of parameters:3686410
[2022-08-08 02:40:32,406] ## end time: 2022-08-08 02:40:32.402668
[2022-08-08 02:40:32,407] ## used time: 0:01:47.446381
