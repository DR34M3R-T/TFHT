[2022-08-07 17:06:54,693] ## start time: 2022-08-07 17:06:54.560371
[2022-08-07 17:06:54,694] Using cuda device
[2022-08-07 17:06:54,695] In train:p&d10.npy.
[2022-08-07 17:06:54,695] One Channel
[2022-08-07 17:06:54,695] With Normal data.
[2022-08-07 17:06:54,695] Nunber of classes:10.
[2022-08-07 17:06:54,695] Nunber of ViT channels:1.
[2022-08-07 17:06:55,790] Totol epochs: 15
[2022-08-07 17:06:55,793] Epoch 1---------------
[2022-08-07 17:06:55,793] lr: 2.000000e-03
[2022-08-07 17:06:59,255] loss: 2.488152  [    0/ 4733]
[2022-08-07 17:07:26,258] loss: 1.911971  [  480/ 4733]
[2022-08-07 17:07:53,392] loss: 1.497948  [  960/ 4733]
[2022-08-07 17:08:20,595] loss: 1.442265  [ 1440/ 4733]
[2022-08-07 17:08:47,846] loss: 1.165130  [ 1920/ 4733]
[2022-08-07 17:09:15,097] loss: 0.836693  [ 2400/ 4733]
[2022-08-07 17:09:42,346] loss: 0.806895  [ 2880/ 4733]
[2022-08-07 17:10:09,595] loss: 0.670943  [ 3360/ 4733]
[2022-08-07 17:10:36,842] loss: 1.022276  [ 3840/ 4733]
[2022-08-07 17:11:04,090] loss: 0.567414  [ 4320/ 4733]
[2022-08-07 17:13:05,124] Train Error: Accuracy: 90.894%, Avg loss: 0.288470
[2022-08-07 17:13:48,153] Test  Error: Accuracy: 90.293%, Avg loss: 0.316984
[2022-08-07 17:13:48,153] Epoch 2---------------
[2022-08-07 17:13:48,154] lr: 1.900000e-03
[2022-08-07 17:13:49,984] loss: 0.612479  [    0/ 4733]
[2022-08-07 17:14:17,415] loss: 0.233691  [  480/ 4733]
[2022-08-07 17:14:44,845] loss: 0.249542  [  960/ 4733]
[2022-08-07 17:15:12,276] loss: 0.070356  [ 1440/ 4733]
[2022-08-07 17:15:39,708] loss: 0.272271  [ 1920/ 4733]
[2022-08-07 17:16:07,140] loss: 0.104750  [ 2400/ 4733]
[2022-08-07 17:16:34,572] loss: 0.224691  [ 2880/ 4733]
[2022-08-07 17:17:02,005] loss: 0.218411  [ 3360/ 4733]
[2022-08-07 17:17:29,436] loss: 0.049991  [ 3840/ 4733]
[2022-08-07 17:17:56,867] loss: 0.016632  [ 4320/ 4733]
[2022-08-07 17:19:57,945] Train Error: Accuracy: 97.866%, Avg loss: 0.073996
[2022-08-07 17:20:40,973] Test  Error: Accuracy: 97.415%, Avg loss: 0.097113
[2022-08-07 17:20:40,973] Epoch 3---------------
[2022-08-07 17:20:40,974] lr: 1.805000e-03
[2022-08-07 17:20:42,805] loss: 0.025830  [    0/ 4733]
[2022-08-07 17:21:10,239] loss: 0.043008  [  480/ 4733]
[2022-08-07 17:21:37,671] loss: 0.055332  [  960/ 4733]
[2022-08-07 17:22:05,104] loss: 0.080717  [ 1440/ 4733]
[2022-08-07 17:22:32,538] loss: 0.070535  [ 1920/ 4733]
[2022-08-07 17:22:59,971] loss: 0.080907  [ 2400/ 4733]
[2022-08-07 17:23:27,403] loss: 0.040260  [ 2880/ 4733]
[2022-08-07 17:23:54,836] loss: 0.022448  [ 3360/ 4733]
[2022-08-07 17:24:22,268] loss: 0.108937  [ 3840/ 4733]
[2022-08-07 17:24:49,699] loss: 0.016870  [ 4320/ 4733]
[2022-08-07 17:26:50,772] Train Error: Accuracy: 94.612%, Avg loss: 0.148338
[2022-08-07 17:27:33,804] Test  Error: Accuracy: 94.244%, Avg loss: 0.165388
[2022-08-07 17:27:33,804] Epoch 4---------------
[2022-08-07 17:27:33,806] lr: 1.260499e-03
[2022-08-07 17:27:35,637] loss: 0.059242  [    0/ 4733]
[2022-08-07 17:28:03,069] loss: 0.084111  [  480/ 4733]
[2022-08-07 17:28:30,503] loss: 0.050417  [  960/ 4733]
[2022-08-07 17:28:57,935] loss: 0.138899  [ 1440/ 4733]
[2022-08-07 17:29:25,368] loss: 0.084708  [ 1920/ 4733]
[2022-08-07 17:29:52,800] loss: 0.103326  [ 2400/ 4733]
[2022-08-07 17:30:20,232] loss: 0.067415  [ 2880/ 4733]
[2022-08-07 17:30:47,665] loss: 0.090327  [ 3360/ 4733]
[2022-08-07 17:31:15,099] loss: 0.017241  [ 3840/ 4733]
[2022-08-07 17:31:42,531] loss: 0.235246  [ 4320/ 4733]
[2022-08-07 17:33:43,605] Train Error: Accuracy: 98.225%, Avg loss: 0.054051
[2022-08-07 17:34:26,636] Test  Error: Accuracy: 97.268%, Avg loss: 0.077270
[2022-08-07 17:34:26,637] Epoch 5---------------
[2022-08-07 17:34:26,638] lr: 1.197474e-03
[2022-08-07 17:34:28,469] loss: 0.039861  [    0/ 4733]
[2022-08-07 17:34:55,901] loss: 0.026951  [  480/ 4733]
[2022-08-07 17:35:23,334] loss: 0.018632  [  960/ 4733]
[2022-08-07 17:35:50,767] loss: 0.002227  [ 1440/ 4733]
[2022-08-07 17:36:18,199] loss: 0.011586  [ 1920/ 4733]
[2022-08-07 17:36:45,631] loss: 0.013007  [ 2400/ 4733]
[2022-08-07 17:37:13,060] loss: 0.033986  [ 2880/ 4733]
[2022-08-07 17:37:40,490] loss: 0.003688  [ 3360/ 4733]
[2022-08-07 17:38:07,921] loss: 0.037705  [ 3840/ 4733]
[2022-08-07 17:38:35,352] loss: 0.101082  [ 4320/ 4733]
[2022-08-07 17:40:36,425] Train Error: Accuracy: 98.627%, Avg loss: 0.047670
[2022-08-07 17:41:19,456] Test  Error: Accuracy: 98.000%, Avg loss: 0.068391
[2022-08-07 17:41:19,456] Epoch 6---------------
[2022-08-07 17:41:19,457] lr: 1.137600e-03
[2022-08-07 17:41:21,288] loss: 0.008741  [    0/ 4733]
[2022-08-07 17:41:48,719] loss: 0.018688  [  480/ 4733]
[2022-08-07 17:42:16,150] loss: 0.012278  [  960/ 4733]
[2022-08-07 17:42:43,580] loss: 0.047280  [ 1440/ 4733]
[2022-08-07 17:43:11,013] loss: 0.109642  [ 1920/ 4733]
[2022-08-07 17:43:38,444] loss: 0.011470  [ 2400/ 4733]
[2022-08-07 17:44:05,875] loss: 0.020150  [ 2880/ 4733]
[2022-08-07 17:44:33,307] loss: 0.017485  [ 3360/ 4733]
[2022-08-07 17:45:00,739] loss: 0.118703  [ 3840/ 4733]
[2022-08-07 17:45:28,170] loss: 0.002513  [ 4320/ 4733]
[2022-08-07 17:47:29,242] Train Error: Accuracy: 99.408%, Avg loss: 0.021964
[2022-08-07 17:48:12,267] Test  Error: Accuracy: 98.683%, Avg loss: 0.047489
[2022-08-07 17:48:12,267] Epoch 7---------------
[2022-08-07 17:48:12,268] lr: 1.080720e-03
[2022-08-07 17:48:14,099] loss: 0.005657  [    0/ 4733]
[2022-08-07 17:48:41,532] loss: 0.002349  [  480/ 4733]
[2022-08-07 17:49:08,966] loss: 0.010203  [  960/ 4733]
[2022-08-07 17:49:36,400] loss: 0.017818  [ 1440/ 4733]
[2022-08-07 17:50:03,833] loss: 0.005225  [ 1920/ 4733]
[2022-08-07 17:50:31,267] loss: 0.021965  [ 2400/ 4733]
[2022-08-07 17:50:58,702] loss: 0.033967  [ 2880/ 4733]
[2022-08-07 17:51:26,133] loss: 0.017629  [ 3360/ 4733]
[2022-08-07 17:51:53,567] loss: 0.020669  [ 3840/ 4733]
[2022-08-07 17:52:20,999] loss: 0.017615  [ 4320/ 4733]
[2022-08-07 17:54:22,080] Train Error: Accuracy: 99.282%, Avg loss: 0.021574
[2022-08-07 17:55:05,109] Test  Error: Accuracy: 98.683%, Avg loss: 0.039373
[2022-08-07 17:55:05,109] Epoch 8---------------
[2022-08-07 17:55:05,110] lr: 1.026684e-03
[2022-08-07 17:55:06,941] loss: 0.067747  [    0/ 4733]
[2022-08-07 17:55:34,377] loss: 0.022880  [  480/ 4733]
[2022-08-07 17:56:01,810] loss: 0.050687  [  960/ 4733]
[2022-08-07 17:56:29,243] loss: 0.201366  [ 1440/ 4733]
[2022-08-07 17:56:56,677] loss: 0.001276  [ 1920/ 4733]
[2022-08-07 17:57:24,110] loss: 0.005961  [ 2400/ 4733]
[2022-08-07 17:57:51,542] loss: 0.047894  [ 2880/ 4733]
[2022-08-07 17:58:18,974] loss: 0.026569  [ 3360/ 4733]
[2022-08-07 17:58:46,407] loss: 0.008964  [ 3840/ 4733]
[2022-08-07 17:59:13,841] loss: 0.205444  [ 4320/ 4733]
[2022-08-07 18:01:14,921] Train Error: Accuracy: 99.599%, Avg loss: 0.014308
[2022-08-07 18:01:57,951] Test  Error: Accuracy: 98.829%, Avg loss: 0.040967
[2022-08-07 18:01:57,952] Epoch 9---------------
[2022-08-07 18:01:57,953] lr: 8.802533e-04
[2022-08-07 18:01:59,784] loss: 0.007613  [    0/ 4733]
[2022-08-07 18:02:27,216] loss: 0.003286  [  480/ 4733]
[2022-08-07 18:02:54,651] loss: 0.000934  [  960/ 4733]
[2022-08-07 18:03:22,082] loss: 0.030099  [ 1440/ 4733]
[2022-08-07 18:03:49,514] loss: 0.003623  [ 1920/ 4733]
[2022-08-07 18:04:16,946] loss: 0.032193  [ 2400/ 4733]
[2022-08-07 18:04:44,377] loss: 0.048218  [ 2880/ 4733]
[2022-08-07 18:05:11,808] loss: 0.004023  [ 3360/ 4733]
[2022-08-07 18:05:39,240] loss: 0.004988  [ 3840/ 4733]
[2022-08-07 18:06:06,673] loss: 0.044579  [ 4320/ 4733]
[2022-08-07 18:08:07,749] Train Error: Accuracy: 99.620%, Avg loss: 0.011724
[2022-08-07 18:08:50,774] Test  Error: Accuracy: 98.829%, Avg loss: 0.033601
[2022-08-07 18:08:50,775] Epoch 10---------------
[2022-08-07 18:08:50,775] lr: 8.362407e-04
[2022-08-07 18:08:52,606] loss: 0.006403  [    0/ 4733]
[2022-08-07 18:09:20,038] loss: 0.002264  [  480/ 4733]
[2022-08-07 18:09:47,468] loss: 0.001482  [  960/ 4733]
[2022-08-07 18:10:14,900] loss: 0.002368  [ 1440/ 4733]
[2022-08-07 18:10:42,330] loss: 0.000513  [ 1920/ 4733]
[2022-08-07 18:11:09,763] loss: 0.015494  [ 2400/ 4733]
[2022-08-07 18:11:37,195] loss: 0.007378  [ 2880/ 4733]
[2022-08-07 18:12:04,625] loss: 0.003612  [ 3360/ 4733]
[2022-08-07 18:12:32,058] loss: 0.000869  [ 3840/ 4733]
[2022-08-07 18:12:59,490] loss: 0.017652  [ 4320/ 4733]
[2022-08-07 18:15:00,562] Train Error: Accuracy: 98.859%, Avg loss: 0.029276
[2022-08-07 18:15:43,592] Test  Error: Accuracy: 98.293%, Avg loss: 0.042950
[2022-08-07 18:15:43,593] Epoch 11---------------
[2022-08-07 18:15:43,594] lr: 5.839780e-04
[2022-08-07 18:15:45,424] loss: 0.057844  [    0/ 4733]
[2022-08-07 18:16:12,858] loss: 0.008692  [  480/ 4733]
[2022-08-07 18:16:40,290] loss: 0.000905  [  960/ 4733]
[2022-08-07 18:17:07,721] loss: 0.008346  [ 1440/ 4733]
[2022-08-07 18:17:35,154] loss: 0.000882  [ 1920/ 4733]
[2022-08-07 18:18:02,587] loss: 0.003239  [ 2400/ 4733]
[2022-08-07 18:18:30,018] loss: 0.005581  [ 2880/ 4733]
[2022-08-07 18:18:57,451] loss: 0.002901  [ 3360/ 4733]
[2022-08-07 18:19:24,884] loss: 0.005405  [ 3840/ 4733]
[2022-08-07 18:19:52,317] loss: 0.001518  [ 4320/ 4733]
[2022-08-07 18:21:53,390] Train Error: Accuracy: 99.493%, Avg loss: 0.018064
[2022-08-07 18:22:36,418] Test  Error: Accuracy: 98.927%, Avg loss: 0.034487
[2022-08-07 18:22:36,419] Epoch 12---------------
[2022-08-07 18:22:36,420] lr: 5.547791e-04
[2022-08-07 18:22:38,251] loss: 0.035778  [    0/ 4733]
[2022-08-07 18:23:05,685] loss: 0.002858  [  480/ 4733]
[2022-08-07 18:23:33,118] loss: 0.004318  [  960/ 4733]
[2022-08-07 18:24:00,552] loss: 0.000760  [ 1440/ 4733]
[2022-08-07 18:24:27,984] loss: 0.002016  [ 1920/ 4733]
[2022-08-07 18:24:55,417] loss: 0.003331  [ 2400/ 4733]
[2022-08-07 18:25:22,849] loss: 0.004078  [ 2880/ 4733]
[2022-08-07 18:25:50,281] loss: 0.002285  [ 3360/ 4733]
[2022-08-07 18:26:17,714] loss: 0.001332  [ 3840/ 4733]
[2022-08-07 18:26:45,148] loss: 0.000488  [ 4320/ 4733]
[2022-08-07 18:28:46,221] Train Error: Accuracy: 99.873%, Avg loss: 0.005237
[2022-08-07 18:29:29,252] Test  Error: Accuracy: 99.317%, Avg loss: 0.027402
[2022-08-07 18:29:29,253] Epoch 13---------------
[2022-08-07 18:29:29,254] lr: 5.270402e-04
[2022-08-07 18:29:31,085] loss: 0.006202  [    0/ 4733]
[2022-08-07 18:29:58,519] loss: 0.003352  [  480/ 4733]
[2022-08-07 18:30:25,949] loss: 0.004794  [  960/ 4733]
[2022-08-07 18:30:53,381] loss: 0.068970  [ 1440/ 4733]
[2022-08-07 18:31:20,813] loss: 0.012412  [ 1920/ 4733]
[2022-08-07 18:31:48,246] loss: 0.001724  [ 2400/ 4733]
[2022-08-07 18:32:15,678] loss: 0.007352  [ 2880/ 4733]
[2022-08-07 18:32:43,110] loss: 0.002234  [ 3360/ 4733]
[2022-08-07 18:33:10,543] loss: 0.001441  [ 3840/ 4733]
[2022-08-07 18:33:37,976] loss: 0.003332  [ 4320/ 4733]
[2022-08-07 18:35:39,048] Train Error: Accuracy: 100.000%, Avg loss: 0.001978
[2022-08-07 18:36:22,078] Test  Error: Accuracy: 99.463%, Avg loss: 0.019633
[2022-08-07 18:36:22,078] Epoch 14---------------
[2022-08-07 18:36:22,078] lr: 5.006882e-04
[2022-08-07 18:36:23,909] loss: 0.000751  [    0/ 4733]
[2022-08-07 18:36:51,340] loss: 0.001212  [  480/ 4733]
[2022-08-07 18:37:18,771] loss: 0.001057  [  960/ 4733]
[2022-08-07 18:37:46,204] loss: 0.001124  [ 1440/ 4733]
[2022-08-07 18:38:13,633] loss: 0.000623  [ 1920/ 4733]
[2022-08-07 18:38:41,062] loss: 0.001516  [ 2400/ 4733]
[2022-08-07 18:39:08,494] loss: 0.000618  [ 2880/ 4733]
[2022-08-07 18:39:35,924] loss: 0.000563  [ 3360/ 4733]
[2022-08-07 18:40:03,356] loss: 0.000480  [ 3840/ 4733]
[2022-08-07 18:40:30,786] loss: 0.000866  [ 4320/ 4733]
[2022-08-07 18:42:31,859] Train Error: Accuracy: 99.915%, Avg loss: 0.003318
[2022-08-07 18:43:14,888] Test  Error: Accuracy: 99.512%, Avg loss: 0.022491
[2022-08-07 18:43:14,889] Epoch 15---------------
[2022-08-07 18:43:14,890] lr: 3.874230e-04
[2022-08-07 18:43:16,721] loss: 0.000970  [    0/ 4733]
[2022-08-07 18:43:44,153] loss: 0.001655  [  480/ 4733]
[2022-08-07 18:44:11,586] loss: 0.016127  [  960/ 4733]
[2022-08-07 18:44:39,018] loss: 0.014576  [ 1440/ 4733]
[2022-08-07 18:45:06,450] loss: 0.005251  [ 1920/ 4733]
[2022-08-07 18:45:33,883] loss: 0.000663  [ 2400/ 4733]
[2022-08-07 18:46:01,315] loss: 0.000798  [ 2880/ 4733]
[2022-08-07 18:46:28,747] loss: 0.000609  [ 3360/ 4733]
[2022-08-07 18:46:56,179] loss: 0.000977  [ 3840/ 4733]
[2022-08-07 18:47:23,613] loss: 0.002986  [ 4320/ 4733]
[2022-08-07 18:49:24,691] Train Error: Accuracy: 100.000%, Avg loss: 0.001184
[2022-08-07 18:50:07,732] Test  Error: Accuracy: 99.415%, Avg loss: 0.019180
[2022-08-07 18:50:07,732] Done!
[2022-08-07 18:50:07,736] Number of parameters:3229450
[2022-08-07 18:50:07,736] ## end time: 2022-08-07 18:50:07.732925
[2022-08-07 18:50:07,737] ## used time: 1:43:13.172554
