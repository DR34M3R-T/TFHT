[2022-08-08 02:31:30,185] ## start time: 2022-08-08 02:31:30.046666
[2022-08-08 02:31:30,186] Using cuda device
[2022-08-08 02:31:30,187] In train:p&d10.npy.
[2022-08-08 02:31:30,188] One Channel
[2022-08-08 02:31:30,188] With Normal data.
[2022-08-08 02:31:30,189] Nunber of classes:10.
[2022-08-08 02:31:30,189] Nunber of ViT channels:1.
[2022-08-08 02:31:30,433] Totol epochs: 15
[2022-08-08 02:31:30,435] Epoch 1---------------
[2022-08-08 02:31:30,435] lr: 2.000000e-03
[2022-08-08 02:31:30,481] loss: 2.320303  [    0/ 4719]
[2022-08-08 02:31:31,029] loss: 1.874098  [  480/ 4719]
[2022-08-08 02:31:31,592] loss: 2.060923  [  960/ 4719]
[2022-08-08 02:31:32,128] loss: 1.854292  [ 1440/ 4719]
[2022-08-08 02:31:32,660] loss: 0.444632  [ 1920/ 4719]
[2022-08-08 02:31:33,194] loss: 0.123886  [ 2400/ 4719]
[2022-08-08 02:31:33,729] loss: 0.077642  [ 2880/ 4719]
[2022-08-08 02:31:34,262] loss: 0.814228  [ 3360/ 4719]
[2022-08-08 02:31:34,793] loss: 0.021764  [ 3840/ 4719]
[2022-08-08 02:31:35,321] loss: 0.049056  [ 4320/ 4719]
[2022-08-08 02:31:37,112] Train Error: Accuracy: 99.894%, Avg loss: 0.014054
[2022-08-08 02:31:37,725] Test  Error: Accuracy: 99.661%, Avg loss: 0.020020
[2022-08-08 02:31:37,725] Epoch 2---------------
[2022-08-08 02:31:37,726] lr: 1.900000e-03
[2022-08-08 02:31:37,766] loss: 0.009724  [    0/ 4719]
[2022-08-08 02:31:38,302] loss: 0.006786  [  480/ 4719]
[2022-08-08 02:31:38,833] loss: 0.075981  [  960/ 4719]
[2022-08-08 02:31:39,364] loss: 0.006463  [ 1440/ 4719]
[2022-08-08 02:31:39,896] loss: 0.007213  [ 1920/ 4719]
[2022-08-08 02:31:40,423] loss: 0.003315  [ 2400/ 4719]
[2022-08-08 02:31:40,953] loss: 0.005523  [ 2880/ 4719]
[2022-08-08 02:31:41,482] loss: 0.003264  [ 3360/ 4719]
[2022-08-08 02:31:42,012] loss: 0.003552  [ 3840/ 4719]
[2022-08-08 02:31:42,539] loss: 0.003017  [ 4320/ 4719]
[2022-08-08 02:31:44,329] Train Error: Accuracy: 100.000%, Avg loss: 0.002616
[2022-08-08 02:31:44,936] Test  Error: Accuracy: 99.952%, Avg loss: 0.003896
[2022-08-08 02:31:44,936] Epoch 3---------------
[2022-08-08 02:31:44,937] lr: 1.805000e-03
[2022-08-08 02:31:44,976] loss: 0.003144  [    0/ 4719]
[2022-08-08 02:31:45,503] loss: 0.002154  [  480/ 4719]
[2022-08-08 02:31:46,036] loss: 0.001905  [  960/ 4719]
[2022-08-08 02:31:46,568] loss: 0.002612  [ 1440/ 4719]
[2022-08-08 02:31:47,104] loss: 0.002268  [ 1920/ 4719]
[2022-08-08 02:31:47,636] loss: 0.001678  [ 2400/ 4719]
[2022-08-08 02:31:48,165] loss: 0.001697  [ 2880/ 4719]
[2022-08-08 02:31:48,691] loss: 0.002263  [ 3360/ 4719]
[2022-08-08 02:31:49,221] loss: 0.001311  [ 3840/ 4719]
[2022-08-08 02:31:49,749] loss: 0.001162  [ 4320/ 4719]
[2022-08-08 02:31:51,544] Train Error: Accuracy: 99.979%, Avg loss: 0.001968
[2022-08-08 02:31:52,149] Test  Error: Accuracy: 99.903%, Avg loss: 0.005019
[2022-08-08 02:31:52,149] Epoch 4---------------
[2022-08-08 02:31:52,150] lr: 1.260499e-03
[2022-08-08 02:31:52,189] loss: 0.001078  [    0/ 4719]
[2022-08-08 02:31:52,727] loss: 0.001785  [  480/ 4719]
[2022-08-08 02:31:53,263] loss: 0.001521  [  960/ 4719]
[2022-08-08 02:31:53,800] loss: 0.001104  [ 1440/ 4719]
[2022-08-08 02:31:54,343] loss: 0.001610  [ 1920/ 4719]
[2022-08-08 02:31:54,876] loss: 0.000879  [ 2400/ 4719]
[2022-08-08 02:31:55,408] loss: 0.001231  [ 2880/ 4719]
[2022-08-08 02:31:55,939] loss: 0.000915  [ 3360/ 4719]
[2022-08-08 02:31:56,471] loss: 0.000881  [ 3840/ 4719]
[2022-08-08 02:31:57,001] loss: 0.001194  [ 4320/ 4719]
[2022-08-08 02:31:58,803] Train Error: Accuracy: 100.000%, Avg loss: 0.001008
[2022-08-08 02:31:59,402] Test  Error: Accuracy: 99.952%, Avg loss: 0.002164
[2022-08-08 02:31:59,402] Epoch 5---------------
[2022-08-08 02:31:59,403] lr: 1.197474e-03
[2022-08-08 02:31:59,442] loss: 0.001116  [    0/ 4719]
[2022-08-08 02:31:59,980] loss: 0.001014  [  480/ 4719]
[2022-08-08 02:32:00,511] loss: 0.000957  [  960/ 4719]
[2022-08-08 02:32:01,045] loss: 0.001182  [ 1440/ 4719]
[2022-08-08 02:32:01,576] loss: 0.001090  [ 1920/ 4719]
[2022-08-08 02:32:02,105] loss: 0.000930  [ 2400/ 4719]
[2022-08-08 02:32:02,636] loss: 0.001018  [ 2880/ 4719]
[2022-08-08 02:32:03,164] loss: 0.000824  [ 3360/ 4719]
[2022-08-08 02:32:03,693] loss: 0.000802  [ 3840/ 4719]
[2022-08-08 02:32:04,223] loss: 0.000986  [ 4320/ 4719]
[2022-08-08 02:32:06,020] Train Error: Accuracy: 100.000%, Avg loss: 0.000820
[2022-08-08 02:32:06,622] Test  Error: Accuracy: 99.903%, Avg loss: 0.001777
[2022-08-08 02:32:06,622] Epoch 6---------------
[2022-08-08 02:32:06,623] lr: 1.137600e-03
[2022-08-08 02:32:06,661] loss: 0.000676  [    0/ 4719]
[2022-08-08 02:32:07,196] loss: 0.000847  [  480/ 4719]
[2022-08-08 02:32:07,726] loss: 0.000737  [  960/ 4719]
[2022-08-08 02:32:08,259] loss: 0.000743  [ 1440/ 4719]
[2022-08-08 02:32:08,786] loss: 0.000901  [ 1920/ 4719]
[2022-08-08 02:32:09,313] loss: 0.000684  [ 2400/ 4719]
[2022-08-08 02:32:09,841] loss: 0.000684  [ 2880/ 4719]
[2022-08-08 02:32:10,369] loss: 0.000732  [ 3360/ 4719]
[2022-08-08 02:32:10,895] loss: 0.000685  [ 3840/ 4719]
[2022-08-08 02:32:11,427] loss: 0.000893  [ 4320/ 4719]
[2022-08-08 02:32:13,223] Train Error: Accuracy: 100.000%, Avg loss: 0.000740
[2022-08-08 02:32:13,832] Test  Error: Accuracy: 100.000%, Avg loss: 0.001229
[2022-08-08 02:32:13,832] Epoch 7---------------
[2022-08-08 02:32:13,833] lr: 1.080720e-03
[2022-08-08 02:32:13,876] loss: 0.000974  [    0/ 4719]
[2022-08-08 02:32:14,430] loss: 0.001139  [  480/ 4719]
[2022-08-08 02:32:14,969] loss: 0.000549  [  960/ 4719]
[2022-08-08 02:32:15,497] loss: 0.000793  [ 1440/ 4719]
[2022-08-08 02:32:16,026] loss: 0.000588  [ 1920/ 4719]
[2022-08-08 02:32:16,555] loss: 0.000579  [ 2400/ 4719]
[2022-08-08 02:32:17,084] loss: 0.000665  [ 2880/ 4719]
[2022-08-08 02:32:17,611] loss: 0.000642  [ 3360/ 4719]
[2022-08-08 02:32:18,138] loss: 0.000592  [ 3840/ 4719]
[2022-08-08 02:32:18,669] loss: 0.000648  [ 4320/ 4719]
[2022-08-08 02:32:20,471] Train Error: Accuracy: 100.000%, Avg loss: 0.000637
[2022-08-08 02:32:21,089] Test  Error: Accuracy: 99.952%, Avg loss: 0.001670
[2022-08-08 02:32:21,089] Epoch 8---------------
[2022-08-08 02:32:21,090] lr: 7.547072e-04
[2022-08-08 02:32:21,128] loss: 0.000535  [    0/ 4719]
[2022-08-08 02:32:21,659] loss: 0.000562  [  480/ 4719]
[2022-08-08 02:32:22,194] loss: 0.000741  [  960/ 4719]
[2022-08-08 02:32:22,724] loss: 0.000629  [ 1440/ 4719]
[2022-08-08 02:32:23,253] loss: 0.000788  [ 1920/ 4719]
[2022-08-08 02:32:23,779] loss: 0.000699  [ 2400/ 4719]
[2022-08-08 02:32:24,306] loss: 0.000898  [ 2880/ 4719]
[2022-08-08 02:32:24,835] loss: 0.000617  [ 3360/ 4719]
[2022-08-08 02:32:25,365] loss: 0.000498  [ 3840/ 4719]
[2022-08-08 02:32:25,902] loss: 0.000745  [ 4320/ 4719]
[2022-08-08 02:32:27,700] Train Error: Accuracy: 100.000%, Avg loss: 0.000592
[2022-08-08 02:32:28,301] Test  Error: Accuracy: 99.952%, Avg loss: 0.001389
[2022-08-08 02:32:28,301] Epoch 9---------------
[2022-08-08 02:32:28,302] lr: 7.169718e-04
[2022-08-08 02:32:28,340] loss: 0.000550  [    0/ 4719]
[2022-08-08 02:32:28,869] loss: 0.000461  [  480/ 4719]
[2022-08-08 02:32:29,404] loss: 0.000592  [  960/ 4719]
[2022-08-08 02:32:29,938] loss: 0.000512  [ 1440/ 4719]
[2022-08-08 02:32:30,471] loss: 0.000712  [ 1920/ 4719]
[2022-08-08 02:32:31,005] loss: 0.000504  [ 2400/ 4719]
[2022-08-08 02:32:31,538] loss: 0.000621  [ 2880/ 4719]
[2022-08-08 02:32:32,072] loss: 0.000748  [ 3360/ 4719]
[2022-08-08 02:32:32,604] loss: 0.000420  [ 3840/ 4719]
[2022-08-08 02:32:33,132] loss: 0.000656  [ 4320/ 4719]
[2022-08-08 02:32:34,938] Train Error: Accuracy: 100.000%, Avg loss: 0.000558
[2022-08-08 02:32:35,542] Test  Error: Accuracy: 99.952%, Avg loss: 0.002149
[2022-08-08 02:32:35,542] Epoch 10---------------
[2022-08-08 02:32:35,543] lr: 5.006882e-04
[2022-08-08 02:32:35,580] loss: 0.000490  [    0/ 4719]
[2022-08-08 02:32:36,125] loss: 0.000640  [  480/ 4719]
[2022-08-08 02:32:36,657] loss: 0.000601  [  960/ 4719]
[2022-08-08 02:32:37,192] loss: 0.000552  [ 1440/ 4719]
[2022-08-08 02:32:37,719] loss: 0.000511  [ 1920/ 4719]
[2022-08-08 02:32:38,251] loss: 0.000625  [ 2400/ 4719]
[2022-08-08 02:32:38,778] loss: 0.001557  [ 2880/ 4719]
[2022-08-08 02:32:39,306] loss: 0.000607  [ 3360/ 4719]
[2022-08-08 02:32:39,833] loss: 0.000537  [ 3840/ 4719]
[2022-08-08 02:32:40,363] loss: 0.000544  [ 4320/ 4719]
[2022-08-08 02:32:42,154] Train Error: Accuracy: 100.000%, Avg loss: 0.000526
[2022-08-08 02:32:42,758] Test  Error: Accuracy: 99.952%, Avg loss: 0.001815
[2022-08-08 02:32:42,759] Epoch 11---------------
[2022-08-08 02:32:42,761] lr: 4.756538e-04
[2022-08-08 02:32:42,799] loss: 0.000750  [    0/ 4719]
[2022-08-08 02:32:43,327] loss: 0.000428  [  480/ 4719]
[2022-08-08 02:32:43,862] loss: 0.000848  [  960/ 4719]
[2022-08-08 02:32:44,394] loss: 0.000414  [ 1440/ 4719]
[2022-08-08 02:32:44,932] loss: 0.000626  [ 1920/ 4719]
[2022-08-08 02:32:45,457] loss: 0.000472  [ 2400/ 4719]
[2022-08-08 02:32:45,989] loss: 0.000521  [ 2880/ 4719]
[2022-08-08 02:32:46,535] loss: 0.000645  [ 3360/ 4719]
[2022-08-08 02:32:47,066] loss: 0.000447  [ 3840/ 4719]
[2022-08-08 02:32:47,592] loss: 0.000489  [ 4320/ 4719]
[2022-08-08 02:32:49,403] Train Error: Accuracy: 100.000%, Avg loss: 0.000512
[2022-08-08 02:32:50,006] Test  Error: Accuracy: 99.952%, Avg loss: 0.001478
[2022-08-08 02:32:50,006] Epoch 12---------------
[2022-08-08 02:32:50,007] lr: 4.518711e-04
[2022-08-08 02:32:50,046] loss: 0.000591  [    0/ 4719]
[2022-08-08 02:32:50,579] loss: 0.000595  [  480/ 4719]
[2022-08-08 02:32:51,109] loss: 0.000551  [  960/ 4719]
[2022-08-08 02:32:51,633] loss: 0.000549  [ 1440/ 4719]
[2022-08-08 02:32:52,163] loss: 0.000489  [ 1920/ 4719]
[2022-08-08 02:32:52,687] loss: 0.000484  [ 2400/ 4719]
[2022-08-08 02:32:53,222] loss: 0.000510  [ 2880/ 4719]
[2022-08-08 02:32:53,751] loss: 0.000509  [ 3360/ 4719]
[2022-08-08 02:32:54,281] loss: 0.000464  [ 3840/ 4719]
[2022-08-08 02:32:54,809] loss: 0.000503  [ 4320/ 4719]
[2022-08-08 02:32:56,615] Train Error: Accuracy: 100.000%, Avg loss: 0.000497
[2022-08-08 02:32:57,218] Test  Error: Accuracy: 100.000%, Avg loss: 0.001091
[2022-08-08 02:32:57,218] Epoch 13---------------
[2022-08-08 02:32:57,219] lr: 4.292775e-04
[2022-08-08 02:32:57,257] loss: 0.000685  [    0/ 4719]
[2022-08-08 02:32:57,790] loss: 0.000443  [  480/ 4719]
[2022-08-08 02:32:58,322] loss: 0.000432  [  960/ 4719]
[2022-08-08 02:32:58,856] loss: 0.000393  [ 1440/ 4719]
[2022-08-08 02:32:59,388] loss: 0.000591  [ 1920/ 4719]
[2022-08-08 02:32:59,917] loss: 0.000603  [ 2400/ 4719]
[2022-08-08 02:33:00,446] loss: 0.000407  [ 2880/ 4719]
[2022-08-08 02:33:00,974] loss: 0.000479  [ 3360/ 4719]
[2022-08-08 02:33:01,499] loss: 0.000502  [ 3840/ 4719]
[2022-08-08 02:33:02,026] loss: 0.000463  [ 4320/ 4719]
[2022-08-08 02:33:03,820] Train Error: Accuracy: 100.000%, Avg loss: 0.000466
[2022-08-08 02:33:04,426] Test  Error: Accuracy: 99.903%, Avg loss: 0.001620
[2022-08-08 02:33:04,427] Epoch 14---------------
[2022-08-08 02:33:04,427] lr: 2.997805e-04
[2022-08-08 02:33:04,465] loss: 0.000453  [    0/ 4719]
[2022-08-08 02:33:04,996] loss: 0.000441  [  480/ 4719]
[2022-08-08 02:33:05,527] loss: 0.000411  [  960/ 4719]
[2022-08-08 02:33:06,058] loss: 0.000427  [ 1440/ 4719]
[2022-08-08 02:33:06,589] loss: 0.000437  [ 1920/ 4719]
[2022-08-08 02:33:07,119] loss: 0.000380  [ 2400/ 4719]
[2022-08-08 02:33:07,646] loss: 0.000561  [ 2880/ 4719]
[2022-08-08 02:33:08,174] loss: 0.000410  [ 3360/ 4719]
[2022-08-08 02:33:08,701] loss: 0.000467  [ 3840/ 4719]
[2022-08-08 02:33:09,230] loss: 0.000450  [ 4320/ 4719]
[2022-08-08 02:33:11,023] Train Error: Accuracy: 100.000%, Avg loss: 0.000455
[2022-08-08 02:33:11,630] Test  Error: Accuracy: 99.952%, Avg loss: 0.001244
[2022-08-08 02:33:11,631] Epoch 15---------------
[2022-08-08 02:33:11,632] lr: 2.847915e-04
[2022-08-08 02:33:11,669] loss: 0.000415  [    0/ 4719]
[2022-08-08 02:33:12,208] loss: 0.000396  [  480/ 4719]
[2022-08-08 02:33:12,741] loss: 0.000456  [  960/ 4719]
[2022-08-08 02:33:13,276] loss: 0.000387  [ 1440/ 4719]
[2022-08-08 02:33:13,808] loss: 0.000516  [ 1920/ 4719]
[2022-08-08 02:33:14,347] loss: 0.000435  [ 2400/ 4719]
[2022-08-08 02:33:14,876] loss: 0.000535  [ 2880/ 4719]
[2022-08-08 02:33:15,401] loss: 0.000489  [ 3360/ 4719]
[2022-08-08 02:33:15,928] loss: 0.000461  [ 3840/ 4719]
[2022-08-08 02:33:16,463] loss: 0.000625  [ 4320/ 4719]
[2022-08-08 02:33:18,278] Train Error: Accuracy: 100.000%, Avg loss: 0.000456
[2022-08-08 02:33:18,883] Test  Error: Accuracy: 99.903%, Avg loss: 0.001498
[2022-08-08 02:33:18,883] Done!
[2022-08-08 02:33:18,887] Number of parameters:3424522
[2022-08-08 02:33:18,888] ## end time: 2022-08-08 02:33:18.883573
[2022-08-08 02:33:18,888] ## used time: 0:01:48.836907
