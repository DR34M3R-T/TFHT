[2022-08-06 17:00:13,668] ## start time: 2022-08-06 17:00:13.533423
[2022-08-06 17:00:13,669] Using cuda device
[2022-08-06 17:00:13,670] In train:p&d10.npy.
[2022-08-06 17:00:13,671] One Channel
[2022-08-06 17:00:13,671] With Normal data.
[2022-08-06 17:00:13,672] Nunber of classes:10.
[2022-08-06 17:00:13,672] Nunber of ViT channels:1.
[2022-08-06 17:00:13,903] Totol epochs: 15
[2022-08-06 17:00:13,906] Epoch 1---------------
[2022-08-06 17:00:13,906] lr: 2.000000e-03
[2022-08-06 17:00:14,622] loss: 2.715418  [    0/ 4747]
[2022-08-06 17:00:25,341] loss: 1.840358  [  960/ 4747]
[2022-08-06 17:00:36,060] loss: 1.512135  [ 1920/ 4747]
[2022-08-06 17:00:46,779] loss: 1.096065  [ 2880/ 4747]
[2022-08-06 17:00:57,497] loss: 0.883841  [ 3840/ 4747]
[2022-08-06 17:01:26,687] Train Error: Accuracy: 54.771%, Avg loss: 1.203375
[2022-08-06 17:01:35,159] Test  Error: Accuracy: 53.487%, Avg loss: 1.252454
[2022-08-06 17:01:35,160] Epoch 2---------------
[2022-08-06 17:01:35,161] lr: 1.900000e-03
[2022-08-06 17:01:35,878] loss: 1.303761  [    0/ 4747]
[2022-08-06 17:01:46,595] loss: 0.779739  [  960/ 4747]
[2022-08-06 17:01:57,313] loss: 0.337584  [ 1920/ 4747]
[2022-08-06 17:02:08,030] loss: 0.791160  [ 2880/ 4747]
[2022-08-06 17:02:18,748] loss: 0.112229  [ 3840/ 4747]
[2022-08-06 17:02:47,930] Train Error: Accuracy: 85.486%, Avg loss: 0.430125
[2022-08-06 17:02:56,400] Test  Error: Accuracy: 84.234%, Avg loss: 0.471709
[2022-08-06 17:02:56,401] Epoch 3---------------
[2022-08-06 17:02:56,401] lr: 1.805000e-03
[2022-08-06 17:02:57,117] loss: 0.573542  [    0/ 4747]
[2022-08-06 17:03:07,835] loss: 0.150205  [  960/ 4747]
[2022-08-06 17:03:18,552] loss: 0.202723  [ 1920/ 4747]
[2022-08-06 17:03:29,269] loss: 0.190990  [ 2880/ 4747]
[2022-08-06 17:03:39,986] loss: 0.318648  [ 3840/ 4747]
[2022-08-06 17:04:09,156] Train Error: Accuracy: 97.451%, Avg loss: 0.093736
[2022-08-06 17:04:17,624] Test  Error: Accuracy: 97.102%, Avg loss: 0.103310
[2022-08-06 17:04:17,624] Epoch 4---------------
[2022-08-06 17:04:17,625] lr: 1.714750e-03
[2022-08-06 17:04:18,343] loss: 0.185566  [    0/ 4747]
[2022-08-06 17:04:29,060] loss: 0.070938  [  960/ 4747]
[2022-08-06 17:04:39,779] loss: 0.059880  [ 1920/ 4747]
[2022-08-06 17:04:50,495] loss: 0.088520  [ 2880/ 4747]
[2022-08-06 17:05:01,212] loss: 0.115101  [ 3840/ 4747]
[2022-08-06 17:05:30,381] Train Error: Accuracy: 85.212%, Avg loss: 0.577312
[2022-08-06 17:05:38,846] Test  Error: Accuracy: 84.086%, Avg loss: 0.652282
[2022-08-06 17:05:38,846] Epoch 5---------------
[2022-08-06 17:05:38,847] lr: 1.197474e-03
[2022-08-06 17:05:39,564] loss: 0.698474  [    0/ 4747]
[2022-08-06 17:05:50,282] loss: 0.048339  [  960/ 4747]
[2022-08-06 17:06:01,000] loss: 0.108110  [ 1920/ 4747]
[2022-08-06 17:06:11,717] loss: 0.020528  [ 2880/ 4747]
[2022-08-06 17:06:22,435] loss: 0.018860  [ 3840/ 4747]
[2022-08-06 17:06:51,603] Train Error: Accuracy: 92.964%, Avg loss: 0.224970
[2022-08-06 17:07:00,067] Test  Error: Accuracy: 92.387%, Avg loss: 0.280169
[2022-08-06 17:07:00,068] Epoch 6---------------
[2022-08-06 17:07:00,069] lr: 1.137600e-03
[2022-08-06 17:07:00,786] loss: 0.168345  [    0/ 4747]
[2022-08-06 17:07:11,504] loss: 0.096191  [  960/ 4747]
[2022-08-06 17:07:22,221] loss: 0.085724  [ 1920/ 4747]
[2022-08-06 17:07:32,938] loss: 0.018509  [ 2880/ 4747]
[2022-08-06 17:07:43,655] loss: 0.033916  [ 3840/ 4747]
[2022-08-06 17:08:12,825] Train Error: Accuracy: 99.115%, Avg loss: 0.031179
[2022-08-06 17:08:21,290] Test  Error: Accuracy: 98.870%, Avg loss: 0.036203
[2022-08-06 17:08:21,291] Epoch 7---------------
[2022-08-06 17:08:21,293] lr: 1.080720e-03
[2022-08-06 17:08:22,010] loss: 0.008416  [    0/ 4747]
[2022-08-06 17:08:32,727] loss: 0.004949  [  960/ 4747]
[2022-08-06 17:08:43,446] loss: 0.008064  [ 1920/ 4747]
[2022-08-06 17:08:54,164] loss: 0.013915  [ 2880/ 4747]
[2022-08-06 17:09:04,880] loss: 0.008256  [ 3840/ 4747]
[2022-08-06 17:09:34,051] Train Error: Accuracy: 99.621%, Avg loss: 0.015467
[2022-08-06 17:09:42,515] Test  Error: Accuracy: 99.067%, Avg loss: 0.028823
[2022-08-06 17:09:42,516] Epoch 8---------------
[2022-08-06 17:09:42,517] lr: 1.026684e-03
[2022-08-06 17:09:43,234] loss: 0.029342  [    0/ 4747]
[2022-08-06 17:09:53,952] loss: 0.087399  [  960/ 4747]
[2022-08-06 17:10:04,671] loss: 0.012793  [ 1920/ 4747]
[2022-08-06 17:10:15,389] loss: 0.014388  [ 2880/ 4747]
[2022-08-06 17:10:26,106] loss: 0.044759  [ 3840/ 4747]
[2022-08-06 17:10:55,274] Train Error: Accuracy: 99.579%, Avg loss: 0.014291
[2022-08-06 17:11:03,741] Test  Error: Accuracy: 99.558%, Avg loss: 0.019069
[2022-08-06 17:11:03,741] Epoch 9---------------
[2022-08-06 17:11:03,742] lr: 9.753500e-04
[2022-08-06 17:11:04,459] loss: 0.007302  [    0/ 4747]
[2022-08-06 17:11:15,177] loss: 0.047663  [  960/ 4747]
[2022-08-06 17:11:25,896] loss: 0.053569  [ 1920/ 4747]
[2022-08-06 17:11:36,613] loss: 0.047551  [ 2880/ 4747]
[2022-08-06 17:11:47,330] loss: 0.005752  [ 3840/ 4747]
[2022-08-06 17:12:16,510] Train Error: Accuracy: 99.452%, Avg loss: 0.018753
[2022-08-06 17:12:24,976] Test  Error: Accuracy: 99.312%, Avg loss: 0.023024
[2022-08-06 17:12:24,977] Epoch 10---------------
[2022-08-06 17:12:24,978] lr: 7.547072e-04
[2022-08-06 17:12:25,696] loss: 0.032651  [    0/ 4747]
[2022-08-06 17:12:36,414] loss: 0.004655  [  960/ 4747]
[2022-08-06 17:12:47,130] loss: 0.001487  [ 1920/ 4747]
[2022-08-06 17:12:57,849] loss: 0.028806  [ 2880/ 4747]
[2022-08-06 17:13:08,567] loss: 0.006654  [ 3840/ 4747]
[2022-08-06 17:13:37,741] Train Error: Accuracy: 99.663%, Avg loss: 0.012758
[2022-08-06 17:13:46,211] Test  Error: Accuracy: 99.411%, Avg loss: 0.019578
[2022-08-06 17:13:46,211] Epoch 11---------------
[2022-08-06 17:13:46,212] lr: 7.169718e-04
[2022-08-06 17:13:46,929] loss: 0.007761  [    0/ 4747]
[2022-08-06 17:13:57,649] loss: 0.006618  [  960/ 4747]
[2022-08-06 17:14:08,367] loss: 0.002772  [ 1920/ 4747]
[2022-08-06 17:14:19,087] loss: 0.006697  [ 2880/ 4747]
[2022-08-06 17:14:29,807] loss: 0.003936  [ 3840/ 4747]
[2022-08-06 17:14:58,991] Train Error: Accuracy: 99.895%, Avg loss: 0.007850
[2022-08-06 17:15:07,461] Test  Error: Accuracy: 99.509%, Avg loss: 0.013713
[2022-08-06 17:15:07,462] Epoch 12---------------
[2022-08-06 17:15:07,463] lr: 6.811233e-04
[2022-08-06 17:15:08,180] loss: 0.002383  [    0/ 4747]
[2022-08-06 17:15:18,897] loss: 0.003253  [  960/ 4747]
[2022-08-06 17:15:29,615] loss: 0.002622  [ 1920/ 4747]
[2022-08-06 17:15:40,333] loss: 0.003438  [ 2880/ 4747]
[2022-08-06 17:15:51,050] loss: 0.002165  [ 3840/ 4747]
[2022-08-06 17:16:20,241] Train Error: Accuracy: 99.663%, Avg loss: 0.011340
[2022-08-06 17:16:28,713] Test  Error: Accuracy: 99.312%, Avg loss: 0.024080
[2022-08-06 17:16:28,714] Epoch 13---------------
[2022-08-06 17:16:28,715] lr: 4.756538e-04
[2022-08-06 17:16:29,432] loss: 0.004718  [    0/ 4747]
[2022-08-06 17:16:40,148] loss: 0.001504  [  960/ 4747]
[2022-08-06 17:16:50,865] loss: 0.004923  [ 1920/ 4747]
[2022-08-06 17:17:01,581] loss: 0.016547  [ 2880/ 4747]
[2022-08-06 17:17:12,299] loss: 0.001580  [ 3840/ 4747]
[2022-08-06 17:17:41,479] Train Error: Accuracy: 99.831%, Avg loss: 0.007283
[2022-08-06 17:17:49,949] Test  Error: Accuracy: 99.411%, Avg loss: 0.019523
[2022-08-06 17:17:49,949] Epoch 14---------------
[2022-08-06 17:17:49,950] lr: 4.518711e-04
[2022-08-06 17:17:50,667] loss: 0.005387  [    0/ 4747]
[2022-08-06 17:18:01,385] loss: 0.001246  [  960/ 4747]
[2022-08-06 17:18:12,103] loss: 0.004548  [ 1920/ 4747]
[2022-08-06 17:18:22,821] loss: 0.001164  [ 2880/ 4747]
[2022-08-06 17:18:33,540] loss: 0.001050  [ 3840/ 4747]
[2022-08-06 17:19:02,717] Train Error: Accuracy: 100.000%, Avg loss: 0.001932
[2022-08-06 17:19:11,192] Test  Error: Accuracy: 99.607%, Avg loss: 0.011455
[2022-08-06 17:19:11,193] Epoch 15---------------
[2022-08-06 17:19:11,194] lr: 4.292775e-04
[2022-08-06 17:19:11,911] loss: 0.003051  [    0/ 4747]
[2022-08-06 17:19:22,629] loss: 0.001560  [  960/ 4747]
[2022-08-06 17:19:33,349] loss: 0.010630  [ 1920/ 4747]
[2022-08-06 17:19:44,068] loss: 0.017932  [ 2880/ 4747]
[2022-08-06 17:19:54,785] loss: 0.001375  [ 3840/ 4747]
[2022-08-06 17:20:23,968] Train Error: Accuracy: 99.642%, Avg loss: 0.011961
[2022-08-06 17:20:32,439] Test  Error: Accuracy: 99.411%, Avg loss: 0.018352
[2022-08-06 17:20:32,439] Done!
[2022-08-06 17:20:32,444] Number of parameters:1625866
[2022-08-06 17:20:32,444] ## end time: 2022-08-06 17:20:32.439772
[2022-08-06 17:20:32,444] ## used time: 0:20:18.906349
