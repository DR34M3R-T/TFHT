[2022-08-08 21:16:35,049] ## start time: 2022-08-08 21:16:34.878574
[2022-08-08 21:16:35,050] Using cuda device
[2022-08-08 21:16:35,051] In train:p&d10.npy.
[2022-08-08 21:16:35,053] One Channel
[2022-08-08 21:16:35,054] With Normal data.
[2022-08-08 21:16:35,054] Nunber of classes:10.
[2022-08-08 21:16:35,055] Nunber of ViT channels:1.
[2022-08-08 21:16:35,355] Totol epochs: 15
[2022-08-08 21:16:35,358] Epoch 1---------------
[2022-08-08 21:16:35,359] lr: 2.000000e-03
[2022-08-08 21:16:36,672] loss: 2.371233  [    0/ 4779]
[2022-08-08 21:16:56,346] loss: 1.964218  [  480/ 4779]
[2022-08-08 21:17:16,019] loss: 1.854574  [  960/ 4779]
[2022-08-08 21:17:35,691] loss: 1.556482  [ 1440/ 4779]
[2022-08-08 21:17:55,365] loss: 1.384509  [ 1920/ 4779]
[2022-08-08 21:18:15,038] loss: 1.341438  [ 2400/ 4779]
[2022-08-08 21:18:34,712] loss: 1.578730  [ 2880/ 4779]
[2022-08-08 21:18:54,385] loss: 1.429950  [ 3360/ 4779]
[2022-08-08 21:19:14,058] loss: 0.863959  [ 3840/ 4779]
[2022-08-08 21:19:33,732] loss: 1.088674  [ 4320/ 4779]
[2022-08-08 21:21:04,586] Train Error: Accuracy: 56.706%, Avg loss: 1.496513
[2022-08-08 21:21:35,347] Test  Error: Accuracy: 53.892%, Avg loss: 1.639358
[2022-08-08 21:21:35,347] Epoch 2---------------
[2022-08-08 21:21:35,349] lr: 1.900000e-03
[2022-08-08 21:21:36,662] loss: 1.292528  [    0/ 4779]
[2022-08-08 21:21:56,334] loss: 0.569140  [  480/ 4779]
[2022-08-08 21:22:16,008] loss: 0.425457  [  960/ 4779]
[2022-08-08 21:22:35,682] loss: 0.891253  [ 1440/ 4779]
[2022-08-08 21:22:55,355] loss: 0.577714  [ 1920/ 4779]
[2022-08-08 21:23:15,027] loss: 0.141387  [ 2400/ 4779]
[2022-08-08 21:23:34,701] loss: 0.187155  [ 2880/ 4779]
[2022-08-08 21:23:54,374] loss: 0.275360  [ 3360/ 4779]
[2022-08-08 21:24:14,047] loss: 0.331416  [ 3840/ 4779]
[2022-08-08 21:24:33,719] loss: 0.165713  [ 4320/ 4779]
[2022-08-08 21:26:04,574] Train Error: Accuracy: 78.406%, Avg loss: 0.682506
[2022-08-08 21:26:35,330] Test  Error: Accuracy: 77.295%, Avg loss: 0.710442
[2022-08-08 21:26:35,330] Epoch 3---------------
[2022-08-08 21:26:35,331] lr: 1.805000e-03
[2022-08-08 21:26:36,645] loss: 0.637318  [    0/ 4779]
[2022-08-08 21:26:56,318] loss: 0.214366  [  480/ 4779]
[2022-08-08 21:27:15,992] loss: 0.213751  [  960/ 4779]
[2022-08-08 21:27:35,666] loss: 0.123939  [ 1440/ 4779]
[2022-08-08 21:27:55,341] loss: 0.107838  [ 1920/ 4779]
[2022-08-08 21:28:15,013] loss: 0.131826  [ 2400/ 4779]
[2022-08-08 21:28:34,687] loss: 0.237110  [ 2880/ 4779]
[2022-08-08 21:28:54,361] loss: 0.142743  [ 3360/ 4779]
[2022-08-08 21:29:14,035] loss: 0.089618  [ 3840/ 4779]
[2022-08-08 21:29:33,710] loss: 0.311863  [ 4320/ 4779]
[2022-08-08 21:31:04,557] Train Error: Accuracy: 86.629%, Avg loss: 0.374921
[2022-08-08 21:31:35,296] Test  Error: Accuracy: 86.577%, Avg loss: 0.410808
[2022-08-08 21:31:35,296] Epoch 4---------------
[2022-08-08 21:31:35,298] lr: 1.714750e-03
[2022-08-08 21:31:36,610] loss: 0.278607  [    0/ 4779]
[2022-08-08 21:31:56,267] loss: 0.014749  [  480/ 4779]
[2022-08-08 21:32:15,924] loss: 0.122090  [  960/ 4779]
[2022-08-08 21:32:35,581] loss: 0.057815  [ 1440/ 4779]
[2022-08-08 21:32:55,241] loss: 0.083319  [ 1920/ 4779]
[2022-08-08 21:33:14,898] loss: 0.213921  [ 2400/ 4779]
[2022-08-08 21:33:34,557] loss: 0.181191  [ 2880/ 4779]
[2022-08-08 21:33:54,214] loss: 0.695599  [ 3360/ 4779]
[2022-08-08 21:34:13,872] loss: 0.103032  [ 3840/ 4779]
[2022-08-08 21:34:33,527] loss: 0.063191  [ 4320/ 4779]
[2022-08-08 21:36:04,315] Train Error: Accuracy: 95.689%, Avg loss: 0.123358
[2022-08-08 21:36:35,048] Test  Error: Accuracy: 94.611%, Avg loss: 0.150979
[2022-08-08 21:36:35,049] Epoch 5---------------
[2022-08-08 21:36:35,049] lr: 1.629012e-03
[2022-08-08 21:36:36,360] loss: 0.102029  [    0/ 4779]
[2022-08-08 21:36:56,016] loss: 0.067044  [  480/ 4779]
[2022-08-08 21:37:15,671] loss: 0.127136  [  960/ 4779]
[2022-08-08 21:37:35,328] loss: 0.054597  [ 1440/ 4779]
[2022-08-08 21:37:54,984] loss: 0.042460  [ 1920/ 4779]
[2022-08-08 21:38:14,641] loss: 0.051061  [ 2400/ 4779]
[2022-08-08 21:38:34,298] loss: 0.029117  [ 2880/ 4779]
[2022-08-08 21:38:53,954] loss: 0.116358  [ 3360/ 4779]
[2022-08-08 21:39:13,610] loss: 0.252091  [ 3840/ 4779]
[2022-08-08 21:39:33,266] loss: 0.077010  [ 4320/ 4779]
[2022-08-08 21:41:04,045] Train Error: Accuracy: 91.818%, Avg loss: 0.268695
[2022-08-08 21:41:34,781] Test  Error: Accuracy: 91.966%, Avg loss: 0.300008
[2022-08-08 21:41:34,781] Epoch 6---------------
[2022-08-08 21:41:34,782] lr: 1.137600e-03
[2022-08-08 21:41:36,093] loss: 0.148748  [    0/ 4779]
[2022-08-08 21:41:55,748] loss: 0.049030  [  480/ 4779]
[2022-08-08 21:42:15,403] loss: 0.140942  [  960/ 4779]
[2022-08-08 21:42:35,059] loss: 0.015521  [ 1440/ 4779]
[2022-08-08 21:42:54,714] loss: 0.012948  [ 1920/ 4779]
[2022-08-08 21:43:14,368] loss: 0.030479  [ 2400/ 4779]
[2022-08-08 21:43:34,024] loss: 0.011672  [ 2880/ 4779]
[2022-08-08 21:43:53,679] loss: 0.090978  [ 3360/ 4779]
[2022-08-08 21:44:13,334] loss: 0.073669  [ 3840/ 4779]
[2022-08-08 21:44:32,989] loss: 0.070612  [ 4320/ 4779]
[2022-08-08 21:46:03,769] Train Error: Accuracy: 93.848%, Avg loss: 0.182702
[2022-08-08 21:46:34,502] Test  Error: Accuracy: 92.116%, Avg loss: 0.244532
[2022-08-08 21:46:34,502] Epoch 7---------------
[2022-08-08 21:46:34,502] lr: 1.080720e-03
[2022-08-08 21:46:35,813] loss: 0.181833  [    0/ 4779]
[2022-08-08 21:46:55,471] loss: 0.020763  [  480/ 4779]
[2022-08-08 21:47:15,127] loss: 0.028478  [  960/ 4779]
[2022-08-08 21:47:34,784] loss: 0.045130  [ 1440/ 4779]
[2022-08-08 21:47:54,444] loss: 0.069767  [ 1920/ 4779]
[2022-08-08 21:48:14,102] loss: 0.023436  [ 2400/ 4779]
[2022-08-08 21:48:33,759] loss: 0.003606  [ 2880/ 4779]
[2022-08-08 21:48:53,416] loss: 0.007596  [ 3360/ 4779]
[2022-08-08 21:49:13,074] loss: 0.068869  [ 3840/ 4779]
[2022-08-08 21:49:32,731] loss: 0.004425  [ 4320/ 4779]
[2022-08-08 21:51:03,510] Train Error: Accuracy: 98.807%, Avg loss: 0.040888
[2022-08-08 21:51:34,240] Test  Error: Accuracy: 97.754%, Avg loss: 0.071015
[2022-08-08 21:51:34,240] Epoch 8---------------
[2022-08-08 21:51:34,240] lr: 1.026684e-03
[2022-08-08 21:51:35,552] loss: 0.064332  [    0/ 4779]
[2022-08-08 21:51:55,207] loss: 0.006735  [  480/ 4779]
[2022-08-08 21:52:14,863] loss: 0.003428  [  960/ 4779]
[2022-08-08 21:52:34,518] loss: 0.026472  [ 1440/ 4779]
[2022-08-08 21:52:54,173] loss: 0.003163  [ 1920/ 4779]
[2022-08-08 21:53:13,828] loss: 0.012329  [ 2400/ 4779]
[2022-08-08 21:53:33,483] loss: 0.023021  [ 2880/ 4779]
[2022-08-08 21:53:53,138] loss: 0.016802  [ 3360/ 4779]
[2022-08-08 21:54:12,794] loss: 0.048885  [ 3840/ 4779]
[2022-08-08 21:54:32,449] loss: 0.038374  [ 4320/ 4779]
[2022-08-08 21:56:03,234] Train Error: Accuracy: 99.393%, Avg loss: 0.021229
[2022-08-08 21:56:33,969] Test  Error: Accuracy: 98.752%, Avg loss: 0.048841
[2022-08-08 21:56:33,969] Epoch 9---------------
[2022-08-08 21:56:33,970] lr: 9.753500e-04
[2022-08-08 21:56:35,281] loss: 0.079842  [    0/ 4779]
[2022-08-08 21:56:54,938] loss: 0.053642  [  480/ 4779]
[2022-08-08 21:57:14,595] loss: 0.011993  [  960/ 4779]
[2022-08-08 21:57:34,251] loss: 0.064632  [ 1440/ 4779]
[2022-08-08 21:57:53,907] loss: 0.074984  [ 1920/ 4779]
[2022-08-08 21:58:13,565] loss: 0.096065  [ 2400/ 4779]
[2022-08-08 21:58:33,222] loss: 0.038133  [ 2880/ 4779]
[2022-08-08 21:58:52,878] loss: 0.021993  [ 3360/ 4779]
[2022-08-08 21:59:12,533] loss: 0.002679  [ 3840/ 4779]
[2022-08-08 21:59:32,188] loss: 0.019931  [ 4320/ 4779]
[2022-08-08 22:01:02,969] Train Error: Accuracy: 99.435%, Avg loss: 0.017958
[2022-08-08 22:01:33,701] Test  Error: Accuracy: 98.653%, Avg loss: 0.045930
[2022-08-08 22:01:33,701] Epoch 10---------------
[2022-08-08 22:01:33,702] lr: 9.265825e-04
[2022-08-08 22:01:35,013] loss: 0.005243  [    0/ 4779]
[2022-08-08 22:01:54,669] loss: 0.020214  [  480/ 4779]
[2022-08-08 22:02:14,324] loss: 0.005767  [  960/ 4779]
[2022-08-08 22:02:33,979] loss: 0.070335  [ 1440/ 4779]
[2022-08-08 22:02:53,634] loss: 0.004848  [ 1920/ 4779]
[2022-08-08 22:03:13,290] loss: 0.078963  [ 2400/ 4779]
[2022-08-08 22:03:32,944] loss: 0.004856  [ 2880/ 4779]
[2022-08-08 22:03:52,599] loss: 0.011537  [ 3360/ 4779]
[2022-08-08 22:04:12,254] loss: 0.005898  [ 3840/ 4779]
[2022-08-08 22:04:31,910] loss: 0.001538  [ 4320/ 4779]
[2022-08-08 22:06:02,685] Train Error: Accuracy: 99.749%, Avg loss: 0.010359
[2022-08-08 22:06:33,417] Test  Error: Accuracy: 98.902%, Avg loss: 0.033678
[2022-08-08 22:06:33,418] Epoch 11---------------
[2022-08-08 22:06:33,418] lr: 8.802533e-04
[2022-08-08 22:06:34,729] loss: 0.144776  [    0/ 4779]
[2022-08-08 22:06:54,386] loss: 0.009671  [  480/ 4779]
[2022-08-08 22:07:14,041] loss: 0.011687  [  960/ 4779]
[2022-08-08 22:07:33,697] loss: 0.008032  [ 1440/ 4779]
[2022-08-08 22:07:53,351] loss: 0.003520  [ 1920/ 4779]
[2022-08-08 22:08:13,007] loss: 0.036514  [ 2400/ 4779]
[2022-08-08 22:08:32,662] loss: 0.008811  [ 2880/ 4779]
[2022-08-08 22:08:52,318] loss: 0.022698  [ 3360/ 4779]
[2022-08-08 22:09:11,973] loss: 0.052166  [ 3840/ 4779]
[2022-08-08 22:09:31,628] loss: 0.163323  [ 4320/ 4779]
[2022-08-08 22:11:02,405] Train Error: Accuracy: 94.852%, Avg loss: 0.168449
[2022-08-08 22:11:33,137] Test  Error: Accuracy: 94.561%, Avg loss: 0.186683
[2022-08-08 22:11:33,138] Epoch 12---------------
[2022-08-08 22:11:33,138] lr: 6.147137e-04
[2022-08-08 22:11:34,449] loss: 0.036941  [    0/ 4779]
[2022-08-08 22:11:54,103] loss: 0.053135  [  480/ 4779]
[2022-08-08 22:12:13,758] loss: 0.107248  [  960/ 4779]
[2022-08-08 22:12:33,413] loss: 0.171068  [ 1440/ 4779]
[2022-08-08 22:12:53,067] loss: 0.030858  [ 1920/ 4779]
[2022-08-08 22:13:12,722] loss: 0.009167  [ 2400/ 4779]
[2022-08-08 22:13:32,376] loss: 0.032462  [ 2880/ 4779]
[2022-08-08 22:13:52,031] loss: 0.009037  [ 3360/ 4779]
[2022-08-08 22:14:11,686] loss: 0.009678  [ 3840/ 4779]
[2022-08-08 22:14:31,339] loss: 0.015181  [ 4320/ 4779]
[2022-08-08 22:16:02,115] Train Error: Accuracy: 99.372%, Avg loss: 0.021165
[2022-08-08 22:16:32,852] Test  Error: Accuracy: 98.703%, Avg loss: 0.053223
[2022-08-08 22:16:32,853] Epoch 13---------------
[2022-08-08 22:16:32,853] lr: 5.839780e-04
[2022-08-08 22:16:34,165] loss: 0.003653  [    0/ 4779]
[2022-08-08 22:16:53,821] loss: 0.002500  [  480/ 4779]
[2022-08-08 22:17:13,476] loss: 0.003282  [  960/ 4779]
[2022-08-08 22:17:33,132] loss: 0.005950  [ 1440/ 4779]
[2022-08-08 22:17:52,787] loss: 0.008647  [ 1920/ 4779]
[2022-08-08 22:18:12,443] loss: 0.023506  [ 2400/ 4779]
[2022-08-08 22:18:32,098] loss: 0.005089  [ 2880/ 4779]
[2022-08-08 22:18:51,754] loss: 0.002865  [ 3360/ 4779]
[2022-08-08 22:19:11,409] loss: 0.000586  [ 3840/ 4779]
[2022-08-08 22:19:31,064] loss: 0.011072  [ 4320/ 4779]
[2022-08-08 22:21:01,850] Train Error: Accuracy: 99.770%, Avg loss: 0.011064
[2022-08-08 22:21:32,588] Test  Error: Accuracy: 98.802%, Avg loss: 0.042455
[2022-08-08 22:21:32,588] Epoch 14---------------
[2022-08-08 22:21:32,588] lr: 5.547791e-04
[2022-08-08 22:21:33,900] loss: 0.000766  [    0/ 4779]
[2022-08-08 22:21:53,558] loss: 0.020395  [  480/ 4779]
[2022-08-08 22:22:13,216] loss: 0.002929  [  960/ 4779]
[2022-08-08 22:22:32,891] loss: 0.004926  [ 1440/ 4779]
[2022-08-08 22:22:52,567] loss: 0.002585  [ 1920/ 4779]
[2022-08-08 22:23:15,031] loss: 0.001495  [ 2400/ 4779]
[2022-08-08 22:23:34,705] loss: 0.021671  [ 2880/ 4779]
[2022-08-08 22:23:54,379] loss: 0.014613  [ 3360/ 4779]
[2022-08-08 22:24:14,200] loss: 0.010985  [ 3840/ 4779]
[2022-08-08 22:24:33,876] loss: 0.035759  [ 4320/ 4779]
[2022-08-08 22:26:04,767] Train Error: Accuracy: 99.812%, Avg loss: 0.008330
[2022-08-08 22:26:35,533] Test  Error: Accuracy: 98.852%, Avg loss: 0.032214
[2022-08-08 22:26:35,534] Epoch 15---------------
[2022-08-08 22:26:35,535] lr: 5.270402e-04
[2022-08-08 22:26:36,849] loss: 0.002161  [    0/ 4779]
[2022-08-08 22:26:56,527] loss: 0.001485  [  480/ 4779]
[2022-08-08 22:27:16,205] loss: 0.025776  [  960/ 4779]
[2022-08-08 22:27:35,883] loss: 0.003349  [ 1440/ 4779]
[2022-08-08 22:27:55,560] loss: 0.001315  [ 1920/ 4779]
[2022-08-08 22:28:15,237] loss: 0.007922  [ 2400/ 4779]
[2022-08-08 22:28:34,915] loss: 0.010475  [ 2880/ 4779]
[2022-08-08 22:28:54,594] loss: 0.000571  [ 3360/ 4779]
[2022-08-08 22:29:14,271] loss: 0.034237  [ 3840/ 4779]
[2022-08-08 22:29:33,948] loss: 0.001579  [ 4320/ 4779]
[2022-08-08 22:31:04,801] Train Error: Accuracy: 99.770%, Avg loss: 0.008461
[2022-08-08 22:31:35,566] Test  Error: Accuracy: 98.852%, Avg loss: 0.038835
[2022-08-08 22:31:35,567] Done!
[2022-08-08 22:31:35,572] Number of parameters:2443018
[2022-08-08 22:31:35,572] ## end time: 2022-08-08 22:31:35.567125
[2022-08-08 22:31:35,573] ## used time: 1:15:00.688551
