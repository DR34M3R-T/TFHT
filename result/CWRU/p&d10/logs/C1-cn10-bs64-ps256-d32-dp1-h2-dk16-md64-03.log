[2022-08-06 15:21:36,645] ## start time: 2022-08-06 15:21:36.520855
[2022-08-06 15:21:36,646] Using cuda device
[2022-08-06 15:21:36,647] In train:p&d10.npy.
[2022-08-06 15:21:36,648] One Channel
[2022-08-06 15:21:36,648] With Normal data.
[2022-08-06 15:21:36,649] Nunber of classes:10.
[2022-08-06 15:21:36,649] Nunber of ViT channels:1.
[2022-08-06 15:21:36,838] Totol epochs: 10
[2022-08-06 15:21:36,839] Epoch 1---------------
[2022-08-06 15:21:36,839] lr: 2.000000e-03
[2022-08-06 15:21:36,854] loss: 2.327879  [    0/ 4770]
[2022-08-06 15:21:37,015] loss: 1.926597  [  960/ 4770]
[2022-08-06 15:21:37,173] loss: 1.900811  [ 1920/ 4770]
[2022-08-06 15:21:37,327] loss: 1.497882  [ 2880/ 4770]
[2022-08-06 15:21:37,477] loss: 1.099611  [ 3840/ 4770]
[2022-08-06 15:21:37,883] Train Error: Accuracy: 74.486%, Avg loss: 0.757645
[2022-08-06 15:21:37,996] Test  Error: Accuracy: 74.367%, Avg loss: 0.769574
[2022-08-06 15:21:37,996] Epoch 2---------------
[2022-08-06 15:21:37,997] lr: 1.900000e-03
[2022-08-06 15:21:38,010] loss: 0.802806  [    0/ 4770]
[2022-08-06 15:21:38,166] loss: 0.454650  [  960/ 4770]
[2022-08-06 15:21:38,320] loss: 0.314512  [ 1920/ 4770]
[2022-08-06 15:21:38,474] loss: 0.390270  [ 2880/ 4770]
[2022-08-06 15:21:38,630] loss: 0.465528  [ 3840/ 4770]
[2022-08-06 15:21:39,041] Train Error: Accuracy: 91.132%, Avg loss: 0.268943
[2022-08-06 15:21:39,156] Test  Error: Accuracy: 89.319%, Avg loss: 0.306876
[2022-08-06 15:21:39,157] Epoch 3---------------
[2022-08-06 15:21:39,158] lr: 1.805000e-03
[2022-08-06 15:21:39,169] loss: 0.219515  [    0/ 4770]
[2022-08-06 15:21:39,319] loss: 0.154315  [  960/ 4770]
[2022-08-06 15:21:39,470] loss: 0.145715  [ 1920/ 4770]
[2022-08-06 15:21:39,620] loss: 0.099652  [ 2880/ 4770]
[2022-08-06 15:21:39,772] loss: 0.130812  [ 3840/ 4770]
[2022-08-06 15:21:40,178] Train Error: Accuracy: 94.990%, Avg loss: 0.164478
[2022-08-06 15:21:40,293] Test  Error: Accuracy: 93.194%, Avg loss: 0.207055
[2022-08-06 15:21:40,293] Epoch 4---------------
[2022-08-06 15:21:40,294] lr: 1.714750e-03
[2022-08-06 15:21:40,306] loss: 0.244041  [    0/ 4770]
[2022-08-06 15:21:40,456] loss: 0.150843  [  960/ 4770]
[2022-08-06 15:21:40,605] loss: 0.096667  [ 1920/ 4770]
[2022-08-06 15:21:40,754] loss: 0.181557  [ 2880/ 4770]
[2022-08-06 15:21:40,905] loss: 0.066949  [ 3840/ 4770]
[2022-08-06 15:21:41,320] Train Error: Accuracy: 98.742%, Avg loss: 0.059115
[2022-08-06 15:21:41,434] Test  Error: Accuracy: 97.914%, Avg loss: 0.081346
[2022-08-06 15:21:41,434] Epoch 5---------------
[2022-08-06 15:21:41,435] lr: 1.629012e-03
[2022-08-06 15:21:41,448] loss: 0.055403  [    0/ 4770]
[2022-08-06 15:21:41,599] loss: 0.029403  [  960/ 4770]
[2022-08-06 15:21:41,750] loss: 0.030979  [ 1920/ 4770]
[2022-08-06 15:21:41,904] loss: 0.056174  [ 2880/ 4770]
[2022-08-06 15:21:42,055] loss: 0.041765  [ 3840/ 4770]
[2022-08-06 15:21:42,466] Train Error: Accuracy: 98.679%, Avg loss: 0.056005
[2022-08-06 15:21:42,580] Test  Error: Accuracy: 97.516%, Avg loss: 0.083002
[2022-08-06 15:21:42,580] Epoch 6---------------
[2022-08-06 15:21:42,581] lr: 1.396675e-03
[2022-08-06 15:21:42,593] loss: 0.036152  [    0/ 4770]
[2022-08-06 15:21:42,743] loss: 0.022686  [  960/ 4770]
[2022-08-06 15:21:42,895] loss: 0.030882  [ 1920/ 4770]
[2022-08-06 15:21:43,047] loss: 0.014760  [ 2880/ 4770]
[2022-08-06 15:21:43,198] loss: 0.016808  [ 3840/ 4770]
[2022-08-06 15:21:43,612] Train Error: Accuracy: 98.910%, Avg loss: 0.042819
[2022-08-06 15:21:43,727] Test  Error: Accuracy: 97.963%, Avg loss: 0.058566
[2022-08-06 15:21:43,727] Epoch 7---------------
[2022-08-06 15:21:43,728] lr: 1.326841e-03
[2022-08-06 15:21:43,740] loss: 0.084306  [    0/ 4770]
[2022-08-06 15:21:43,891] loss: 0.015868  [  960/ 4770]
[2022-08-06 15:21:44,043] loss: 0.015576  [ 1920/ 4770]
[2022-08-06 15:21:44,193] loss: 0.019110  [ 2880/ 4770]
[2022-08-06 15:21:44,342] loss: 0.022422  [ 3840/ 4770]
[2022-08-06 15:21:44,748] Train Error: Accuracy: 99.623%, Avg loss: 0.021622
[2022-08-06 15:21:44,862] Test  Error: Accuracy: 99.006%, Avg loss: 0.042414
[2022-08-06 15:21:44,863] Epoch 8---------------
[2022-08-06 15:21:44,864] lr: 1.260499e-03
[2022-08-06 15:21:44,875] loss: 0.013495  [    0/ 4770]
[2022-08-06 15:21:45,025] loss: 0.032923  [  960/ 4770]
[2022-08-06 15:21:45,176] loss: 0.018584  [ 1920/ 4770]
[2022-08-06 15:21:45,326] loss: 0.028355  [ 2880/ 4770]
[2022-08-06 15:21:45,476] loss: 0.008145  [ 3840/ 4770]
[2022-08-06 15:21:45,884] Train Error: Accuracy: 99.518%, Avg loss: 0.023780
[2022-08-06 15:21:45,999] Test  Error: Accuracy: 98.460%, Avg loss: 0.052412
[2022-08-06 15:21:46,000] Epoch 9---------------
[2022-08-06 15:21:46,000] lr: 9.753500e-04
[2022-08-06 15:21:46,012] loss: 0.016781  [    0/ 4770]
[2022-08-06 15:21:46,165] loss: 0.015927  [  960/ 4770]
[2022-08-06 15:21:46,315] loss: 0.027616  [ 1920/ 4770]
[2022-08-06 15:21:46,465] loss: 0.016269  [ 2880/ 4770]
[2022-08-06 15:21:46,614] loss: 0.030287  [ 3840/ 4770]
[2022-08-06 15:21:47,021] Train Error: Accuracy: 99.727%, Avg loss: 0.015691
[2022-08-06 15:21:47,136] Test  Error: Accuracy: 98.708%, Avg loss: 0.046421
[2022-08-06 15:21:47,136] Epoch 10---------------
[2022-08-06 15:21:47,138] lr: 9.265825e-04
[2022-08-06 15:21:47,149] loss: 0.029698  [    0/ 4770]
[2022-08-06 15:21:47,300] loss: 0.005668  [  960/ 4770]
[2022-08-06 15:21:47,450] loss: 0.006807  [ 1920/ 4770]
[2022-08-06 15:21:47,599] loss: 0.005163  [ 2880/ 4770]
[2022-08-06 15:21:47,750] loss: 0.007799  [ 3840/ 4770]
[2022-08-06 15:21:48,157] Train Error: Accuracy: 99.497%, Avg loss: 0.020349
[2022-08-06 15:21:48,271] Test  Error: Accuracy: 98.460%, Avg loss: 0.045360
[2022-08-06 15:21:48,272] Done!
[2022-08-06 15:21:48,274] Number of parameters:34762
[2022-08-06 15:21:48,274] ## end time: 2022-08-06 15:21:48.272420
[2022-08-06 15:21:48,275] ## used time: 0:00:11.751565
