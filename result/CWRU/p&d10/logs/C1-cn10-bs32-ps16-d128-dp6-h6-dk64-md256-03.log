[2022-08-07 23:45:13,113] ## start time: 2022-08-07 23:45:12.986873
[2022-08-07 23:45:13,114] Using cuda device
[2022-08-07 23:45:13,115] In train:p&d10.npy.
[2022-08-07 23:45:13,116] One Channel
[2022-08-07 23:45:13,116] With Normal data.
[2022-08-07 23:45:13,117] Nunber of classes:10.
[2022-08-07 23:45:13,117] Nunber of ViT channels:1.
[2022-08-07 23:45:13,350] Totol epochs: 15
[2022-08-07 23:45:13,352] Epoch 1---------------
[2022-08-07 23:45:13,353] lr: 2.000000e-03
[2022-08-07 23:45:14,144] loss: 2.624383  [    0/ 4739]
[2022-08-07 23:45:26,001] loss: 2.058882  [  480/ 4739]
[2022-08-07 23:45:37,857] loss: 1.708218  [  960/ 4739]
[2022-08-07 23:45:49,712] loss: 1.179609  [ 1440/ 4739]
[2022-08-07 23:46:01,571] loss: 0.897710  [ 1920/ 4739]
[2022-08-07 23:46:13,429] loss: 0.876665  [ 2400/ 4739]
[2022-08-07 23:46:25,288] loss: 0.641012  [ 2880/ 4739]
[2022-08-07 23:46:37,143] loss: 0.613969  [ 3360/ 4739]
[2022-08-07 23:46:48,999] loss: 0.375524  [ 3840/ 4739]
[2022-08-07 23:47:00,856] loss: 0.624067  [ 4320/ 4739]
[2022-08-07 23:47:51,563] Train Error: Accuracy: 56.257%, Avg loss: 1.636082
[2022-08-07 23:48:09,297] Test  Error: Accuracy: 54.501%, Avg loss: 1.682097
[2022-08-07 23:48:09,298] Epoch 2---------------
[2022-08-07 23:48:09,300] lr: 1.900000e-03
[2022-08-07 23:48:10,092] loss: 2.012045  [    0/ 4739]
[2022-08-07 23:48:21,941] loss: 0.246402  [  480/ 4739]
[2022-08-07 23:48:33,796] loss: 0.084416  [  960/ 4739]
[2022-08-07 23:48:45,653] loss: 0.098826  [ 1440/ 4739]
[2022-08-07 23:48:57,509] loss: 0.156877  [ 1920/ 4739]
[2022-08-07 23:49:09,371] loss: 0.064226  [ 2400/ 4739]
[2022-08-07 23:49:21,231] loss: 0.133955  [ 2880/ 4739]
[2022-08-07 23:49:33,091] loss: 0.065341  [ 3360/ 4739]
[2022-08-07 23:49:44,948] loss: 0.026315  [ 3840/ 4739]
[2022-08-07 23:49:56,807] loss: 0.100473  [ 4320/ 4739]
[2022-08-07 23:50:47,523] Train Error: Accuracy: 95.780%, Avg loss: 0.126002
[2022-08-07 23:51:05,258] Test  Error: Accuracy: 95.450%, Avg loss: 0.156563
[2022-08-07 23:51:05,258] Epoch 3---------------
[2022-08-07 23:51:05,259] lr: 1.805000e-03
[2022-08-07 23:51:06,052] loss: 0.087180  [    0/ 4739]
[2022-08-07 23:51:17,909] loss: 0.066055  [  480/ 4739]
[2022-08-07 23:51:29,770] loss: 0.036264  [  960/ 4739]
[2022-08-07 23:51:41,628] loss: 0.131959  [ 1440/ 4739]
[2022-08-07 23:51:53,486] loss: 0.507366  [ 1920/ 4739]
[2022-08-07 23:52:05,342] loss: 0.098225  [ 2400/ 4739]
[2022-08-07 23:52:17,200] loss: 0.042825  [ 2880/ 4739]
[2022-08-07 23:52:29,059] loss: 0.085428  [ 3360/ 4739]
[2022-08-07 23:52:40,916] loss: 0.065948  [ 3840/ 4739]
[2022-08-07 23:52:52,774] loss: 0.110859  [ 4320/ 4739]
[2022-08-07 23:53:43,475] Train Error: Accuracy: 97.489%, Avg loss: 0.083020
[2022-08-07 23:54:01,207] Test  Error: Accuracy: 96.673%, Avg loss: 0.094342
[2022-08-07 23:54:01,208] Epoch 4---------------
[2022-08-07 23:54:01,209] lr: 1.714750e-03
[2022-08-07 23:54:02,001] loss: 0.047398  [    0/ 4739]
[2022-08-07 23:54:13,858] loss: 0.053120  [  480/ 4739]
[2022-08-07 23:54:25,715] loss: 0.048308  [  960/ 4739]
[2022-08-07 23:54:37,570] loss: 0.139719  [ 1440/ 4739]
[2022-08-07 23:54:49,426] loss: 0.012354  [ 1920/ 4739]
[2022-08-07 23:55:01,283] loss: 0.021594  [ 2400/ 4739]
[2022-08-07 23:55:13,140] loss: 0.075487  [ 2880/ 4739]
[2022-08-07 23:55:24,997] loss: 0.050295  [ 3360/ 4739]
[2022-08-07 23:55:36,853] loss: 0.044462  [ 3840/ 4739]
[2022-08-07 23:55:48,710] loss: 0.076408  [ 4320/ 4739]
[2022-08-07 23:56:39,404] Train Error: Accuracy: 96.202%, Avg loss: 0.120346
[2022-08-07 23:56:57,133] Test  Error: Accuracy: 94.912%, Avg loss: 0.145385
[2022-08-07 23:56:57,134] Epoch 5---------------
[2022-08-07 23:56:57,136] lr: 1.197474e-03
[2022-08-07 23:56:57,927] loss: 0.101301  [    0/ 4739]
[2022-08-07 23:57:09,788] loss: 0.020037  [  480/ 4739]
[2022-08-07 23:57:21,644] loss: 0.051229  [  960/ 4739]
[2022-08-07 23:57:33,500] loss: 0.024010  [ 1440/ 4739]
[2022-08-07 23:57:45,358] loss: 0.004209  [ 1920/ 4739]
[2022-08-07 23:57:57,214] loss: 0.030918  [ 2400/ 4739]
[2022-08-07 23:58:09,069] loss: 0.023654  [ 2880/ 4739]
[2022-08-07 23:58:20,925] loss: 0.024929  [ 3360/ 4739]
[2022-08-07 23:58:32,783] loss: 0.198806  [ 3840/ 4739]
[2022-08-07 23:58:44,640] loss: 0.117205  [ 4320/ 4739]
[2022-08-07 23:59:35,338] Train Error: Accuracy: 98.818%, Avg loss: 0.038507
[2022-08-07 23:59:53,069] Test  Error: Accuracy: 97.945%, Avg loss: 0.060608
[2022-08-07 23:59:53,070] Epoch 6---------------
[2022-08-07 23:59:53,071] lr: 1.137600e-03
[2022-08-07 23:59:53,863] loss: 0.010323  [    0/ 4739]
[2022-08-08 00:00:05,719] loss: 0.008668  [  480/ 4739]
[2022-08-08 00:00:17,577] loss: 0.021776  [  960/ 4739]
[2022-08-08 00:00:29,433] loss: 0.025650  [ 1440/ 4739]
[2022-08-08 00:00:41,289] loss: 0.031762  [ 1920/ 4739]
[2022-08-08 00:00:53,148] loss: 0.018494  [ 2400/ 4739]
[2022-08-08 00:01:05,005] loss: 0.005895  [ 2880/ 4739]
[2022-08-08 00:01:16,863] loss: 0.018026  [ 3360/ 4739]
[2022-08-08 00:01:28,721] loss: 0.002242  [ 3840/ 4739]
[2022-08-08 00:01:40,577] loss: 0.003491  [ 4320/ 4739]
[2022-08-08 00:02:31,281] Train Error: Accuracy: 99.726%, Avg loss: 0.011293
[2022-08-08 00:02:49,010] Test  Error: Accuracy: 99.511%, Avg loss: 0.021042
[2022-08-08 00:02:49,011] Epoch 7---------------
[2022-08-08 00:02:49,013] lr: 1.080720e-03
[2022-08-08 00:02:49,805] loss: 0.006221  [    0/ 4739]
[2022-08-08 00:03:01,662] loss: 0.003717  [  480/ 4739]
[2022-08-08 00:03:13,519] loss: 0.004845  [  960/ 4739]
[2022-08-08 00:03:25,378] loss: 0.007090  [ 1440/ 4739]
[2022-08-08 00:03:37,233] loss: 0.065784  [ 1920/ 4739]
[2022-08-08 00:03:49,089] loss: 0.060149  [ 2400/ 4739]
[2022-08-08 00:04:00,946] loss: 0.001494  [ 2880/ 4739]
[2022-08-08 00:04:12,805] loss: 0.002085  [ 3360/ 4739]
[2022-08-08 00:04:24,663] loss: 0.001494  [ 3840/ 4739]
[2022-08-08 00:04:36,520] loss: 0.004305  [ 4320/ 4739]
[2022-08-08 00:05:27,220] Train Error: Accuracy: 99.810%, Avg loss: 0.008738
[2022-08-08 00:05:44,954] Test  Error: Accuracy: 99.266%, Avg loss: 0.023257
[2022-08-08 00:05:44,955] Epoch 8---------------
[2022-08-08 00:05:44,956] lr: 9.265825e-04
[2022-08-08 00:05:45,748] loss: 0.002073  [    0/ 4739]
[2022-08-08 00:05:57,609] loss: 0.005842  [  480/ 4739]
[2022-08-08 00:06:09,465] loss: 0.087634  [  960/ 4739]
[2022-08-08 00:06:21,323] loss: 0.004828  [ 1440/ 4739]
[2022-08-08 00:06:33,180] loss: 0.009667  [ 1920/ 4739]
[2022-08-08 00:06:45,037] loss: 0.000893  [ 2400/ 4739]
[2022-08-08 00:06:56,893] loss: 0.002182  [ 2880/ 4739]
[2022-08-08 00:07:08,749] loss: 0.009297  [ 3360/ 4739]
[2022-08-08 00:07:20,607] loss: 0.002936  [ 3840/ 4739]
[2022-08-08 00:07:32,467] loss: 0.008737  [ 4320/ 4739]
[2022-08-08 00:08:23,158] Train Error: Accuracy: 99.726%, Avg loss: 0.009403
[2022-08-08 00:08:40,888] Test  Error: Accuracy: 99.609%, Avg loss: 0.015048
[2022-08-08 00:08:40,889] Epoch 9---------------
[2022-08-08 00:08:40,890] lr: 8.802533e-04
[2022-08-08 00:08:41,682] loss: 0.001108  [    0/ 4739]
[2022-08-08 00:08:53,538] loss: 0.012079  [  480/ 4739]
[2022-08-08 00:09:05,395] loss: 0.001885  [  960/ 4739]
[2022-08-08 00:09:17,252] loss: 0.014638  [ 1440/ 4739]
[2022-08-08 00:09:29,108] loss: 0.011452  [ 1920/ 4739]
[2022-08-08 00:09:40,966] loss: 0.001108  [ 2400/ 4739]
[2022-08-08 00:09:52,823] loss: 0.003802  [ 2880/ 4739]
[2022-08-08 00:10:04,680] loss: 0.001552  [ 3360/ 4739]
[2022-08-08 00:10:16,538] loss: 0.035229  [ 3840/ 4739]
[2022-08-08 00:10:28,394] loss: 0.006586  [ 4320/ 4739]
[2022-08-08 00:11:19,094] Train Error: Accuracy: 99.578%, Avg loss: 0.013053
[2022-08-08 00:11:36,825] Test  Error: Accuracy: 98.826%, Avg loss: 0.024450
[2022-08-08 00:11:36,825] Epoch 10---------------
[2022-08-08 00:11:36,826] lr: 6.147137e-04
[2022-08-08 00:11:37,619] loss: 0.002046  [    0/ 4739]
[2022-08-08 00:11:49,474] loss: 0.003347  [  480/ 4739]
[2022-08-08 00:12:01,329] loss: 0.002197  [  960/ 4739]
[2022-08-08 00:12:13,189] loss: 0.020225  [ 1440/ 4739]
[2022-08-08 00:12:25,047] loss: 0.000990  [ 1920/ 4739]
[2022-08-08 00:12:36,903] loss: 0.001627  [ 2400/ 4739]
[2022-08-08 00:12:48,758] loss: 0.001798  [ 2880/ 4739]
[2022-08-08 00:13:00,615] loss: 0.001075  [ 3360/ 4739]
[2022-08-08 00:13:12,470] loss: 0.000848  [ 3840/ 4739]
[2022-08-08 00:13:24,327] loss: 0.000759  [ 4320/ 4739]
[2022-08-08 00:14:15,036] Train Error: Accuracy: 100.000%, Avg loss: 0.002374
[2022-08-08 00:14:32,768] Test  Error: Accuracy: 99.804%, Avg loss: 0.010430
[2022-08-08 00:14:32,768] Epoch 11---------------
[2022-08-08 00:14:32,770] lr: 5.839780e-04
[2022-08-08 00:14:33,563] loss: 0.001150  [    0/ 4739]
[2022-08-08 00:14:45,418] loss: 0.001251  [  480/ 4739]
[2022-08-08 00:14:57,276] loss: 0.000901  [  960/ 4739]
[2022-08-08 00:15:09,131] loss: 0.000504  [ 1440/ 4739]
[2022-08-08 00:15:20,989] loss: 0.000904  [ 1920/ 4739]
[2022-08-08 00:15:32,849] loss: 0.002718  [ 2400/ 4739]
[2022-08-08 00:15:44,707] loss: 0.005714  [ 2880/ 4739]
[2022-08-08 00:15:56,566] loss: 0.000983  [ 3360/ 4739]
[2022-08-08 00:16:08,424] loss: 0.000855  [ 3840/ 4739]
[2022-08-08 00:16:20,280] loss: 0.000737  [ 4320/ 4739]
[2022-08-08 00:17:10,978] Train Error: Accuracy: 99.937%, Avg loss: 0.002569
[2022-08-08 00:17:28,709] Test  Error: Accuracy: 99.266%, Avg loss: 0.016397
[2022-08-08 00:17:28,710] Epoch 12---------------
[2022-08-08 00:17:28,711] lr: 4.078137e-04
[2022-08-08 00:17:29,503] loss: 0.000888  [    0/ 4739]
[2022-08-08 00:17:41,358] loss: 0.000527  [  480/ 4739]
[2022-08-08 00:17:53,212] loss: 0.000716  [  960/ 4739]
[2022-08-08 00:18:05,070] loss: 0.000895  [ 1440/ 4739]
[2022-08-08 00:18:16,926] loss: 0.002172  [ 1920/ 4739]
[2022-08-08 00:18:28,784] loss: 0.000681  [ 2400/ 4739]
[2022-08-08 00:18:40,642] loss: 0.001708  [ 2880/ 4739]
[2022-08-08 00:18:52,501] loss: 0.001333  [ 3360/ 4739]
[2022-08-08 00:19:04,359] loss: 0.024096  [ 3840/ 4739]
[2022-08-08 00:19:16,215] loss: 0.000642  [ 4320/ 4739]
[2022-08-08 00:20:06,912] Train Error: Accuracy: 99.873%, Avg loss: 0.005737
[2022-08-08 00:20:24,642] Test  Error: Accuracy: 99.706%, Avg loss: 0.012415
[2022-08-08 00:20:24,642] Epoch 13---------------
[2022-08-08 00:20:24,644] lr: 3.874230e-04
[2022-08-08 00:20:25,436] loss: 0.002203  [    0/ 4739]
[2022-08-08 00:20:37,293] loss: 0.008543  [  480/ 4739]
[2022-08-08 00:20:49,150] loss: 0.003114  [  960/ 4739]
[2022-08-08 00:21:01,008] loss: 0.002238  [ 1440/ 4739]
[2022-08-08 00:21:12,863] loss: 0.000672  [ 1920/ 4739]
[2022-08-08 00:21:24,721] loss: 0.000736  [ 2400/ 4739]
[2022-08-08 00:21:36,576] loss: 0.000590  [ 2880/ 4739]
[2022-08-08 00:21:48,432] loss: 0.000497  [ 3360/ 4739]
[2022-08-08 00:22:00,291] loss: 0.000637  [ 3840/ 4739]
[2022-08-08 00:22:12,147] loss: 0.002948  [ 4320/ 4739]
[2022-08-08 00:23:02,844] Train Error: Accuracy: 100.000%, Avg loss: 0.001126
[2022-08-08 00:23:20,579] Test  Error: Accuracy: 99.706%, Avg loss: 0.009700
[2022-08-08 00:23:20,579] Epoch 14---------------
[2022-08-08 00:23:20,580] lr: 3.680518e-04
[2022-08-08 00:23:21,373] loss: 0.002158  [    0/ 4739]
[2022-08-08 00:23:33,227] loss: 0.000281  [  480/ 4739]
[2022-08-08 00:23:45,084] loss: 0.000620  [  960/ 4739]
[2022-08-08 00:23:56,942] loss: 0.002535  [ 1440/ 4739]
[2022-08-08 00:24:08,797] loss: 0.000521  [ 1920/ 4739]
[2022-08-08 00:24:20,655] loss: 0.000627  [ 2400/ 4739]
[2022-08-08 00:24:32,511] loss: 0.000380  [ 2880/ 4739]
[2022-08-08 00:24:44,369] loss: 0.000447  [ 3360/ 4739]
[2022-08-08 00:24:56,225] loss: 0.000449  [ 3840/ 4739]
[2022-08-08 00:25:08,080] loss: 0.005069  [ 4320/ 4739]
[2022-08-08 00:25:58,784] Train Error: Accuracy: 99.937%, Avg loss: 0.003121
[2022-08-08 00:26:16,515] Test  Error: Accuracy: 99.609%, Avg loss: 0.011341
[2022-08-08 00:26:16,515] Epoch 15---------------
[2022-08-08 00:26:16,516] lr: 2.847915e-04
[2022-08-08 00:26:17,307] loss: 0.001945  [    0/ 4739]
[2022-08-08 00:26:29,163] loss: 0.000954  [  480/ 4739]
[2022-08-08 00:26:41,020] loss: 0.000842  [  960/ 4739]
[2022-08-08 00:26:52,876] loss: 0.002327  [ 1440/ 4739]
[2022-08-08 00:27:04,734] loss: 0.000726  [ 1920/ 4739]
[2022-08-08 00:27:16,589] loss: 0.000497  [ 2400/ 4739]
[2022-08-08 00:27:28,447] loss: 0.000707  [ 2880/ 4739]
[2022-08-08 00:27:40,303] loss: 0.000721  [ 3360/ 4739]
[2022-08-08 00:27:52,158] loss: 0.004033  [ 3840/ 4739]
[2022-08-08 00:28:04,014] loss: 0.000932  [ 4320/ 4739]
[2022-08-08 00:28:54,711] Train Error: Accuracy: 99.979%, Avg loss: 0.001734
[2022-08-08 00:29:12,444] Test  Error: Accuracy: 99.804%, Avg loss: 0.008774
[2022-08-08 00:29:12,445] Done!
[2022-08-08 00:29:12,449] Number of parameters:3198730
[2022-08-08 00:29:12,449] ## end time: 2022-08-08 00:29:12.445182
[2022-08-08 00:29:12,450] ## used time: 0:43:59.458309
