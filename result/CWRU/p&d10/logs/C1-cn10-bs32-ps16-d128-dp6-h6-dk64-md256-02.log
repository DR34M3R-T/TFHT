[2022-08-07 23:01:01,052] ## start time: 2022-08-07 23:01:00.922279
[2022-08-07 23:01:01,052] Using cuda device
[2022-08-07 23:01:01,053] In train:p&d10.npy.
[2022-08-07 23:01:01,054] One Channel
[2022-08-07 23:01:01,055] With Normal data.
[2022-08-07 23:01:01,055] Nunber of classes:10.
[2022-08-07 23:01:01,056] Nunber of ViT channels:1.
[2022-08-07 23:01:01,304] Totol epochs: 15
[2022-08-07 23:01:01,306] Epoch 1---------------
[2022-08-07 23:01:01,307] lr: 2.000000e-03
[2022-08-07 23:01:02,092] loss: 2.278462  [    0/ 4779]
[2022-08-07 23:01:13,869] loss: 2.100162  [  480/ 4779]
[2022-08-07 23:01:25,642] loss: 1.444500  [  960/ 4779]
[2022-08-07 23:01:37,417] loss: 1.814570  [ 1440/ 4779]
[2022-08-07 23:01:49,193] loss: 0.784881  [ 1920/ 4779]
[2022-08-07 23:02:00,971] loss: 0.932656  [ 2400/ 4779]
[2022-08-07 23:02:12,744] loss: 0.413211  [ 2880/ 4779]
[2022-08-07 23:02:24,519] loss: 0.354594  [ 3360/ 4779]
[2022-08-07 23:02:36,294] loss: 0.307181  [ 3840/ 4779]
[2022-08-07 23:02:48,066] loss: 0.169185  [ 4320/ 4779]
[2022-08-07 23:03:39,724] Train Error: Accuracy: 92.509%, Avg loss: 0.241333
[2022-08-07 23:03:56,997] Test  Error: Accuracy: 91.916%, Avg loss: 0.262502
[2022-08-07 23:03:56,997] Epoch 2---------------
[2022-08-07 23:03:56,998] lr: 1.900000e-03
[2022-08-07 23:03:57,786] loss: 0.341783  [    0/ 4779]
[2022-08-07 23:04:09,559] loss: 0.040929  [  480/ 4779]
[2022-08-07 23:04:21,333] loss: 0.143817  [  960/ 4779]
[2022-08-07 23:04:33,109] loss: 0.088020  [ 1440/ 4779]
[2022-08-07 23:04:44,884] loss: 0.047572  [ 1920/ 4779]
[2022-08-07 23:04:56,660] loss: 0.067656  [ 2400/ 4779]
[2022-08-07 23:05:08,433] loss: 0.141377  [ 2880/ 4779]
[2022-08-07 23:05:20,208] loss: 0.270504  [ 3360/ 4779]
[2022-08-07 23:05:31,985] loss: 0.185509  [ 3840/ 4779]
[2022-08-07 23:05:43,762] loss: 0.110404  [ 4320/ 4779]
[2022-08-07 23:06:35,756] Train Error: Accuracy: 97.405%, Avg loss: 0.084538
[2022-08-07 23:06:53,146] Test  Error: Accuracy: 97.106%, Avg loss: 0.102116
[2022-08-07 23:06:53,146] Epoch 3---------------
[2022-08-07 23:06:53,147] lr: 1.805000e-03
[2022-08-07 23:06:53,940] loss: 0.042196  [    0/ 4779]
[2022-08-07 23:07:05,795] loss: 0.043542  [  480/ 4779]
[2022-08-07 23:07:17,652] loss: 0.048317  [  960/ 4779]
[2022-08-07 23:07:29,508] loss: 0.030737  [ 1440/ 4779]
[2022-08-07 23:07:41,366] loss: 0.039545  [ 1920/ 4779]
[2022-08-07 23:07:53,222] loss: 0.005725  [ 2400/ 4779]
[2022-08-07 23:08:05,078] loss: 0.134150  [ 2880/ 4779]
[2022-08-07 23:08:16,934] loss: 0.063421  [ 3360/ 4779]
[2022-08-07 23:08:28,791] loss: 0.073884  [ 3840/ 4779]
[2022-08-07 23:08:40,650] loss: 0.016829  [ 4320/ 4779]
[2022-08-07 23:09:32,670] Train Error: Accuracy: 96.673%, Avg loss: 0.103818
[2022-08-07 23:09:50,060] Test  Error: Accuracy: 96.058%, Avg loss: 0.121016
[2022-08-07 23:09:50,061] Epoch 4---------------
[2022-08-07 23:09:50,062] lr: 1.396675e-03
[2022-08-07 23:09:50,854] loss: 0.268611  [    0/ 4779]
[2022-08-07 23:10:02,708] loss: 0.028710  [  480/ 4779]
[2022-08-07 23:10:14,566] loss: 0.025140  [  960/ 4779]
[2022-08-07 23:10:26,422] loss: 0.022253  [ 1440/ 4779]
[2022-08-07 23:10:38,277] loss: 0.009189  [ 1920/ 4779]
[2022-08-07 23:10:50,134] loss: 0.035161  [ 2400/ 4779]
[2022-08-07 23:11:01,992] loss: 0.016819  [ 2880/ 4779]
[2022-08-07 23:11:13,849] loss: 0.060165  [ 3360/ 4779]
[2022-08-07 23:11:25,707] loss: 0.121233  [ 3840/ 4779]
[2022-08-07 23:11:37,562] loss: 0.012295  [ 4320/ 4779]
[2022-08-07 23:12:29,580] Train Error: Accuracy: 99.100%, Avg loss: 0.032370
[2022-08-07 23:12:46,972] Test  Error: Accuracy: 98.802%, Avg loss: 0.048235
[2022-08-07 23:12:46,972] Epoch 5---------------
[2022-08-07 23:12:46,973] lr: 1.326841e-03
[2022-08-07 23:12:47,766] loss: 0.028331  [    0/ 4779]
[2022-08-07 23:12:59,622] loss: 0.008270  [  480/ 4779]
[2022-08-07 23:13:11,478] loss: 0.009028  [  960/ 4779]
[2022-08-07 23:13:23,334] loss: 0.005157  [ 1440/ 4779]
[2022-08-07 23:13:35,190] loss: 0.007216  [ 1920/ 4779]
[2022-08-07 23:13:47,049] loss: 0.001156  [ 2400/ 4779]
[2022-08-07 23:13:58,904] loss: 0.001437  [ 2880/ 4779]
[2022-08-07 23:14:10,761] loss: 0.004123  [ 3360/ 4779]
[2022-08-07 23:14:22,618] loss: 0.004086  [ 3840/ 4779]
[2022-08-07 23:14:34,476] loss: 0.002230  [ 4320/ 4779]
[2022-08-07 23:15:26,496] Train Error: Accuracy: 99.519%, Avg loss: 0.019087
[2022-08-07 23:15:43,885] Test  Error: Accuracy: 98.752%, Avg loss: 0.039291
[2022-08-07 23:15:43,886] Epoch 6---------------
[2022-08-07 23:15:43,888] lr: 1.260499e-03
[2022-08-07 23:15:44,679] loss: 0.004305  [    0/ 4779]
[2022-08-07 23:15:56,537] loss: 0.001635  [  480/ 4779]
[2022-08-07 23:16:08,393] loss: 0.013810  [  960/ 4779]
[2022-08-07 23:16:20,250] loss: 0.063591  [ 1440/ 4779]
[2022-08-07 23:16:32,107] loss: 0.034496  [ 1920/ 4779]
[2022-08-07 23:16:43,964] loss: 0.003540  [ 2400/ 4779]
[2022-08-07 23:16:55,821] loss: 0.056895  [ 2880/ 4779]
[2022-08-07 23:17:07,679] loss: 0.008998  [ 3360/ 4779]
[2022-08-07 23:17:19,539] loss: 0.048802  [ 3840/ 4779]
[2022-08-07 23:17:31,395] loss: 0.018938  [ 4320/ 4779]
[2022-08-07 23:18:23,409] Train Error: Accuracy: 99.351%, Avg loss: 0.024032
[2022-08-07 23:18:40,800] Test  Error: Accuracy: 98.603%, Avg loss: 0.050107
[2022-08-07 23:18:40,800] Epoch 7---------------
[2022-08-07 23:18:40,801] lr: 8.802533e-04
[2022-08-07 23:18:41,594] loss: 0.003532  [    0/ 4779]
[2022-08-07 23:18:53,451] loss: 0.002473  [  480/ 4779]
[2022-08-07 23:19:05,310] loss: 0.011653  [  960/ 4779]
[2022-08-07 23:19:17,166] loss: 0.001762  [ 1440/ 4779]
[2022-08-07 23:19:29,022] loss: 0.058639  [ 1920/ 4779]
[2022-08-07 23:19:40,880] loss: 0.020302  [ 2400/ 4779]
[2022-08-07 23:19:52,739] loss: 0.008628  [ 2880/ 4779]
[2022-08-07 23:20:04,596] loss: 0.002600  [ 3360/ 4779]
[2022-08-07 23:20:16,455] loss: 0.012260  [ 3840/ 4779]
[2022-08-07 23:20:28,311] loss: 0.008522  [ 4320/ 4779]
[2022-08-07 23:21:20,329] Train Error: Accuracy: 99.979%, Avg loss: 0.003363
[2022-08-07 23:21:37,721] Test  Error: Accuracy: 99.451%, Avg loss: 0.015654
[2022-08-07 23:21:37,722] Epoch 8---------------
[2022-08-07 23:21:37,723] lr: 8.362407e-04
[2022-08-07 23:21:38,514] loss: 0.006461  [    0/ 4779]
[2022-08-07 23:21:50,370] loss: 0.008772  [  480/ 4779]
[2022-08-07 23:22:02,226] loss: 0.001205  [  960/ 4779]
[2022-08-07 23:22:14,083] loss: 0.001880  [ 1440/ 4779]
[2022-08-07 23:22:25,939] loss: 0.001332  [ 1920/ 4779]
[2022-08-07 23:22:37,797] loss: 0.001207  [ 2400/ 4779]
[2022-08-07 23:22:49,650] loss: 0.002878  [ 2880/ 4779]
[2022-08-07 23:23:01,507] loss: 0.014309  [ 3360/ 4779]
[2022-08-07 23:23:13,365] loss: 0.001409  [ 3840/ 4779]
[2022-08-07 23:23:25,219] loss: 0.002042  [ 4320/ 4779]
[2022-08-07 23:24:17,234] Train Error: Accuracy: 99.937%, Avg loss: 0.004986
[2022-08-07 23:24:34,625] Test  Error: Accuracy: 99.501%, Avg loss: 0.014010
[2022-08-07 23:24:34,625] Epoch 9---------------
[2022-08-07 23:24:34,626] lr: 7.944286e-04
[2022-08-07 23:24:35,419] loss: 0.000502  [    0/ 4779]
[2022-08-07 23:24:47,276] loss: 0.001045  [  480/ 4779]
[2022-08-07 23:24:59,131] loss: 0.004727  [  960/ 4779]
[2022-08-07 23:25:10,987] loss: 0.002381  [ 1440/ 4779]
[2022-08-07 23:25:22,844] loss: 0.001397  [ 1920/ 4779]
[2022-08-07 23:25:34,702] loss: 0.001859  [ 2400/ 4779]
[2022-08-07 23:25:46,558] loss: 0.003916  [ 2880/ 4779]
[2022-08-07 23:25:58,415] loss: 0.001353  [ 3360/ 4779]
[2022-08-07 23:26:10,270] loss: 0.000577  [ 3840/ 4779]
[2022-08-07 23:26:22,125] loss: 0.009924  [ 4320/ 4779]
[2022-08-07 23:27:14,142] Train Error: Accuracy: 99.895%, Avg loss: 0.003533
[2022-08-07 23:27:31,531] Test  Error: Accuracy: 99.750%, Avg loss: 0.012806
[2022-08-07 23:27:31,532] Epoch 10---------------
[2022-08-07 23:27:31,533] lr: 7.547072e-04
[2022-08-07 23:27:32,325] loss: 0.000919  [    0/ 4779]
[2022-08-07 23:27:44,182] loss: 0.001401  [  480/ 4779]
[2022-08-07 23:27:56,037] loss: 0.000699  [  960/ 4779]
[2022-08-07 23:28:07,893] loss: 0.000856  [ 1440/ 4779]
[2022-08-07 23:28:19,748] loss: 0.000703  [ 1920/ 4779]
[2022-08-07 23:28:31,605] loss: 0.000770  [ 2400/ 4779]
[2022-08-07 23:28:43,460] loss: 0.008759  [ 2880/ 4779]
[2022-08-07 23:28:55,317] loss: 0.000919  [ 3360/ 4779]
[2022-08-07 23:29:07,176] loss: 0.000531  [ 3840/ 4779]
[2022-08-07 23:29:19,029] loss: 0.000727  [ 4320/ 4779]
[2022-08-07 23:30:11,031] Train Error: Accuracy: 99.979%, Avg loss: 0.001744
[2022-08-07 23:30:28,417] Test  Error: Accuracy: 99.750%, Avg loss: 0.008997
[2022-08-07 23:30:28,418] Epoch 11---------------
[2022-08-07 23:30:28,421] lr: 7.169718e-04
[2022-08-07 23:30:29,214] loss: 0.000556  [    0/ 4779]
[2022-08-07 23:30:41,070] loss: 0.000390  [  480/ 4779]
[2022-08-07 23:30:52,926] loss: 0.001235  [  960/ 4779]
[2022-08-07 23:31:04,782] loss: 0.001763  [ 1440/ 4779]
[2022-08-07 23:31:16,640] loss: 0.013478  [ 1920/ 4779]
[2022-08-07 23:31:28,497] loss: 0.000907  [ 2400/ 4779]
[2022-08-07 23:31:40,354] loss: 0.009430  [ 2880/ 4779]
[2022-08-07 23:31:52,209] loss: 0.000519  [ 3360/ 4779]
[2022-08-07 23:32:04,067] loss: 0.029868  [ 3840/ 4779]
[2022-08-07 23:32:15,925] loss: 0.001279  [ 4320/ 4779]
[2022-08-07 23:33:07,936] Train Error: Accuracy: 99.937%, Avg loss: 0.002183
[2022-08-07 23:33:25,324] Test  Error: Accuracy: 99.601%, Avg loss: 0.010979
[2022-08-07 23:33:25,325] Epoch 12---------------
[2022-08-07 23:33:25,326] lr: 5.547791e-04
[2022-08-07 23:33:26,118] loss: 0.000671  [    0/ 4779]
[2022-08-07 23:33:37,972] loss: 0.000587  [  480/ 4779]
[2022-08-07 23:33:49,830] loss: 0.000582  [  960/ 4779]
[2022-08-07 23:34:01,687] loss: 0.000805  [ 1440/ 4779]
[2022-08-07 23:34:13,545] loss: 0.000385  [ 1920/ 4779]
[2022-08-07 23:34:25,401] loss: 0.000385  [ 2400/ 4779]
[2022-08-07 23:34:37,256] loss: 0.000353  [ 2880/ 4779]
[2022-08-07 23:34:49,114] loss: 0.000643  [ 3360/ 4779]
[2022-08-07 23:35:00,968] loss: 0.001329  [ 3840/ 4779]
[2022-08-07 23:35:12,825] loss: 0.002418  [ 4320/ 4779]
[2022-08-07 23:36:04,843] Train Error: Accuracy: 100.000%, Avg loss: 0.000840
[2022-08-07 23:36:22,232] Test  Error: Accuracy: 99.800%, Avg loss: 0.008135
[2022-08-07 23:36:22,232] Epoch 13---------------
[2022-08-07 23:36:22,233] lr: 5.270402e-04
[2022-08-07 23:36:23,027] loss: 0.000310  [    0/ 4779]
[2022-08-07 23:36:34,886] loss: 0.000529  [  480/ 4779]
[2022-08-07 23:36:46,744] loss: 0.000480  [  960/ 4779]
[2022-08-07 23:36:58,597] loss: 0.006001  [ 1440/ 4779]
[2022-08-07 23:37:10,456] loss: 0.000541  [ 1920/ 4779]
[2022-08-07 23:37:22,312] loss: 0.000329  [ 2400/ 4779]
[2022-08-07 23:37:34,167] loss: 0.000494  [ 2880/ 4779]
[2022-08-07 23:37:46,023] loss: 0.000563  [ 3360/ 4779]
[2022-08-07 23:37:57,881] loss: 0.001234  [ 3840/ 4779]
[2022-08-07 23:38:09,736] loss: 0.001049  [ 4320/ 4779]
[2022-08-07 23:39:01,750] Train Error: Accuracy: 100.000%, Avg loss: 0.000667
[2022-08-07 23:39:19,143] Test  Error: Accuracy: 99.651%, Avg loss: 0.009943
[2022-08-07 23:39:19,143] Epoch 14---------------
[2022-08-07 23:39:19,144] lr: 4.078137e-04
[2022-08-07 23:39:19,937] loss: 0.000432  [    0/ 4779]
[2022-08-07 23:39:31,794] loss: 0.000652  [  480/ 4779]
[2022-08-07 23:39:43,652] loss: 0.000437  [  960/ 4779]
[2022-08-07 23:39:55,508] loss: 0.000413  [ 1440/ 4779]
[2022-08-07 23:40:07,365] loss: 0.000536  [ 1920/ 4779]
[2022-08-07 23:40:19,220] loss: 0.000181  [ 2400/ 4779]
[2022-08-07 23:40:31,075] loss: 0.000435  [ 2880/ 4779]
[2022-08-07 23:40:42,933] loss: 0.000192  [ 3360/ 4779]
[2022-08-07 23:40:54,787] loss: 0.000678  [ 3840/ 4779]
[2022-08-07 23:41:06,643] loss: 0.000477  [ 4320/ 4779]
[2022-08-07 23:41:58,653] Train Error: Accuracy: 100.000%, Avg loss: 0.000608
[2022-08-07 23:42:16,043] Test  Error: Accuracy: 99.800%, Avg loss: 0.009276
[2022-08-07 23:42:16,043] Epoch 15---------------
[2022-08-07 23:42:16,044] lr: 3.874230e-04
[2022-08-07 23:42:16,837] loss: 0.000460  [    0/ 4779]
[2022-08-07 23:42:28,692] loss: 0.000303  [  480/ 4779]
[2022-08-07 23:42:40,551] loss: 0.000337  [  960/ 4779]
[2022-08-07 23:42:52,408] loss: 0.000489  [ 1440/ 4779]
[2022-08-07 23:43:04,264] loss: 0.000664  [ 1920/ 4779]
[2022-08-07 23:43:16,118] loss: 0.000314  [ 2400/ 4779]
[2022-08-07 23:43:27,969] loss: 0.000695  [ 2880/ 4779]
[2022-08-07 23:43:39,821] loss: 0.000569  [ 3360/ 4779]
[2022-08-07 23:43:51,672] loss: 0.000608  [ 3840/ 4779]
[2022-08-07 23:44:03,524] loss: 0.000480  [ 4320/ 4779]
[2022-08-07 23:44:55,531] Train Error: Accuracy: 100.000%, Avg loss: 0.000692
[2022-08-07 23:45:12,921] Test  Error: Accuracy: 99.651%, Avg loss: 0.009847
[2022-08-07 23:45:12,921] Done!
[2022-08-07 23:45:12,925] Number of parameters:3198730
[2022-08-07 23:45:12,925] ## end time: 2022-08-07 23:45:12.921857
[2022-08-07 23:45:12,926] ## used time: 0:44:11.999578
