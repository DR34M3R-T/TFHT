[2022-08-06 15:17:18,000] ## start time: 2022-08-06 15:17:17.871961
[2022-08-06 15:17:18,001] Using cuda device
[2022-08-06 15:17:18,002] In train:p&d10.npy.
[2022-08-06 15:17:18,002] One Channel
[2022-08-06 15:17:18,003] With Normal data.
[2022-08-06 15:17:18,003] Nunber of classes:10.
[2022-08-06 15:17:18,004] Nunber of ViT channels:1.
[2022-08-06 15:17:18,193] Totol epochs: 10
[2022-08-06 15:17:18,194] Epoch 1---------------
[2022-08-06 15:17:18,194] lr: 2.000000e-03
[2022-08-06 15:17:18,207] loss: 2.525023  [    0/ 4747]
[2022-08-06 15:17:18,382] loss: 1.391710  [  960/ 4747]
[2022-08-06 15:17:18,550] loss: 1.153206  [ 1920/ 4747]
[2022-08-06 15:17:18,703] loss: 0.698317  [ 2880/ 4747]
[2022-08-06 15:17:18,856] loss: 0.434548  [ 3840/ 4747]
[2022-08-06 15:17:19,291] Train Error: Accuracy: 95.323%, Avg loss: 0.290013
[2022-08-06 15:17:19,415] Test  Error: Accuracy: 95.285%, Avg loss: 0.301017
[2022-08-06 15:17:19,416] Epoch 2---------------
[2022-08-06 15:17:19,417] lr: 1.900000e-03
[2022-08-06 15:17:19,429] loss: 0.295037  [    0/ 4747]
[2022-08-06 15:17:19,585] loss: 0.180827  [  960/ 4747]
[2022-08-06 15:17:19,743] loss: 0.122634  [ 1920/ 4747]
[2022-08-06 15:17:19,896] loss: 0.084608  [ 2880/ 4747]
[2022-08-06 15:17:20,050] loss: 0.098510  [ 3840/ 4747]
[2022-08-06 15:17:20,473] Train Error: Accuracy: 99.537%, Avg loss: 0.051652
[2022-08-06 15:17:20,596] Test  Error: Accuracy: 98.919%, Avg loss: 0.067915
[2022-08-06 15:17:20,597] Epoch 3---------------
[2022-08-06 15:17:20,598] lr: 1.805000e-03
[2022-08-06 15:17:20,610] loss: 0.032521  [    0/ 4747]
[2022-08-06 15:17:20,766] loss: 0.037692  [  960/ 4747]
[2022-08-06 15:17:20,918] loss: 0.025110  [ 1920/ 4747]
[2022-08-06 15:17:21,070] loss: 0.025932  [ 2880/ 4747]
[2022-08-06 15:17:21,224] loss: 0.017839  [ 3840/ 4747]
[2022-08-06 15:17:21,645] Train Error: Accuracy: 78.239%, Avg loss: 0.826181
[2022-08-06 15:17:21,766] Test  Error: Accuracy: 75.393%, Avg loss: 0.907717
[2022-08-06 15:17:21,766] Epoch 4---------------
[2022-08-06 15:17:21,767] lr: 1.260499e-03
[2022-08-06 15:17:21,780] loss: 0.794771  [    0/ 4747]
[2022-08-06 15:17:21,934] loss: 0.030753  [  960/ 4747]
[2022-08-06 15:17:22,087] loss: 0.020679  [ 1920/ 4747]
[2022-08-06 15:17:22,239] loss: 0.015760  [ 2880/ 4747]
[2022-08-06 15:17:22,390] loss: 0.017936  [ 3840/ 4747]
[2022-08-06 15:17:22,811] Train Error: Accuracy: 99.726%, Avg loss: 0.021613
[2022-08-06 15:17:22,931] Test  Error: Accuracy: 99.411%, Avg loss: 0.032835
[2022-08-06 15:17:22,931] Epoch 5---------------
[2022-08-06 15:17:22,932] lr: 1.197474e-03
[2022-08-06 15:17:22,944] loss: 0.026032  [    0/ 4747]
[2022-08-06 15:17:23,092] loss: 0.014590  [  960/ 4747]
[2022-08-06 15:17:23,244] loss: 0.011544  [ 1920/ 4747]
[2022-08-06 15:17:23,396] loss: 0.009137  [ 2880/ 4747]
[2022-08-06 15:17:23,547] loss: 0.009566  [ 3840/ 4747]
[2022-08-06 15:17:23,965] Train Error: Accuracy: 99.831%, Avg loss: 0.012425
[2022-08-06 15:17:24,085] Test  Error: Accuracy: 99.558%, Avg loss: 0.022374
[2022-08-06 15:17:24,085] Epoch 6---------------
[2022-08-06 15:17:24,086] lr: 1.137600e-03
[2022-08-06 15:17:24,098] loss: 0.006734  [    0/ 4747]
[2022-08-06 15:17:24,250] loss: 0.015091  [  960/ 4747]
[2022-08-06 15:17:24,404] loss: 0.008866  [ 1920/ 4747]
[2022-08-06 15:17:24,554] loss: 0.008406  [ 2880/ 4747]
[2022-08-06 15:17:24,712] loss: 0.013605  [ 3840/ 4747]
[2022-08-06 15:17:25,132] Train Error: Accuracy: 81.125%, Avg loss: 0.593928
[2022-08-06 15:17:25,252] Test  Error: Accuracy: 81.631%, Avg loss: 0.615651
[2022-08-06 15:17:25,252] Epoch 7---------------
[2022-08-06 15:17:25,254] lr: 7.944286e-04
[2022-08-06 15:17:25,265] loss: 0.723834  [    0/ 4747]
[2022-08-06 15:17:25,418] loss: 0.016500  [  960/ 4747]
[2022-08-06 15:17:25,570] loss: 0.044917  [ 1920/ 4747]
[2022-08-06 15:17:25,722] loss: 0.006502  [ 2880/ 4747]
[2022-08-06 15:17:25,874] loss: 0.025213  [ 3840/ 4747]
[2022-08-06 15:17:26,295] Train Error: Accuracy: 99.895%, Avg loss: 0.010323
[2022-08-06 15:17:26,413] Test  Error: Accuracy: 99.705%, Avg loss: 0.020119
[2022-08-06 15:17:26,413] Epoch 8---------------
[2022-08-06 15:17:26,414] lr: 7.547072e-04
[2022-08-06 15:17:26,426] loss: 0.009378  [    0/ 4747]
[2022-08-06 15:17:26,580] loss: 0.033865  [  960/ 4747]
[2022-08-06 15:17:26,730] loss: 0.005297  [ 1920/ 4747]
[2022-08-06 15:17:26,881] loss: 0.005601  [ 2880/ 4747]
[2022-08-06 15:17:27,031] loss: 0.004689  [ 3840/ 4747]
[2022-08-06 15:17:27,447] Train Error: Accuracy: 100.000%, Avg loss: 0.005136
[2022-08-06 15:17:27,571] Test  Error: Accuracy: 99.853%, Avg loss: 0.012601
[2022-08-06 15:17:27,571] Epoch 9---------------
[2022-08-06 15:17:27,572] lr: 7.169718e-04
[2022-08-06 15:17:27,584] loss: 0.012073  [    0/ 4747]
[2022-08-06 15:17:27,735] loss: 0.005819  [  960/ 4747]
[2022-08-06 15:17:27,885] loss: 0.008174  [ 1920/ 4747]
[2022-08-06 15:17:28,036] loss: 0.004269  [ 2880/ 4747]
[2022-08-06 15:17:28,185] loss: 0.005482  [ 3840/ 4747]
[2022-08-06 15:17:28,606] Train Error: Accuracy: 99.937%, Avg loss: 0.005545
[2022-08-06 15:17:28,728] Test  Error: Accuracy: 99.902%, Avg loss: 0.011888
[2022-08-06 15:17:28,729] Epoch 10---------------
[2022-08-06 15:17:28,730] lr: 6.811233e-04
[2022-08-06 15:17:28,741] loss: 0.004290  [    0/ 4747]
[2022-08-06 15:17:28,892] loss: 0.004720  [  960/ 4747]
[2022-08-06 15:17:29,042] loss: 0.007042  [ 1920/ 4747]
[2022-08-06 15:17:29,192] loss: 0.003287  [ 2880/ 4747]
[2022-08-06 15:17:29,345] loss: 0.004106  [ 3840/ 4747]
[2022-08-06 15:17:29,778] Train Error: Accuracy: 99.979%, Avg loss: 0.004016
[2022-08-06 15:17:29,909] Test  Error: Accuracy: 99.902%, Avg loss: 0.009794
[2022-08-06 15:17:29,909] Done!
[2022-08-06 15:17:29,911] Number of parameters:92106
[2022-08-06 15:17:29,911] ## end time: 2022-08-06 15:17:29.909136
[2022-08-06 15:17:29,912] ## used time: 0:00:12.037175
