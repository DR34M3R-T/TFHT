[2022-08-08 01:38:47,737] ## start time: 2022-08-08 01:38:47.604998
[2022-08-08 01:38:47,737] Using cuda device
[2022-08-08 01:38:47,738] In train:p&d10.npy.
[2022-08-08 01:38:47,740] One Channel
[2022-08-08 01:38:47,740] With Normal data.
[2022-08-08 01:38:47,740] Nunber of classes:10.
[2022-08-08 01:38:47,741] Nunber of ViT channels:1.
[2022-08-08 01:38:47,997] Totol epochs: 15
[2022-08-08 01:38:47,999] Epoch 1---------------
[2022-08-08 01:38:47,999] lr: 2.000000e-03
[2022-08-08 01:38:48,182] loss: 2.309083  [    0/ 4742]
[2022-08-08 01:38:50,907] loss: 1.794022  [  480/ 4742]
[2022-08-08 01:38:53,634] loss: 1.744358  [  960/ 4742]
[2022-08-08 01:38:56,356] loss: 1.126201  [ 1440/ 4742]
[2022-08-08 01:38:59,082] loss: 0.831071  [ 1920/ 4742]
[2022-08-08 01:39:01,806] loss: 0.344786  [ 2400/ 4742]
[2022-08-08 01:39:04,532] loss: 0.296092  [ 2880/ 4742]
[2022-08-08 01:39:07,259] loss: 0.156959  [ 3360/ 4742]
[2022-08-08 01:39:09,985] loss: 0.658562  [ 3840/ 4742]
[2022-08-08 01:39:12,711] loss: 0.038733  [ 4320/ 4742]
[2022-08-08 01:39:23,993] Train Error: Accuracy: 99.220%, Avg loss: 0.045194
[2022-08-08 01:39:27,893] Test  Error: Accuracy: 99.265%, Avg loss: 0.050297
[2022-08-08 01:39:27,893] Epoch 2---------------
[2022-08-08 01:39:27,894] lr: 1.900000e-03
[2022-08-08 01:39:28,078] loss: 0.015396  [    0/ 4742]
[2022-08-08 01:39:30,804] loss: 0.167496  [  480/ 4742]
[2022-08-08 01:39:33,532] loss: 0.017747  [  960/ 4742]
[2022-08-08 01:39:36,259] loss: 1.849233  [ 1440/ 4742]
[2022-08-08 01:39:38,982] loss: 0.067679  [ 1920/ 4742]
[2022-08-08 01:39:41,709] loss: 0.045059  [ 2400/ 4742]
[2022-08-08 01:39:44,434] loss: 0.015332  [ 2880/ 4742]
[2022-08-08 01:39:47,161] loss: 0.053385  [ 3360/ 4742]
[2022-08-08 01:39:49,887] loss: 0.031536  [ 3840/ 4742]
[2022-08-08 01:39:52,611] loss: 0.003626  [ 4320/ 4742]
[2022-08-08 01:40:03,890] Train Error: Accuracy: 99.810%, Avg loss: 0.012510
[2022-08-08 01:40:07,787] Test  Error: Accuracy: 99.755%, Avg loss: 0.017468
[2022-08-08 01:40:07,788] Epoch 3---------------
[2022-08-08 01:40:07,788] lr: 1.805000e-03
[2022-08-08 01:40:07,973] loss: 0.007933  [    0/ 4742]
[2022-08-08 01:40:10,699] loss: 0.007522  [  480/ 4742]
[2022-08-08 01:40:13,426] loss: 0.003677  [  960/ 4742]
[2022-08-08 01:40:16,152] loss: 0.005590  [ 1440/ 4742]
[2022-08-08 01:40:18,877] loss: 0.002620  [ 1920/ 4742]
[2022-08-08 01:40:21,603] loss: 0.043302  [ 2400/ 4742]
[2022-08-08 01:40:24,328] loss: 0.003150  [ 2880/ 4742]
[2022-08-08 01:40:27,054] loss: 0.039424  [ 3360/ 4742]
[2022-08-08 01:40:29,780] loss: 0.001924  [ 3840/ 4742]
[2022-08-08 01:40:32,506] loss: 0.004667  [ 4320/ 4742]
[2022-08-08 01:40:43,785] Train Error: Accuracy: 99.873%, Avg loss: 0.010020
[2022-08-08 01:40:47,681] Test  Error: Accuracy: 99.804%, Avg loss: 0.012725
[2022-08-08 01:40:47,681] Epoch 4---------------
[2022-08-08 01:40:47,683] lr: 1.714750e-03
[2022-08-08 01:40:47,866] loss: 0.005625  [    0/ 4742]
[2022-08-08 01:40:50,590] loss: 0.006482  [  480/ 4742]
[2022-08-08 01:40:53,316] loss: 0.005501  [  960/ 4742]
[2022-08-08 01:40:56,042] loss: 0.002066  [ 1440/ 4742]
[2022-08-08 01:40:58,768] loss: 0.015107  [ 1920/ 4742]
[2022-08-08 01:41:01,492] loss: 0.002431  [ 2400/ 4742]
[2022-08-08 01:41:04,219] loss: 0.004012  [ 2880/ 4742]
[2022-08-08 01:41:06,945] loss: 0.036780  [ 3360/ 4742]
[2022-08-08 01:41:09,669] loss: 0.005785  [ 3840/ 4742]
[2022-08-08 01:41:12,395] loss: 0.035272  [ 4320/ 4742]
[2022-08-08 01:41:23,673] Train Error: Accuracy: 99.873%, Avg loss: 0.007643
[2022-08-08 01:41:27,567] Test  Error: Accuracy: 99.608%, Avg loss: 0.016691
[2022-08-08 01:41:27,568] Epoch 5---------------
[2022-08-08 01:41:27,569] lr: 1.197474e-03
[2022-08-08 01:41:27,752] loss: 0.002922  [    0/ 4742]
[2022-08-08 01:41:30,476] loss: 0.003222  [  480/ 4742]
[2022-08-08 01:41:33,202] loss: 0.001505  [  960/ 4742]
[2022-08-08 01:41:35,924] loss: 0.007618  [ 1440/ 4742]
[2022-08-08 01:41:38,650] loss: 0.001377  [ 1920/ 4742]
[2022-08-08 01:41:41,376] loss: 0.001437  [ 2400/ 4742]
[2022-08-08 01:41:44,102] loss: 0.001292  [ 2880/ 4742]
[2022-08-08 01:41:46,829] loss: 0.001109  [ 3360/ 4742]
[2022-08-08 01:41:49,554] loss: 0.002396  [ 3840/ 4742]
[2022-08-08 01:41:52,281] loss: 0.006898  [ 4320/ 4742]
[2022-08-08 01:42:03,569] Train Error: Accuracy: 99.831%, Avg loss: 0.008312
[2022-08-08 01:42:07,460] Test  Error: Accuracy: 99.608%, Avg loss: 0.016217
[2022-08-08 01:42:07,461] Epoch 6---------------
[2022-08-08 01:42:07,462] lr: 1.137600e-03
[2022-08-08 01:42:07,646] loss: 0.002466  [    0/ 4742]
[2022-08-08 01:42:10,372] loss: 0.021021  [  480/ 4742]
[2022-08-08 01:42:13,100] loss: 0.001304  [  960/ 4742]
[2022-08-08 01:42:15,828] loss: 0.001589  [ 1440/ 4742]
[2022-08-08 01:42:18,555] loss: 0.002023  [ 1920/ 4742]
[2022-08-08 01:42:21,281] loss: 0.003006  [ 2400/ 4742]
[2022-08-08 01:42:24,008] loss: 0.000641  [ 2880/ 4742]
[2022-08-08 01:42:26,734] loss: 0.000822  [ 3360/ 4742]
[2022-08-08 01:42:29,461] loss: 0.001272  [ 3840/ 4742]
[2022-08-08 01:42:32,186] loss: 0.000538  [ 4320/ 4742]
[2022-08-08 01:42:43,474] Train Error: Accuracy: 99.958%, Avg loss: 0.002739
[2022-08-08 01:42:47,370] Test  Error: Accuracy: 100.000%, Avg loss: 0.004747
[2022-08-08 01:42:47,371] Epoch 7---------------
[2022-08-08 01:42:47,372] lr: 1.080720e-03
[2022-08-08 01:42:47,555] loss: 0.000985  [    0/ 4742]
[2022-08-08 01:42:50,281] loss: 1.351586  [  480/ 4742]
[2022-08-08 01:42:53,005] loss: 0.007371  [  960/ 4742]
[2022-08-08 01:42:55,731] loss: 0.007111  [ 1440/ 4742]
[2022-08-08 01:42:58,455] loss: 0.001728  [ 1920/ 4742]
[2022-08-08 01:43:01,179] loss: 0.002006  [ 2400/ 4742]
[2022-08-08 01:43:03,903] loss: 0.001292  [ 2880/ 4742]
[2022-08-08 01:43:06,627] loss: 0.001902  [ 3360/ 4742]
[2022-08-08 01:43:09,352] loss: 0.001390  [ 3840/ 4742]
[2022-08-08 01:43:12,080] loss: 0.001404  [ 4320/ 4742]
[2022-08-08 01:43:23,350] Train Error: Accuracy: 100.000%, Avg loss: 0.001486
[2022-08-08 01:43:27,244] Test  Error: Accuracy: 99.951%, Avg loss: 0.002926
[2022-08-08 01:43:27,244] Epoch 8---------------
[2022-08-08 01:43:27,245] lr: 1.026684e-03
[2022-08-08 01:43:27,428] loss: 0.001811  [    0/ 4742]
[2022-08-08 01:43:30,156] loss: 0.001213  [  480/ 4742]
[2022-08-08 01:43:32,882] loss: 0.000665  [  960/ 4742]
[2022-08-08 01:43:35,607] loss: 0.001304  [ 1440/ 4742]
[2022-08-08 01:43:38,333] loss: 0.001451  [ 1920/ 4742]
[2022-08-08 01:43:41,059] loss: 0.000671  [ 2400/ 4742]
[2022-08-08 01:43:43,783] loss: 0.002217  [ 2880/ 4742]
[2022-08-08 01:43:46,512] loss: 0.001193  [ 3360/ 4742]
[2022-08-08 01:43:49,240] loss: 0.000438  [ 3840/ 4742]
[2022-08-08 01:43:51,965] loss: 0.000705  [ 4320/ 4742]
[2022-08-08 01:44:03,254] Train Error: Accuracy: 100.000%, Avg loss: 0.000833
[2022-08-08 01:44:07,149] Test  Error: Accuracy: 99.951%, Avg loss: 0.001570
[2022-08-08 01:44:07,149] Epoch 9---------------
[2022-08-08 01:44:07,150] lr: 9.753500e-04
[2022-08-08 01:44:07,335] loss: 0.002000  [    0/ 4742]
[2022-08-08 01:44:10,061] loss: 0.000894  [  480/ 4742]
[2022-08-08 01:44:12,787] loss: 0.000552  [  960/ 4742]
[2022-08-08 01:44:15,512] loss: 0.000303  [ 1440/ 4742]
[2022-08-08 01:44:18,239] loss: 0.000452  [ 1920/ 4742]
[2022-08-08 01:44:20,964] loss: 0.001862  [ 2400/ 4742]
[2022-08-08 01:44:23,690] loss: 0.000783  [ 2880/ 4742]
[2022-08-08 01:44:26,416] loss: 0.001211  [ 3360/ 4742]
[2022-08-08 01:44:29,140] loss: 0.000878  [ 3840/ 4742]
[2022-08-08 01:44:31,864] loss: 0.000535  [ 4320/ 4742]
[2022-08-08 01:44:43,133] Train Error: Accuracy: 99.979%, Avg loss: 0.001491
[2022-08-08 01:44:47,025] Test  Error: Accuracy: 99.902%, Avg loss: 0.002997
[2022-08-08 01:44:47,025] Epoch 10---------------
[2022-08-08 01:44:47,029] lr: 6.811233e-04
[2022-08-08 01:44:47,211] loss: 0.000734  [    0/ 4742]
[2022-08-08 01:44:49,936] loss: 0.000803  [  480/ 4742]
[2022-08-08 01:44:52,661] loss: 0.000495  [  960/ 4742]
[2022-08-08 01:44:55,388] loss: 0.000414  [ 1440/ 4742]
[2022-08-08 01:44:58,114] loss: 0.000780  [ 1920/ 4742]
[2022-08-08 01:45:00,841] loss: 0.000873  [ 2400/ 4742]
[2022-08-08 01:45:03,566] loss: 0.000551  [ 2880/ 4742]
[2022-08-08 01:45:06,292] loss: 0.001093  [ 3360/ 4742]
[2022-08-08 01:45:09,019] loss: 0.000413  [ 3840/ 4742]
[2022-08-08 01:45:11,744] loss: 0.000467  [ 4320/ 4742]
[2022-08-08 01:45:23,034] Train Error: Accuracy: 100.000%, Avg loss: 0.000589
[2022-08-08 01:45:26,941] Test  Error: Accuracy: 100.000%, Avg loss: 0.001018
[2022-08-08 01:45:26,942] Epoch 11---------------
[2022-08-08 01:45:26,943] lr: 6.470671e-04
[2022-08-08 01:45:27,126] loss: 0.000358  [    0/ 4742]
[2022-08-08 01:45:29,851] loss: 0.000316  [  480/ 4742]
[2022-08-08 01:45:32,574] loss: 0.000426  [  960/ 4742]
[2022-08-08 01:45:35,298] loss: 0.000331  [ 1440/ 4742]
[2022-08-08 01:45:38,023] loss: 0.000334  [ 1920/ 4742]
[2022-08-08 01:45:40,746] loss: 0.000605  [ 2400/ 4742]
[2022-08-08 01:45:43,471] loss: 0.000632  [ 2880/ 4742]
[2022-08-08 01:45:46,194] loss: 0.000330  [ 3360/ 4742]
[2022-08-08 01:45:48,919] loss: 0.000683  [ 3840/ 4742]
[2022-08-08 01:45:51,643] loss: 0.000367  [ 4320/ 4742]
[2022-08-08 01:46:02,952] Train Error: Accuracy: 100.000%, Avg loss: 0.000515
[2022-08-08 01:46:06,865] Test  Error: Accuracy: 100.000%, Avg loss: 0.001094
[2022-08-08 01:46:06,865] Epoch 12---------------
[2022-08-08 01:46:06,866] lr: 5.547791e-04
[2022-08-08 01:46:07,053] loss: 0.001390  [    0/ 4742]
[2022-08-08 01:46:09,778] loss: 0.000400  [  480/ 4742]
[2022-08-08 01:46:12,501] loss: 0.000273  [  960/ 4742]
[2022-08-08 01:46:15,226] loss: 0.000541  [ 1440/ 4742]
[2022-08-08 01:46:17,951] loss: 0.000273  [ 1920/ 4742]
[2022-08-08 01:46:20,677] loss: 0.000481  [ 2400/ 4742]
[2022-08-08 01:46:23,404] loss: 0.000226  [ 2880/ 4742]
[2022-08-08 01:46:26,129] loss: 0.000215  [ 3360/ 4742]
[2022-08-08 01:46:28,855] loss: 0.000278  [ 3840/ 4742]
[2022-08-08 01:46:31,582] loss: 0.000294  [ 4320/ 4742]
[2022-08-08 01:46:42,855] Train Error: Accuracy: 99.937%, Avg loss: 0.005446
[2022-08-08 01:46:46,748] Test  Error: Accuracy: 99.706%, Avg loss: 0.013598
[2022-08-08 01:46:46,748] Epoch 13---------------
[2022-08-08 01:46:46,749] lr: 3.874230e-04
[2022-08-08 01:46:46,934] loss: 0.009730  [    0/ 4742]
[2022-08-08 01:46:49,660] loss: 0.000420  [  480/ 4742]
[2022-08-08 01:46:52,385] loss: 0.000612  [  960/ 4742]
[2022-08-08 01:46:55,110] loss: 0.000611  [ 1440/ 4742]
[2022-08-08 01:46:57,836] loss: 0.000525  [ 1920/ 4742]
[2022-08-08 01:47:00,561] loss: 0.000427  [ 2400/ 4742]
[2022-08-08 01:47:03,286] loss: 0.000375  [ 2880/ 4742]
[2022-08-08 01:47:06,011] loss: 0.001807  [ 3360/ 4742]
[2022-08-08 01:47:08,738] loss: 0.000591  [ 3840/ 4742]
[2022-08-08 01:47:11,463] loss: 0.000391  [ 4320/ 4742]
[2022-08-08 01:47:22,729] Train Error: Accuracy: 100.000%, Avg loss: 0.000555
[2022-08-08 01:47:26,626] Test  Error: Accuracy: 99.951%, Avg loss: 0.002484
[2022-08-08 01:47:26,627] Epoch 14---------------
[2022-08-08 01:47:26,629] lr: 3.680518e-04
[2022-08-08 01:47:26,813] loss: 0.000261  [    0/ 4742]
[2022-08-08 01:47:29,539] loss: 0.000401  [  480/ 4742]
[2022-08-08 01:47:32,265] loss: 0.000399  [  960/ 4742]
[2022-08-08 01:47:34,992] loss: 0.000939  [ 1440/ 4742]
[2022-08-08 01:47:37,715] loss: 0.000516  [ 1920/ 4742]
[2022-08-08 01:47:40,442] loss: 0.000295  [ 2400/ 4742]
[2022-08-08 01:47:43,168] loss: 0.000276  [ 2880/ 4742]
[2022-08-08 01:47:45,891] loss: 0.000953  [ 3360/ 4742]
[2022-08-08 01:47:48,618] loss: 0.118638  [ 3840/ 4742]
[2022-08-08 01:47:51,345] loss: 0.002683  [ 4320/ 4742]
[2022-08-08 01:48:02,626] Train Error: Accuracy: 100.000%, Avg loss: 0.000945
[2022-08-08 01:48:06,520] Test  Error: Accuracy: 99.902%, Avg loss: 0.002427
[2022-08-08 01:48:06,521] Epoch 15---------------
[2022-08-08 01:48:06,522] lr: 3.496492e-04
[2022-08-08 01:48:06,706] loss: 0.000957  [    0/ 4742]
[2022-08-08 01:48:09,432] loss: 0.000404  [  480/ 4742]
[2022-08-08 01:48:12,157] loss: 0.001587  [  960/ 4742]
[2022-08-08 01:48:14,883] loss: 0.001625  [ 1440/ 4742]
[2022-08-08 01:48:17,608] loss: 0.000251  [ 1920/ 4742]
[2022-08-08 01:48:20,335] loss: 0.000424  [ 2400/ 4742]
[2022-08-08 01:48:23,059] loss: 0.000658  [ 2880/ 4742]
[2022-08-08 01:48:25,786] loss: 0.000334  [ 3360/ 4742]
[2022-08-08 01:48:28,511] loss: 0.000395  [ 3840/ 4742]
[2022-08-08 01:48:31,236] loss: 0.000296  [ 4320/ 4742]
[2022-08-08 01:48:42,516] Train Error: Accuracy: 100.000%, Avg loss: 0.000433
[2022-08-08 01:48:46,409] Test  Error: Accuracy: 99.951%, Avg loss: 0.003160
[2022-08-08 01:48:46,410] Done!
[2022-08-08 01:48:46,414] Number of parameters:3186442
[2022-08-08 01:48:46,414] ## end time: 2022-08-08 01:48:46.410329
[2022-08-08 01:48:46,415] ## used time: 0:09:58.805331
