[2022-08-08 15:22:23,410] ## start time: 2022-08-08 15:22:23.279990
[2022-08-08 15:22:23,411] Using cuda device
[2022-08-08 15:22:23,412] In train:p&d10.npy.
[2022-08-08 15:22:23,413] One Channel
[2022-08-08 15:22:23,414] With Normal data.
[2022-08-08 15:22:23,414] Nunber of classes:10.
[2022-08-08 15:22:23,415] Nunber of ViT channels:1.
[2022-08-08 15:22:23,638] Totol epochs: 15
[2022-08-08 15:22:23,640] Epoch 1---------------
[2022-08-08 15:22:23,641] lr: 2.000000e-03
[2022-08-08 15:22:24,203] loss: 2.138029  [    0/ 4702]
[2022-08-08 15:22:32,621] loss: 1.925252  [  480/ 4702]
[2022-08-08 15:22:41,041] loss: 1.897697  [  960/ 4702]
[2022-08-08 15:22:49,460] loss: 2.156003  [ 1440/ 4702]
[2022-08-08 15:22:57,878] loss: 1.067535  [ 1920/ 4702]
[2022-08-08 15:23:06,297] loss: 1.324092  [ 2400/ 4702]
[2022-08-08 15:23:14,717] loss: 1.079125  [ 2880/ 4702]
[2022-08-08 15:23:23,135] loss: 1.374185  [ 3360/ 4702]
[2022-08-08 15:23:31,555] loss: 1.082287  [ 3840/ 4702]
[2022-08-08 15:23:39,973] loss: 0.753456  [ 4320/ 4702]
[2022-08-08 15:24:18,029] Train Error: Accuracy: 74.926%, Avg loss: 0.688254
[2022-08-08 15:24:32,159] Test  Error: Accuracy: 74.243%, Avg loss: 0.773921
[2022-08-08 15:24:32,160] Epoch 2---------------
[2022-08-08 15:24:32,161] lr: 1.900000e-03
[2022-08-08 15:24:32,724] loss: 0.736264  [    0/ 4702]
[2022-08-08 15:24:41,142] loss: 0.779899  [  480/ 4702]
[2022-08-08 15:24:49,561] loss: 0.793360  [  960/ 4702]
[2022-08-08 15:24:57,979] loss: 0.580065  [ 1440/ 4702]
[2022-08-08 15:25:06,398] loss: 0.505913  [ 1920/ 4702]
[2022-08-08 15:25:14,816] loss: 0.441747  [ 2400/ 4702]
[2022-08-08 15:25:23,235] loss: 0.584397  [ 2880/ 4702]
[2022-08-08 15:25:31,653] loss: 0.624332  [ 3360/ 4702]
[2022-08-08 15:25:40,073] loss: 0.937567  [ 3840/ 4702]
[2022-08-08 15:25:48,490] loss: 0.472455  [ 4320/ 4702]
[2022-08-08 15:26:26,542] Train Error: Accuracy: 87.707%, Avg loss: 0.379124
[2022-08-08 15:26:40,667] Test  Error: Accuracy: 87.362%, Avg loss: 0.381367
[2022-08-08 15:26:40,667] Epoch 3---------------
[2022-08-08 15:26:40,668] lr: 1.805000e-03
[2022-08-08 15:26:41,231] loss: 0.469347  [    0/ 4702]
[2022-08-08 15:26:49,650] loss: 0.380570  [  480/ 4702]
[2022-08-08 15:26:58,072] loss: 0.372794  [  960/ 4702]
[2022-08-08 15:27:06,490] loss: 0.487049  [ 1440/ 4702]
[2022-08-08 15:27:14,909] loss: 0.493363  [ 1920/ 4702]
[2022-08-08 15:27:23,328] loss: 0.698243  [ 2400/ 4702]
[2022-08-08 15:27:31,745] loss: 0.639375  [ 2880/ 4702]
[2022-08-08 15:27:40,164] loss: 0.255106  [ 3360/ 4702]
[2022-08-08 15:27:48,583] loss: 0.472206  [ 3840/ 4702]
[2022-08-08 15:27:57,001] loss: 0.578973  [ 4320/ 4702]
[2022-08-08 15:28:35,056] Train Error: Accuracy: 86.963%, Avg loss: 0.362533
[2022-08-08 15:28:49,183] Test  Error: Accuracy: 86.016%, Avg loss: 0.394916
[2022-08-08 15:28:49,183] Epoch 4---------------
[2022-08-08 15:28:49,184] lr: 1.547562e-03
[2022-08-08 15:28:49,748] loss: 0.260549  [    0/ 4702]
[2022-08-08 15:28:58,166] loss: 0.306082  [  480/ 4702]
[2022-08-08 15:29:06,583] loss: 0.371641  [  960/ 4702]
[2022-08-08 15:29:15,002] loss: 0.260489  [ 1440/ 4702]
[2022-08-08 15:29:23,420] loss: 0.449092  [ 1920/ 4702]
[2022-08-08 15:29:31,837] loss: 0.216341  [ 2400/ 4702]
[2022-08-08 15:29:40,255] loss: 0.099152  [ 2880/ 4702]
[2022-08-08 15:29:48,673] loss: 0.113958  [ 3360/ 4702]
[2022-08-08 15:29:57,090] loss: 0.205827  [ 3840/ 4702]
[2022-08-08 15:30:05,509] loss: 0.238485  [ 4320/ 4702]
[2022-08-08 15:30:43,565] Train Error: Accuracy: 92.535%, Avg loss: 0.207108
[2022-08-08 15:30:57,692] Test  Error: Accuracy: 92.696%, Avg loss: 0.234202
[2022-08-08 15:30:57,693] Epoch 5---------------
[2022-08-08 15:30:57,694] lr: 1.470184e-03
[2022-08-08 15:30:58,257] loss: 0.218527  [    0/ 4702]
[2022-08-08 15:31:06,675] loss: 0.206456  [  480/ 4702]
[2022-08-08 15:31:15,092] loss: 0.071278  [  960/ 4702]
[2022-08-08 15:31:23,510] loss: 0.201045  [ 1440/ 4702]
[2022-08-08 15:31:31,928] loss: 0.209198  [ 1920/ 4702]
[2022-08-08 15:31:40,345] loss: 0.169656  [ 2400/ 4702]
[2022-08-08 15:31:48,763] loss: 0.166291  [ 2880/ 4702]
[2022-08-08 15:31:57,181] loss: 0.205280  [ 3360/ 4702]
[2022-08-08 15:32:05,599] loss: 0.065846  [ 3840/ 4702]
[2022-08-08 15:32:14,017] loss: 0.082754  [ 4320/ 4702]
[2022-08-08 15:32:52,067] Train Error: Accuracy: 91.195%, Avg loss: 0.254446
[2022-08-08 15:33:06,196] Test  Error: Accuracy: 89.572%, Avg loss: 0.297143
[2022-08-08 15:33:06,197] Epoch 6---------------
[2022-08-08 15:33:06,198] lr: 1.026684e-03
[2022-08-08 15:33:06,761] loss: 0.170211  [    0/ 4702]
[2022-08-08 15:33:15,180] loss: 0.138210  [  480/ 4702]
[2022-08-08 15:33:23,598] loss: 0.072994  [  960/ 4702]
[2022-08-08 15:33:32,016] loss: 0.178553  [ 1440/ 4702]
[2022-08-08 15:33:40,432] loss: 0.117450  [ 1920/ 4702]
[2022-08-08 15:33:48,852] loss: 0.087056  [ 2400/ 4702]
[2022-08-08 15:33:57,270] loss: 0.120898  [ 2880/ 4702]
[2022-08-08 15:34:05,688] loss: 0.085191  [ 3360/ 4702]
[2022-08-08 15:34:14,105] loss: 0.136451  [ 3840/ 4702]
[2022-08-08 15:34:22,523] loss: 0.090875  [ 4320/ 4702]
[2022-08-08 15:35:00,578] Train Error: Accuracy: 96.810%, Avg loss: 0.099989
[2022-08-08 15:35:14,704] Test  Error: Accuracy: 95.531%, Avg loss: 0.172871
[2022-08-08 15:35:14,705] Epoch 7---------------
[2022-08-08 15:35:14,706] lr: 9.753500e-04
[2022-08-08 15:35:15,268] loss: 0.107228  [    0/ 4702]
[2022-08-08 15:35:23,687] loss: 0.207607  [  480/ 4702]
[2022-08-08 15:35:32,105] loss: 0.150939  [  960/ 4702]
[2022-08-08 15:35:40,522] loss: 0.131559  [ 1440/ 4702]
[2022-08-08 15:35:48,939] loss: 0.021493  [ 1920/ 4702]
[2022-08-08 15:35:57,357] loss: 0.067149  [ 2400/ 4702]
[2022-08-08 15:36:05,774] loss: 0.112053  [ 2880/ 4702]
[2022-08-08 15:36:14,191] loss: 0.019618  [ 3360/ 4702]
[2022-08-08 15:36:22,612] loss: 0.025663  [ 3840/ 4702]
[2022-08-08 15:36:31,030] loss: 0.193611  [ 4320/ 4702]
[2022-08-08 15:37:09,082] Train Error: Accuracy: 97.937%, Avg loss: 0.073286
[2022-08-08 15:37:23,208] Test  Error: Accuracy: 96.588%, Avg loss: 0.098897
[2022-08-08 15:37:23,208] Epoch 8---------------
[2022-08-08 15:37:23,209] lr: 9.265825e-04
[2022-08-08 15:37:23,773] loss: 0.146127  [    0/ 4702]
[2022-08-08 15:37:32,191] loss: 0.066233  [  480/ 4702]
[2022-08-08 15:37:40,609] loss: 0.069219  [  960/ 4702]
[2022-08-08 15:37:49,028] loss: 0.056138  [ 1440/ 4702]
[2022-08-08 15:37:57,445] loss: 0.049706  [ 1920/ 4702]
[2022-08-08 15:38:05,863] loss: 0.082899  [ 2400/ 4702]
[2022-08-08 15:38:14,280] loss: 0.026976  [ 2880/ 4702]
[2022-08-08 15:38:22,697] loss: 0.084254  [ 3360/ 4702]
[2022-08-08 15:38:31,114] loss: 0.099189  [ 3840/ 4702]
[2022-08-08 15:38:39,532] loss: 0.016528  [ 4320/ 4702]
[2022-08-08 15:39:17,585] Train Error: Accuracy: 97.384%, Avg loss: 0.080636
[2022-08-08 15:39:31,710] Test  Error: Accuracy: 97.069%, Avg loss: 0.103139
[2022-08-08 15:39:31,711] Epoch 9---------------
[2022-08-08 15:39:31,712] lr: 7.944286e-04
[2022-08-08 15:39:32,275] loss: 0.017552  [    0/ 4702]
[2022-08-08 15:39:40,693] loss: 0.127688  [  480/ 4702]
[2022-08-08 15:39:49,111] loss: 0.002771  [  960/ 4702]
[2022-08-08 15:39:57,529] loss: 0.044259  [ 1440/ 4702]
[2022-08-08 15:40:05,948] loss: 0.052510  [ 1920/ 4702]
[2022-08-08 15:40:14,368] loss: 0.016003  [ 2400/ 4702]
[2022-08-08 15:40:22,787] loss: 0.114036  [ 2880/ 4702]
[2022-08-08 15:40:31,205] loss: 0.026499  [ 3360/ 4702]
[2022-08-08 15:40:39,623] loss: 0.025702  [ 3840/ 4702]
[2022-08-08 15:40:48,041] loss: 0.099428  [ 4320/ 4702]
[2022-08-08 15:41:26,086] Train Error: Accuracy: 98.384%, Avg loss: 0.047740
[2022-08-08 15:41:40,210] Test  Error: Accuracy: 97.549%, Avg loss: 0.079479
[2022-08-08 15:41:40,210] Epoch 10---------------
[2022-08-08 15:41:40,211] lr: 7.547072e-04
[2022-08-08 15:41:40,775] loss: 0.046419  [    0/ 4702]
[2022-08-08 15:41:49,192] loss: 0.002025  [  480/ 4702]
[2022-08-08 15:41:57,610] loss: 0.019382  [  960/ 4702]
[2022-08-08 15:42:06,027] loss: 0.015339  [ 1440/ 4702]
[2022-08-08 15:42:14,443] loss: 0.011364  [ 1920/ 4702]
[2022-08-08 15:42:22,860] loss: 0.008197  [ 2400/ 4702]
[2022-08-08 15:42:31,279] loss: 0.005463  [ 2880/ 4702]
[2022-08-08 15:42:39,697] loss: 0.046587  [ 3360/ 4702]
[2022-08-08 15:42:48,115] loss: 0.017632  [ 3840/ 4702]
[2022-08-08 15:42:56,531] loss: 0.033026  [ 4320/ 4702]
[2022-08-08 15:43:34,583] Train Error: Accuracy: 99.277%, Avg loss: 0.026432
[2022-08-08 15:43:48,708] Test  Error: Accuracy: 97.645%, Avg loss: 0.069832
[2022-08-08 15:43:48,708] Epoch 11---------------
[2022-08-08 15:43:48,709] lr: 7.169718e-04
[2022-08-08 15:43:49,273] loss: 0.049159  [    0/ 4702]
[2022-08-08 15:43:57,690] loss: 0.024953  [  480/ 4702]
[2022-08-08 15:44:06,107] loss: 0.015213  [  960/ 4702]
[2022-08-08 15:44:14,524] loss: 0.007970  [ 1440/ 4702]
[2022-08-08 15:44:22,943] loss: 0.003340  [ 1920/ 4702]
[2022-08-08 15:44:31,361] loss: 0.137696  [ 2400/ 4702]
[2022-08-08 15:44:39,778] loss: 0.073458  [ 2880/ 4702]
[2022-08-08 15:44:48,198] loss: 0.381421  [ 3360/ 4702]
[2022-08-08 15:44:56,616] loss: 0.015000  [ 3840/ 4702]
[2022-08-08 15:45:05,036] loss: 0.016640  [ 4320/ 4702]
[2022-08-08 15:45:43,087] Train Error: Accuracy: 98.660%, Avg loss: 0.043370
[2022-08-08 15:45:57,212] Test  Error: Accuracy: 97.165%, Avg loss: 0.134629
[2022-08-08 15:45:57,214] Epoch 12---------------
[2022-08-08 15:45:57,214] lr: 5.006882e-04
[2022-08-08 15:45:57,778] loss: 0.009907  [    0/ 4702]
[2022-08-08 15:46:06,198] loss: 0.070413  [  480/ 4702]
[2022-08-08 15:46:14,617] loss: 0.069737  [  960/ 4702]
[2022-08-08 15:46:23,035] loss: 0.034346  [ 1440/ 4702]
[2022-08-08 15:46:31,454] loss: 0.027372  [ 1920/ 4702]
[2022-08-08 15:46:39,871] loss: 0.066101  [ 2400/ 4702]
[2022-08-08 15:46:48,290] loss: 0.010604  [ 2880/ 4702]
[2022-08-08 15:46:56,709] loss: 0.017313  [ 3360/ 4702]
[2022-08-08 15:47:05,129] loss: 0.048299  [ 3840/ 4702]
[2022-08-08 15:47:13,550] loss: 0.005582  [ 4320/ 4702]
[2022-08-08 15:47:51,606] Train Error: Accuracy: 99.128%, Avg loss: 0.026281
[2022-08-08 15:48:05,733] Test  Error: Accuracy: 98.462%, Avg loss: 0.064717
[2022-08-08 15:48:05,733] Epoch 13---------------
[2022-08-08 15:48:05,734] lr: 4.756538e-04
[2022-08-08 15:48:06,297] loss: 0.003479  [    0/ 4702]
[2022-08-08 15:48:14,717] loss: 0.067814  [  480/ 4702]
[2022-08-08 15:48:23,133] loss: 0.015150  [  960/ 4702]
[2022-08-08 15:48:31,551] loss: 0.041707  [ 1440/ 4702]
[2022-08-08 15:48:39,969] loss: 0.019870  [ 1920/ 4702]
[2022-08-08 15:48:48,387] loss: 0.047826  [ 2400/ 4702]
[2022-08-08 15:48:56,808] loss: 0.004120  [ 2880/ 4702]
[2022-08-08 15:49:05,226] loss: 0.007462  [ 3360/ 4702]
[2022-08-08 15:49:13,644] loss: 0.005280  [ 3840/ 4702]
[2022-08-08 15:49:22,061] loss: 0.016555  [ 4320/ 4702]
[2022-08-08 15:50:00,109] Train Error: Accuracy: 99.447%, Avg loss: 0.020164
[2022-08-08 15:50:14,232] Test  Error: Accuracy: 98.270%, Avg loss: 0.053857
[2022-08-08 15:50:14,233] Epoch 14---------------
[2022-08-08 15:50:14,234] lr: 4.518711e-04
[2022-08-08 15:50:14,797] loss: 0.046532  [    0/ 4702]
[2022-08-08 15:50:23,216] loss: 0.009585  [  480/ 4702]
[2022-08-08 15:50:31,633] loss: 0.032308  [  960/ 4702]
[2022-08-08 15:50:40,051] loss: 0.005023  [ 1440/ 4702]
[2022-08-08 15:50:48,469] loss: 0.011855  [ 1920/ 4702]
[2022-08-08 15:50:56,889] loss: 0.005053  [ 2400/ 4702]
[2022-08-08 15:51:05,307] loss: 0.000833  [ 2880/ 4702]
[2022-08-08 15:51:13,725] loss: 0.094067  [ 3360/ 4702]
[2022-08-08 15:51:22,143] loss: 0.002199  [ 3840/ 4702]
[2022-08-08 15:51:30,561] loss: 0.018288  [ 4320/ 4702]
[2022-08-08 15:52:08,614] Train Error: Accuracy: 99.702%, Avg loss: 0.013019
[2022-08-08 15:52:22,740] Test  Error: Accuracy: 98.847%, Avg loss: 0.041785
[2022-08-08 15:52:22,741] Epoch 15---------------
[2022-08-08 15:52:22,743] lr: 4.292775e-04
[2022-08-08 15:52:23,306] loss: 0.002740  [    0/ 4702]
[2022-08-08 15:52:31,723] loss: 0.011624  [  480/ 4702]
[2022-08-08 15:52:40,141] loss: 0.001376  [  960/ 4702]
[2022-08-08 15:52:48,559] loss: 0.019240  [ 1440/ 4702]
[2022-08-08 15:52:56,976] loss: 0.003789  [ 1920/ 4702]
[2022-08-08 15:53:05,396] loss: 0.012796  [ 2400/ 4702]
[2022-08-08 15:53:13,814] loss: 0.002079  [ 2880/ 4702]
[2022-08-08 15:53:22,232] loss: 0.020565  [ 3360/ 4702]
[2022-08-08 15:53:30,651] loss: 0.064660  [ 3840/ 4702]
[2022-08-08 15:53:39,070] loss: 0.008391  [ 4320/ 4702]
[2022-08-08 15:54:17,125] Train Error: Accuracy: 99.787%, Avg loss: 0.009803
[2022-08-08 15:54:31,249] Test  Error: Accuracy: 98.703%, Avg loss: 0.038044
[2022-08-08 15:54:31,250] Done!
[2022-08-08 15:54:31,254] Number of parameters:1263370
[2022-08-08 15:54:31,254] ## end time: 2022-08-08 15:54:31.250474
[2022-08-08 15:54:31,255] ## used time: 0:32:07.970484
