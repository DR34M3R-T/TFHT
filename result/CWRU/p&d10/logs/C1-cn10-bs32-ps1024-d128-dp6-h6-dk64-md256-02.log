[2022-08-08 02:33:19,087] ## start time: 2022-08-08 02:33:18.941638
[2022-08-08 02:33:19,087] Using cuda device
[2022-08-08 02:33:19,088] In train:p&d10.npy.
[2022-08-08 02:33:19,090] One Channel
[2022-08-08 02:33:19,090] With Normal data.
[2022-08-08 02:33:19,091] Nunber of classes:10.
[2022-08-08 02:33:19,091] Nunber of ViT channels:1.
[2022-08-08 02:33:19,344] Totol epochs: 15
[2022-08-08 02:33:19,348] Epoch 1---------------
[2022-08-08 02:33:19,348] lr: 2.000000e-03
[2022-08-08 02:33:19,388] loss: 2.380498  [    0/ 4709]
[2022-08-08 02:33:19,941] loss: 1.484959  [  480/ 4709]
[2022-08-08 02:33:20,497] loss: 1.321700  [  960/ 4709]
[2022-08-08 02:33:21,039] loss: 1.212048  [ 1440/ 4709]
[2022-08-08 02:33:21,570] loss: 1.040146  [ 1920/ 4709]
[2022-08-08 02:33:22,099] loss: 0.147164  [ 2400/ 4709]
[2022-08-08 02:33:22,643] loss: 0.021906  [ 2880/ 4709]
[2022-08-08 02:33:23,202] loss: 0.020504  [ 3360/ 4709]
[2022-08-08 02:33:23,742] loss: 0.067043  [ 3840/ 4709]
[2022-08-08 02:33:24,275] loss: 0.013875  [ 4320/ 4709]
[2022-08-08 02:33:26,075] Train Error: Accuracy: 99.915%, Avg loss: 0.010871
[2022-08-08 02:33:26,677] Test  Error: Accuracy: 99.904%, Avg loss: 0.011375
[2022-08-08 02:33:26,678] Epoch 2---------------
[2022-08-08 02:33:26,679] lr: 1.900000e-03
[2022-08-08 02:33:26,716] loss: 0.013321  [    0/ 4709]
[2022-08-08 02:33:27,247] loss: 0.012753  [  480/ 4709]
[2022-08-08 02:33:27,799] loss: 0.005142  [  960/ 4709]
[2022-08-08 02:33:28,339] loss: 0.004056  [ 1440/ 4709]
[2022-08-08 02:33:28,873] loss: 1.054182  [ 1920/ 4709]
[2022-08-08 02:33:29,405] loss: 0.030241  [ 2400/ 4709]
[2022-08-08 02:33:29,938] loss: 0.124509  [ 2880/ 4709]
[2022-08-08 02:33:30,470] loss: 0.008036  [ 3360/ 4709]
[2022-08-08 02:33:31,000] loss: 0.007964  [ 3840/ 4709]
[2022-08-08 02:33:31,535] loss: 0.005149  [ 4320/ 4709]
[2022-08-08 02:33:33,338] Train Error: Accuracy: 99.979%, Avg loss: 0.004442
[2022-08-08 02:33:33,941] Test  Error: Accuracy: 99.904%, Avg loss: 0.005501
[2022-08-08 02:33:33,941] Epoch 3---------------
[2022-08-08 02:33:33,942] lr: 1.805000e-03
[2022-08-08 02:33:33,980] loss: 0.002719  [    0/ 4709]
[2022-08-08 02:33:34,513] loss: 0.002922  [  480/ 4709]
[2022-08-08 02:33:35,046] loss: 0.002761  [  960/ 4709]
[2022-08-08 02:33:35,575] loss: 0.002303  [ 1440/ 4709]
[2022-08-08 02:33:36,105] loss: 0.002147  [ 1920/ 4709]
[2022-08-08 02:33:36,637] loss: 0.003595  [ 2400/ 4709]
[2022-08-08 02:33:37,173] loss: 0.001955  [ 2880/ 4709]
[2022-08-08 02:33:37,704] loss: 0.001635  [ 3360/ 4709]
[2022-08-08 02:33:38,238] loss: 0.001281  [ 3840/ 4709]
[2022-08-08 02:33:38,774] loss: 0.001277  [ 4320/ 4709]
[2022-08-08 02:33:40,570] Train Error: Accuracy: 100.000%, Avg loss: 0.001618
[2022-08-08 02:33:41,191] Test  Error: Accuracy: 100.000%, Avg loss: 0.002037
[2022-08-08 02:33:41,192] Epoch 4---------------
[2022-08-08 02:33:41,193] lr: 1.714750e-03
[2022-08-08 02:33:41,230] loss: 0.001430  [    0/ 4709]
[2022-08-08 02:33:41,760] loss: 0.001148  [  480/ 4709]
[2022-08-08 02:33:42,296] loss: 0.001192  [  960/ 4709]
[2022-08-08 02:33:42,830] loss: 0.001352  [ 1440/ 4709]
[2022-08-08 02:33:43,368] loss: 0.001036  [ 1920/ 4709]
[2022-08-08 02:33:43,897] loss: 0.001742  [ 2400/ 4709]
[2022-08-08 02:33:44,429] loss: 0.001373  [ 2880/ 4709]
[2022-08-08 02:33:44,960] loss: 0.000998  [ 3360/ 4709]
[2022-08-08 02:33:45,492] loss: 0.037806  [ 3840/ 4709]
[2022-08-08 02:33:46,025] loss: 0.001635  [ 4320/ 4709]
[2022-08-08 02:33:47,821] Train Error: Accuracy: 100.000%, Avg loss: 0.001479
[2022-08-08 02:33:48,425] Test  Error: Accuracy: 100.000%, Avg loss: 0.001956
[2022-08-08 02:33:48,425] Epoch 5---------------
[2022-08-08 02:33:48,426] lr: 1.629012e-03
[2022-08-08 02:33:48,466] loss: 0.001073  [    0/ 4709]
[2022-08-08 02:33:49,001] loss: 0.000949  [  480/ 4709]
[2022-08-08 02:33:49,536] loss: 0.001038  [  960/ 4709]
[2022-08-08 02:33:50,069] loss: 0.001321  [ 1440/ 4709]
[2022-08-08 02:33:50,602] loss: 0.000794  [ 1920/ 4709]
[2022-08-08 02:33:51,135] loss: 0.000830  [ 2400/ 4709]
[2022-08-08 02:33:51,665] loss: 0.001211  [ 2880/ 4709]
[2022-08-08 02:33:52,204] loss: 0.000722  [ 3360/ 4709]
[2022-08-08 02:33:52,740] loss: 0.000956  [ 3840/ 4709]
[2022-08-08 02:33:53,272] loss: 0.000714  [ 4320/ 4709]
[2022-08-08 02:33:55,080] Train Error: Accuracy: 100.000%, Avg loss: 0.000770
[2022-08-08 02:33:55,679] Test  Error: Accuracy: 100.000%, Avg loss: 0.001062
[2022-08-08 02:33:55,680] Epoch 6---------------
[2022-08-08 02:33:55,681] lr: 1.547562e-03
[2022-08-08 02:33:55,718] loss: 0.001130  [    0/ 4709]
[2022-08-08 02:33:56,253] loss: 0.000765  [  480/ 4709]
[2022-08-08 02:33:56,787] loss: 0.000592  [  960/ 4709]
[2022-08-08 02:33:57,321] loss: 0.000565  [ 1440/ 4709]
[2022-08-08 02:33:57,854] loss: 0.000673  [ 1920/ 4709]
[2022-08-08 02:33:58,389] loss: 0.000779  [ 2400/ 4709]
[2022-08-08 02:33:58,922] loss: 0.000519  [ 2880/ 4709]
[2022-08-08 02:33:59,454] loss: 0.000591  [ 3360/ 4709]
[2022-08-08 02:33:59,985] loss: 0.001000  [ 3840/ 4709]
[2022-08-08 02:34:00,518] loss: 0.000792  [ 4320/ 4709]
[2022-08-08 02:34:02,314] Train Error: Accuracy: 100.000%, Avg loss: 0.000644
[2022-08-08 02:34:02,924] Test  Error: Accuracy: 100.000%, Avg loss: 0.001125
[2022-08-08 02:34:02,925] Epoch 7---------------
[2022-08-08 02:34:02,926] lr: 1.326841e-03
[2022-08-08 02:34:02,963] loss: 0.000677  [    0/ 4709]
[2022-08-08 02:34:03,507] loss: 0.000516  [  480/ 4709]
[2022-08-08 02:34:04,051] loss: 0.001137  [  960/ 4709]
[2022-08-08 02:34:04,591] loss: 0.000503  [ 1440/ 4709]
[2022-08-08 02:34:05,126] loss: 0.001238  [ 1920/ 4709]
[2022-08-08 02:34:05,656] loss: 0.000499  [ 2400/ 4709]
[2022-08-08 02:34:06,185] loss: 0.000666  [ 2880/ 4709]
[2022-08-08 02:34:06,717] loss: 0.000606  [ 3360/ 4709]
[2022-08-08 02:34:07,250] loss: 0.000488  [ 3840/ 4709]
[2022-08-08 02:34:07,781] loss: 0.000949  [ 4320/ 4709]
[2022-08-08 02:34:09,579] Train Error: Accuracy: 100.000%, Avg loss: 0.000597
[2022-08-08 02:34:10,189] Test  Error: Accuracy: 100.000%, Avg loss: 0.000991
[2022-08-08 02:34:10,189] Epoch 8---------------
[2022-08-08 02:34:10,190] lr: 1.260499e-03
[2022-08-08 02:34:10,227] loss: 0.000378  [    0/ 4709]
[2022-08-08 02:34:10,761] loss: 0.000604  [  480/ 4709]
[2022-08-08 02:34:11,307] loss: 0.000494  [  960/ 4709]
[2022-08-08 02:34:11,842] loss: 0.000490  [ 1440/ 4709]
[2022-08-08 02:34:12,376] loss: 0.000716  [ 1920/ 4709]
[2022-08-08 02:34:12,910] loss: 0.000611  [ 2400/ 4709]
[2022-08-08 02:34:13,440] loss: 0.000438  [ 2880/ 4709]
[2022-08-08 02:34:13,973] loss: 0.000481  [ 3360/ 4709]
[2022-08-08 02:34:14,537] loss: 0.000544  [ 3840/ 4709]
[2022-08-08 02:34:15,107] loss: 0.000464  [ 4320/ 4709]
[2022-08-08 02:34:16,949] Train Error: Accuracy: 100.000%, Avg loss: 0.000575
[2022-08-08 02:34:17,552] Test  Error: Accuracy: 99.952%, Avg loss: 0.001192
[2022-08-08 02:34:17,553] Epoch 9---------------
[2022-08-08 02:34:17,554] lr: 9.753500e-04
[2022-08-08 02:34:17,593] loss: 0.000570  [    0/ 4709]
[2022-08-08 02:34:18,129] loss: 0.000629  [  480/ 4709]
[2022-08-08 02:34:18,666] loss: 0.000385  [  960/ 4709]
[2022-08-08 02:34:19,204] loss: 0.000613  [ 1440/ 4709]
[2022-08-08 02:34:19,745] loss: 0.000643  [ 1920/ 4709]
[2022-08-08 02:34:20,274] loss: 0.000471  [ 2400/ 4709]
[2022-08-08 02:34:20,807] loss: 0.000479  [ 2880/ 4709]
[2022-08-08 02:34:21,338] loss: 0.001209  [ 3360/ 4709]
[2022-08-08 02:34:21,868] loss: 0.000533  [ 3840/ 4709]
[2022-08-08 02:34:22,398] loss: 0.000358  [ 4320/ 4709]
[2022-08-08 02:34:24,202] Train Error: Accuracy: 100.000%, Avg loss: 0.000489
[2022-08-08 02:34:24,810] Test  Error: Accuracy: 99.952%, Avg loss: 0.001622
[2022-08-08 02:34:24,811] Epoch 10---------------
[2022-08-08 02:34:24,812] lr: 6.811233e-04
[2022-08-08 02:34:24,849] loss: 0.000499  [    0/ 4709]
[2022-08-08 02:34:25,383] loss: 0.000502  [  480/ 4709]
[2022-08-08 02:34:25,914] loss: 0.000541  [  960/ 4709]
[2022-08-08 02:34:26,446] loss: 0.000454  [ 1440/ 4709]
[2022-08-08 02:34:26,981] loss: 0.000470  [ 1920/ 4709]
[2022-08-08 02:34:27,511] loss: 0.000423  [ 2400/ 4709]
[2022-08-08 02:34:28,042] loss: 0.000521  [ 2880/ 4709]
[2022-08-08 02:34:28,572] loss: 0.000445  [ 3360/ 4709]
[2022-08-08 02:34:29,108] loss: 0.000379  [ 3840/ 4709]
[2022-08-08 02:34:29,641] loss: 0.000587  [ 4320/ 4709]
[2022-08-08 02:34:31,433] Train Error: Accuracy: 100.000%, Avg loss: 0.000501
[2022-08-08 02:34:32,032] Test  Error: Accuracy: 99.952%, Avg loss: 0.001443
[2022-08-08 02:34:32,033] Epoch 11---------------
[2022-08-08 02:34:32,034] lr: 6.470671e-04
[2022-08-08 02:34:32,074] loss: 0.000405  [    0/ 4709]
[2022-08-08 02:34:32,610] loss: 0.000599  [  480/ 4709]
[2022-08-08 02:34:33,144] loss: 0.000652  [  960/ 4709]
[2022-08-08 02:34:33,679] loss: 0.000601  [ 1440/ 4709]
[2022-08-08 02:34:34,212] loss: 0.000463  [ 1920/ 4709]
[2022-08-08 02:34:34,747] loss: 0.000599  [ 2400/ 4709]
[2022-08-08 02:34:35,278] loss: 0.000485  [ 2880/ 4709]
[2022-08-08 02:34:35,809] loss: 0.000503  [ 3360/ 4709]
[2022-08-08 02:34:36,338] loss: 0.000604  [ 3840/ 4709]
[2022-08-08 02:34:36,873] loss: 0.000507  [ 4320/ 4709]
[2022-08-08 02:34:38,683] Train Error: Accuracy: 100.000%, Avg loss: 0.000442
[2022-08-08 02:34:39,283] Test  Error: Accuracy: 99.904%, Avg loss: 0.002627
[2022-08-08 02:34:39,284] Epoch 12---------------
[2022-08-08 02:34:39,285] lr: 4.518711e-04
[2022-08-08 02:34:39,323] loss: 0.000407  [    0/ 4709]
[2022-08-08 02:34:39,857] loss: 0.000503  [  480/ 4709]
[2022-08-08 02:34:40,387] loss: 0.000383  [  960/ 4709]
[2022-08-08 02:34:40,921] loss: 0.000392  [ 1440/ 4709]
[2022-08-08 02:34:41,451] loss: 0.000514  [ 1920/ 4709]
[2022-08-08 02:34:41,982] loss: 0.000413  [ 2400/ 4709]
[2022-08-08 02:34:42,512] loss: 0.000479  [ 2880/ 4709]
[2022-08-08 02:34:43,048] loss: 0.000364  [ 3360/ 4709]
[2022-08-08 02:34:43,581] loss: 0.000387  [ 3840/ 4709]
[2022-08-08 02:34:44,112] loss: 0.000364  [ 4320/ 4709]
[2022-08-08 02:34:45,905] Train Error: Accuracy: 100.000%, Avg loss: 0.000434
[2022-08-08 02:34:46,507] Test  Error: Accuracy: 99.952%, Avg loss: 0.001033
[2022-08-08 02:34:46,508] Epoch 13---------------
[2022-08-08 02:34:46,509] lr: 4.292775e-04
[2022-08-08 02:34:46,546] loss: 0.000390  [    0/ 4709]
[2022-08-08 02:34:47,082] loss: 0.000439  [  480/ 4709]
[2022-08-08 02:34:47,620] loss: 0.000354  [  960/ 4709]
[2022-08-08 02:34:48,155] loss: 0.000353  [ 1440/ 4709]
[2022-08-08 02:34:48,688] loss: 0.000275  [ 1920/ 4709]
[2022-08-08 02:34:49,220] loss: 0.000473  [ 2400/ 4709]
[2022-08-08 02:34:49,753] loss: 0.000347  [ 2880/ 4709]
[2022-08-08 02:34:50,287] loss: 0.000379  [ 3360/ 4709]
[2022-08-08 02:34:50,832] loss: 0.000509  [ 3840/ 4709]
[2022-08-08 02:34:51,362] loss: 0.000342  [ 4320/ 4709]
[2022-08-08 02:34:53,167] Train Error: Accuracy: 100.000%, Avg loss: 0.000420
[2022-08-08 02:34:53,770] Test  Error: Accuracy: 99.952%, Avg loss: 0.001648
[2022-08-08 02:34:53,770] Epoch 14---------------
[2022-08-08 02:34:53,771] lr: 2.997805e-04
[2022-08-08 02:34:53,810] loss: 0.000462  [    0/ 4709]
[2022-08-08 02:34:54,344] loss: 0.000320  [  480/ 4709]
[2022-08-08 02:34:54,884] loss: 0.000297  [  960/ 4709]
[2022-08-08 02:34:55,418] loss: 0.000531  [ 1440/ 4709]
[2022-08-08 02:34:55,952] loss: 0.000397  [ 1920/ 4709]
[2022-08-08 02:34:56,482] loss: 0.000456  [ 2400/ 4709]
[2022-08-08 02:34:57,017] loss: 0.000327  [ 2880/ 4709]
[2022-08-08 02:34:57,548] loss: 0.000344  [ 3360/ 4709]
[2022-08-08 02:34:58,078] loss: 0.000608  [ 3840/ 4709]
[2022-08-08 02:34:58,610] loss: 0.000469  [ 4320/ 4709]
[2022-08-08 02:35:00,406] Train Error: Accuracy: 100.000%, Avg loss: 0.000399
[2022-08-08 02:35:01,010] Test  Error: Accuracy: 99.952%, Avg loss: 0.001296
[2022-08-08 02:35:01,011] Epoch 15---------------
[2022-08-08 02:35:01,012] lr: 2.847915e-04
[2022-08-08 02:35:01,050] loss: 0.000329  [    0/ 4709]
[2022-08-08 02:35:01,603] loss: 0.000582  [  480/ 4709]
[2022-08-08 02:35:02,141] loss: 0.000364  [  960/ 4709]
[2022-08-08 02:35:02,679] loss: 0.000368  [ 1440/ 4709]
[2022-08-08 02:35:03,211] loss: 0.000438  [ 1920/ 4709]
[2022-08-08 02:35:03,741] loss: 0.000363  [ 2400/ 4709]
[2022-08-08 02:35:04,272] loss: 0.000363  [ 2880/ 4709]
[2022-08-08 02:35:04,809] loss: 0.000311  [ 3360/ 4709]
[2022-08-08 02:35:05,340] loss: 0.000328  [ 3840/ 4709]
[2022-08-08 02:35:05,870] loss: 0.000380  [ 4320/ 4709]
[2022-08-08 02:35:07,672] Train Error: Accuracy: 100.000%, Avg loss: 0.000403
[2022-08-08 02:35:08,273] Test  Error: Accuracy: 100.000%, Avg loss: 0.000871
[2022-08-08 02:35:08,274] Done!
[2022-08-08 02:35:08,279] Number of parameters:3424522
[2022-08-08 02:35:08,279] ## end time: 2022-08-08 02:35:08.274875
[2022-08-08 02:35:08,281] ## used time: 0:01:49.333237
