[2022-08-07 20:32:37,196] ## start time: 2022-08-07 20:32:37.050169
[2022-08-07 20:32:37,196] Using cuda device
[2022-08-07 20:32:37,197] In train:p&d10.npy.
[2022-08-07 20:32:37,198] One Channel
[2022-08-07 20:32:37,199] With Normal data.
[2022-08-07 20:32:37,199] Nunber of classes:10.
[2022-08-07 20:32:37,199] Nunber of ViT channels:1.
[2022-08-07 20:32:37,463] Totol epochs: 15
[2022-08-07 20:32:37,465] Epoch 1---------------
[2022-08-07 20:32:37,466] lr: 2.000000e-03
[2022-08-07 20:32:39,282] loss: 2.602132  [    0/ 4811]
[2022-08-07 20:33:06,524] loss: 2.130434  [  480/ 4811]
[2022-08-07 20:33:33,767] loss: 1.834329  [  960/ 4811]
[2022-08-07 20:34:01,008] loss: 1.714288  [ 1440/ 4811]
[2022-08-07 20:34:28,250] loss: 1.356021  [ 1920/ 4811]
[2022-08-07 20:34:55,492] loss: 1.151524  [ 2400/ 4811]
[2022-08-07 20:35:22,734] loss: 0.701926  [ 2880/ 4811]
[2022-08-07 20:35:49,974] loss: 0.580572  [ 3360/ 4811]
[2022-08-07 20:36:17,214] loss: 0.949228  [ 3840/ 4811]
[2022-08-07 20:36:44,570] loss: 0.328866  [ 4320/ 4811]
[2022-08-07 20:37:10,802] loss: 0.359101  [ 4800/ 4811]
[2022-08-07 20:38:51,754] Train Error: Accuracy: 79.194%, Avg loss: 0.588731
[2022-08-07 20:39:33,132] Test  Error: Accuracy: 76.318%, Avg loss: 0.684128
[2022-08-07 20:39:33,133] Epoch 2---------------
[2022-08-07 20:39:33,134] lr: 1.900000e-03
[2022-08-07 20:39:34,965] loss: 0.455265  [    0/ 4811]
[2022-08-07 20:40:02,391] loss: 0.293564  [  480/ 4811]
[2022-08-07 20:40:29,814] loss: 0.262988  [  960/ 4811]
[2022-08-07 20:40:57,238] loss: 0.128233  [ 1440/ 4811]
[2022-08-07 20:41:24,665] loss: 0.378336  [ 1920/ 4811]
[2022-08-07 20:41:52,091] loss: 0.139209  [ 2400/ 4811]
[2022-08-07 20:42:19,517] loss: 0.176662  [ 2880/ 4811]
[2022-08-07 20:42:46,944] loss: 0.156115  [ 3360/ 4811]
[2022-08-07 20:43:14,371] loss: 0.247609  [ 3840/ 4811]
[2022-08-07 20:43:41,798] loss: 0.201608  [ 4320/ 4811]
[2022-08-07 20:44:08,030] loss: 0.102045  [ 4800/ 4811]
[2022-08-07 20:45:48,983] Train Error: Accuracy: 92.268%, Avg loss: 0.231343
[2022-08-07 20:46:30,360] Test  Error: Accuracy: 90.720%, Avg loss: 0.275399
[2022-08-07 20:46:30,361] Epoch 3---------------
[2022-08-07 20:46:30,362] lr: 1.805000e-03
[2022-08-07 20:46:32,193] loss: 0.271959  [    0/ 4811]
[2022-08-07 20:46:59,619] loss: 0.061699  [  480/ 4811]
[2022-08-07 20:47:27,044] loss: 0.208885  [  960/ 4811]
[2022-08-07 20:47:54,468] loss: 0.447271  [ 1440/ 4811]
[2022-08-07 20:48:21,893] loss: 0.323482  [ 1920/ 4811]
[2022-08-07 20:48:49,318] loss: 0.154602  [ 2400/ 4811]
[2022-08-07 20:49:16,744] loss: 0.059523  [ 2880/ 4811]
[2022-08-07 20:49:44,169] loss: 0.151079  [ 3360/ 4811]
[2022-08-07 20:50:11,595] loss: 0.209761  [ 3840/ 4811]
[2022-08-07 20:50:39,020] loss: 0.136400  [ 4320/ 4811]
[2022-08-07 20:51:05,250] loss: 0.110099  [ 4800/ 4811]
[2022-08-07 20:52:46,208] Train Error: Accuracy: 85.221%, Avg loss: 0.365584
[2022-08-07 20:53:27,583] Test  Error: Accuracy: 83.114%, Avg loss: 0.438926
[2022-08-07 20:53:27,583] Epoch 4---------------
[2022-08-07 20:53:27,584] lr: 1.260499e-03
[2022-08-07 20:53:29,415] loss: 0.372142  [    0/ 4811]
[2022-08-07 20:53:56,839] loss: 0.023925  [  480/ 4811]
[2022-08-07 20:54:24,263] loss: 0.024205  [  960/ 4811]
[2022-08-07 20:54:51,691] loss: 0.042701  [ 1440/ 4811]
[2022-08-07 20:55:19,115] loss: 0.223375  [ 1920/ 4811]
[2022-08-07 20:55:46,540] loss: 0.169209  [ 2400/ 4811]
[2022-08-07 20:56:13,965] loss: 0.020862  [ 2880/ 4811]
[2022-08-07 20:56:41,389] loss: 0.056875  [ 3360/ 4811]
[2022-08-07 20:57:08,813] loss: 0.019499  [ 3840/ 4811]
[2022-08-07 20:57:36,238] loss: 0.008067  [ 4320/ 4811]
[2022-08-07 20:58:02,470] loss: 0.002719  [ 4800/ 4811]
[2022-08-07 20:59:43,405] Train Error: Accuracy: 99.065%, Avg loss: 0.031461
[2022-08-07 21:00:24,777] Test  Error: Accuracy: 97.667%, Avg loss: 0.083215
[2022-08-07 21:00:24,778] Epoch 5---------------
[2022-08-07 21:00:24,779] lr: 1.197474e-03
[2022-08-07 21:00:26,609] loss: 0.025672  [    0/ 4811]
[2022-08-07 21:00:54,032] loss: 0.046241  [  480/ 4811]
[2022-08-07 21:01:21,457] loss: 0.008103  [  960/ 4811]
[2022-08-07 21:01:48,884] loss: 0.005449  [ 1440/ 4811]
[2022-08-07 21:02:16,309] loss: 0.020833  [ 1920/ 4811]
[2022-08-07 21:02:43,735] loss: 0.038480  [ 2400/ 4811]
[2022-08-07 21:03:11,160] loss: 0.517311  [ 2880/ 4811]
[2022-08-07 21:03:38,586] loss: 0.025633  [ 3360/ 4811]
[2022-08-07 21:04:06,012] loss: 0.080325  [ 3840/ 4811]
[2022-08-07 21:04:33,436] loss: 0.028295  [ 4320/ 4811]
[2022-08-07 21:04:59,666] loss: 0.011673  [ 4800/ 4811]
[2022-08-07 21:06:40,604] Train Error: Accuracy: 98.005%, Avg loss: 0.058125
[2022-08-07 21:07:21,977] Test  Error: Accuracy: 96.602%, Avg loss: 0.107588
[2022-08-07 21:07:21,978] Epoch 6---------------
[2022-08-07 21:07:21,979] lr: 8.362407e-04
[2022-08-07 21:07:23,809] loss: 0.005011  [    0/ 4811]
[2022-08-07 21:07:51,234] loss: 0.032437  [  480/ 4811]
[2022-08-07 21:08:18,661] loss: 0.005381  [  960/ 4811]
[2022-08-07 21:08:46,087] loss: 0.012132  [ 1440/ 4811]
[2022-08-07 21:09:13,513] loss: 0.135127  [ 1920/ 4811]
[2022-08-07 21:09:40,939] loss: 0.014463  [ 2400/ 4811]
[2022-08-07 21:10:08,363] loss: 0.009137  [ 2880/ 4811]
[2022-08-07 21:10:35,789] loss: 0.015713  [ 3360/ 4811]
[2022-08-07 21:11:03,217] loss: 0.006277  [ 3840/ 4811]
[2022-08-07 21:11:30,643] loss: 0.036595  [ 4320/ 4811]
[2022-08-07 21:11:56,873] loss: 0.003191  [ 4800/ 4811]
[2022-08-07 21:13:37,810] Train Error: Accuracy: 99.605%, Avg loss: 0.013974
[2022-08-07 21:14:19,183] Test  Error: Accuracy: 98.986%, Avg loss: 0.047819
[2022-08-07 21:14:19,183] Epoch 7---------------
[2022-08-07 21:14:19,184] lr: 7.944286e-04
[2022-08-07 21:14:21,013] loss: 0.003758  [    0/ 4811]
[2022-08-07 21:14:48,440] loss: 0.016288  [  480/ 4811]
[2022-08-07 21:15:15,865] loss: 0.006972  [  960/ 4811]
[2022-08-07 21:15:43,289] loss: 0.005672  [ 1440/ 4811]
[2022-08-07 21:16:10,714] loss: 0.004795  [ 1920/ 4811]
[2022-08-07 21:16:38,138] loss: 0.004184  [ 2400/ 4811]
[2022-08-07 21:17:05,565] loss: 0.001901  [ 2880/ 4811]
[2022-08-07 21:17:32,989] loss: 0.004918  [ 3360/ 4811]
[2022-08-07 21:18:00,414] loss: 0.009781  [ 3840/ 4811]
[2022-08-07 21:18:27,838] loss: 0.002403  [ 4320/ 4811]
[2022-08-07 21:18:54,067] loss: 0.003607  [ 4800/ 4811]
[2022-08-07 21:20:35,002] Train Error: Accuracy: 99.771%, Avg loss: 0.010211
[2022-08-07 21:21:16,371] Test  Error: Accuracy: 99.138%, Avg loss: 0.045353
[2022-08-07 21:21:16,371] Epoch 8---------------
[2022-08-07 21:21:16,372] lr: 7.547072e-04
[2022-08-07 21:21:18,202] loss: 0.001559  [    0/ 4811]
[2022-08-07 21:21:45,627] loss: 0.004747  [  480/ 4811]
[2022-08-07 21:22:13,050] loss: 0.009005  [  960/ 4811]
[2022-08-07 21:22:40,472] loss: 0.025257  [ 1440/ 4811]
[2022-08-07 21:23:07,896] loss: 0.010033  [ 1920/ 4811]
[2022-08-07 21:23:35,320] loss: 0.001055  [ 2400/ 4811]
[2022-08-07 21:24:02,744] loss: 0.001064  [ 2880/ 4811]
[2022-08-07 21:24:30,168] loss: 0.003378  [ 3360/ 4811]
[2022-08-07 21:24:57,592] loss: 0.001715  [ 3840/ 4811]
[2022-08-07 21:25:25,016] loss: 0.008971  [ 4320/ 4811]
[2022-08-07 21:25:51,247] loss: 0.005635  [ 4800/ 4811]
[2022-08-07 21:27:32,181] Train Error: Accuracy: 99.044%, Avg loss: 0.028859
[2022-08-07 21:28:13,553] Test  Error: Accuracy: 98.225%, Avg loss: 0.069239
[2022-08-07 21:28:13,553] Epoch 9---------------
[2022-08-07 21:28:13,554] lr: 5.270402e-04
[2022-08-07 21:28:15,385] loss: 0.002937  [    0/ 4811]
[2022-08-07 21:28:42,810] loss: 0.006517  [  480/ 4811]
[2022-08-07 21:29:10,235] loss: 0.005948  [  960/ 4811]
[2022-08-07 21:29:37,659] loss: 0.006449  [ 1440/ 4811]
[2022-08-07 21:30:05,085] loss: 0.003958  [ 1920/ 4811]
[2022-08-07 21:30:32,511] loss: 0.006599  [ 2400/ 4811]
[2022-08-07 21:30:59,936] loss: 0.003583  [ 2880/ 4811]
[2022-08-07 21:31:27,360] loss: 0.005496  [ 3360/ 4811]
[2022-08-07 21:31:54,784] loss: 0.004248  [ 3840/ 4811]
[2022-08-07 21:32:22,212] loss: 0.002165  [ 4320/ 4811]
[2022-08-07 21:32:48,444] loss: 0.010830  [ 4800/ 4811]
[2022-08-07 21:34:29,379] Train Error: Accuracy: 99.917%, Avg loss: 0.003975
[2022-08-07 21:35:10,752] Test  Error: Accuracy: 99.290%, Avg loss: 0.044111
[2022-08-07 21:35:10,752] Epoch 10---------------
[2022-08-07 21:35:10,753] lr: 5.006882e-04
[2022-08-07 21:35:12,583] loss: 0.001436  [    0/ 4811]
[2022-08-07 21:35:40,007] loss: 0.001133  [  480/ 4811]
[2022-08-07 21:36:07,433] loss: 0.003165  [  960/ 4811]
[2022-08-07 21:36:34,858] loss: 0.002924  [ 1440/ 4811]
[2022-08-07 21:37:02,283] loss: 0.002955  [ 1920/ 4811]
[2022-08-07 21:37:29,709] loss: 0.003727  [ 2400/ 4811]
[2022-08-07 21:37:57,136] loss: 0.025074  [ 2880/ 4811]
[2022-08-07 21:38:24,563] loss: 0.028070  [ 3360/ 4811]
[2022-08-07 21:38:51,988] loss: 0.007917  [ 3840/ 4811]
[2022-08-07 21:39:19,418] loss: 0.002367  [ 4320/ 4811]
[2022-08-07 21:39:45,649] loss: 0.008528  [ 4800/ 4811]
[2022-08-07 21:41:26,584] Train Error: Accuracy: 99.958%, Avg loss: 0.003369
[2022-08-07 21:42:07,952] Test  Error: Accuracy: 99.341%, Avg loss: 0.040948
[2022-08-07 21:42:07,953] Epoch 11---------------
[2022-08-07 21:42:07,954] lr: 4.756538e-04
[2022-08-07 21:42:09,785] loss: 0.001269  [    0/ 4811]
[2022-08-07 21:42:37,210] loss: 0.000558  [  480/ 4811]
[2022-08-07 21:43:04,633] loss: 0.002017  [  960/ 4811]
[2022-08-07 21:43:32,058] loss: 0.000896  [ 1440/ 4811]
[2022-08-07 21:43:59,481] loss: 0.001146  [ 1920/ 4811]
[2022-08-07 21:44:26,908] loss: 0.000894  [ 2400/ 4811]
[2022-08-07 21:44:54,333] loss: 0.000726  [ 2880/ 4811]
[2022-08-07 21:45:21,758] loss: 0.001128  [ 3360/ 4811]
[2022-08-07 21:45:49,183] loss: 0.001794  [ 3840/ 4811]
[2022-08-07 21:46:16,608] loss: 0.000729  [ 4320/ 4811]
[2022-08-07 21:46:42,837] loss: 0.064689  [ 4800/ 4811]
[2022-08-07 21:48:23,770] Train Error: Accuracy: 99.667%, Avg loss: 0.012817
[2022-08-07 21:49:05,141] Test  Error: Accuracy: 98.783%, Avg loss: 0.045283
[2022-08-07 21:49:05,141] Epoch 12---------------
[2022-08-07 21:49:05,144] lr: 4.078137e-04
[2022-08-07 21:49:06,973] loss: 0.114986  [    0/ 4811]
[2022-08-07 21:49:34,395] loss: 0.001302  [  480/ 4811]
[2022-08-07 21:50:01,820] loss: 0.001332  [  960/ 4811]
[2022-08-07 21:50:29,244] loss: 0.002186  [ 1440/ 4811]
[2022-08-07 21:50:56,670] loss: 0.003651  [ 1920/ 4811]
[2022-08-07 21:51:24,092] loss: 0.001542  [ 2400/ 4811]
[2022-08-07 21:51:51,517] loss: 0.004386  [ 2880/ 4811]
[2022-08-07 21:52:18,942] loss: 0.000803  [ 3360/ 4811]
[2022-08-07 21:52:46,368] loss: 0.000895  [ 3840/ 4811]
[2022-08-07 21:53:13,793] loss: 0.000634  [ 4320/ 4811]
[2022-08-07 21:53:40,025] loss: 0.006105  [ 4800/ 4811]
[2022-08-07 21:55:20,959] Train Error: Accuracy: 99.938%, Avg loss: 0.002473
[2022-08-07 21:56:02,332] Test  Error: Accuracy: 99.290%, Avg loss: 0.039859
[2022-08-07 21:56:02,333] Epoch 13---------------
[2022-08-07 21:56:02,334] lr: 3.874230e-04
[2022-08-07 21:56:04,165] loss: 0.000718  [    0/ 4811]
[2022-08-07 21:56:31,591] loss: 0.001103  [  480/ 4811]
[2022-08-07 21:56:59,015] loss: 0.001941  [  960/ 4811]
[2022-08-07 21:57:26,441] loss: 0.003136  [ 1440/ 4811]
[2022-08-07 21:57:53,866] loss: 0.000645  [ 1920/ 4811]
[2022-08-07 21:58:21,290] loss: 0.000625  [ 2400/ 4811]
[2022-08-07 21:58:48,715] loss: 0.002012  [ 2880/ 4811]
[2022-08-07 21:59:16,140] loss: 0.002441  [ 3360/ 4811]
[2022-08-07 21:59:43,564] loss: 0.003655  [ 3840/ 4811]
[2022-08-07 22:00:10,989] loss: 0.000442  [ 4320/ 4811]
[2022-08-07 22:00:37,221] loss: 0.000520  [ 4800/ 4811]
[2022-08-07 22:02:18,149] Train Error: Accuracy: 99.958%, Avg loss: 0.002314
[2022-08-07 22:02:59,519] Test  Error: Accuracy: 99.493%, Avg loss: 0.035041
[2022-08-07 22:02:59,520] Epoch 14---------------
[2022-08-07 22:02:59,521] lr: 3.680518e-04
[2022-08-07 22:03:01,352] loss: 0.000595  [    0/ 4811]
[2022-08-07 22:03:28,780] loss: 0.003034  [  480/ 4811]
[2022-08-07 22:03:56,207] loss: 0.001098  [  960/ 4811]
[2022-08-07 22:04:23,633] loss: 0.004051  [ 1440/ 4811]
[2022-08-07 22:04:51,058] loss: 0.000536  [ 1920/ 4811]
[2022-08-07 22:05:18,484] loss: 0.000313  [ 2400/ 4811]
[2022-08-07 22:05:45,910] loss: 0.001566  [ 2880/ 4811]
[2022-08-07 22:06:13,336] loss: 0.000605  [ 3360/ 4811]
[2022-08-07 22:06:40,763] loss: 0.001046  [ 3840/ 4811]
[2022-08-07 22:07:08,189] loss: 0.000862  [ 4320/ 4811]
[2022-08-07 22:07:34,421] loss: 0.000833  [ 4800/ 4811]
[2022-08-07 22:09:15,355] Train Error: Accuracy: 99.979%, Avg loss: 0.001708
[2022-08-07 22:09:56,730] Test  Error: Accuracy: 99.341%, Avg loss: 0.036773
[2022-08-07 22:09:56,731] Epoch 15---------------
[2022-08-07 22:09:56,732] lr: 3.155584e-04
[2022-08-07 22:09:58,563] loss: 0.004924  [    0/ 4811]
[2022-08-07 22:10:25,989] loss: 0.000360  [  480/ 4811]
[2022-08-07 22:10:53,413] loss: 0.001251  [  960/ 4811]
[2022-08-07 22:11:20,835] loss: 0.000197  [ 1440/ 4811]
[2022-08-07 22:11:48,260] loss: 0.000905  [ 1920/ 4811]
[2022-08-07 22:12:15,684] loss: 0.000383  [ 2400/ 4811]
[2022-08-07 22:12:43,110] loss: 0.000530  [ 2880/ 4811]
[2022-08-07 22:13:10,537] loss: 0.007158  [ 3360/ 4811]
[2022-08-07 22:13:37,960] loss: 0.003243  [ 3840/ 4811]
[2022-08-07 22:14:05,387] loss: 0.001054  [ 4320/ 4811]
[2022-08-07 22:14:31,618] loss: 0.000270  [ 4800/ 4811]
[2022-08-07 22:16:12,549] Train Error: Accuracy: 99.958%, Avg loss: 0.001967
[2022-08-07 22:16:53,919] Test  Error: Accuracy: 99.341%, Avg loss: 0.041579
[2022-08-07 22:16:53,919] Done!
[2022-08-07 22:16:53,925] Number of parameters:3229450
[2022-08-07 22:16:53,925] ## end time: 2022-08-07 22:16:53.919961
[2022-08-07 22:16:53,925] ## used time: 1:44:16.869792
