[2022-08-08 17:13:20,316] ## start time: 2022-08-08 17:13:20.182518
[2022-08-08 17:13:20,316] Using cuda device
[2022-08-08 17:13:20,317] In train:p&d10.npy.
[2022-08-08 17:13:20,318] One Channel
[2022-08-08 17:13:20,319] With Normal data.
[2022-08-08 17:13:20,319] Nunber of classes:10.
[2022-08-08 17:13:20,320] Nunber of ViT channels:1.
[2022-08-08 17:13:20,550] Totol epochs: 15
[2022-08-08 17:13:20,553] Epoch 1---------------
[2022-08-08 17:13:20,554] lr: 2.000000e-03
[2022-08-08 17:13:21,384] loss: 2.605014  [    0/ 4763]
[2022-08-08 17:13:33,812] loss: 1.518494  [  480/ 4763]
[2022-08-08 17:13:46,235] loss: 1.735452  [  960/ 4763]
[2022-08-08 17:13:58,662] loss: 1.297884  [ 1440/ 4763]
[2022-08-08 17:14:11,086] loss: 2.762404  [ 1920/ 4763]
[2022-08-08 17:14:23,511] loss: 1.015501  [ 2400/ 4763]
[2022-08-08 17:14:35,934] loss: 0.899299  [ 2880/ 4763]
[2022-08-08 17:14:48,360] loss: 0.533907  [ 3360/ 4763]
[2022-08-08 17:15:00,784] loss: 0.994478  [ 3840/ 4763]
[2022-08-08 17:15:13,211] loss: 0.689710  [ 4320/ 4763]
[2022-08-08 17:16:09,986] Train Error: Accuracy: 84.023%, Avg loss: 0.485468
[2022-08-08 17:16:29,567] Test  Error: Accuracy: 82.921%, Avg loss: 0.505561
[2022-08-08 17:16:29,567] Epoch 2---------------
[2022-08-08 17:16:29,568] lr: 1.900000e-03
[2022-08-08 17:16:30,399] loss: 0.491795  [    0/ 4763]
[2022-08-08 17:16:42,822] loss: 0.372740  [  480/ 4763]
[2022-08-08 17:16:55,245] loss: 0.292084  [  960/ 4763]
[2022-08-08 17:17:07,669] loss: 0.337314  [ 1440/ 4763]
[2022-08-08 17:17:20,094] loss: 0.482548  [ 1920/ 4763]
[2022-08-08 17:17:32,518] loss: 0.319143  [ 2400/ 4763]
[2022-08-08 17:17:44,943] loss: 0.277616  [ 2880/ 4763]
[2022-08-08 17:17:57,370] loss: 0.200346  [ 3360/ 4763]
[2022-08-08 17:18:09,796] loss: 0.378947  [ 3840/ 4763]
[2022-08-08 17:18:22,222] loss: 0.341929  [ 4320/ 4763]
[2022-08-08 17:19:18,996] Train Error: Accuracy: 90.636%, Avg loss: 0.294098
[2022-08-08 17:19:38,576] Test  Error: Accuracy: 88.911%, Avg loss: 0.321341
[2022-08-08 17:19:38,576] Epoch 3---------------
[2022-08-08 17:19:38,578] lr: 1.805000e-03
[2022-08-08 17:19:39,408] loss: 0.185839  [    0/ 4763]
[2022-08-08 17:19:51,833] loss: 0.211320  [  480/ 4763]
[2022-08-08 17:20:04,259] loss: 0.537513  [  960/ 4763]
[2022-08-08 17:20:16,684] loss: 0.152379  [ 1440/ 4763]
[2022-08-08 17:20:29,108] loss: 0.331373  [ 1920/ 4763]
[2022-08-08 17:20:41,532] loss: 0.230368  [ 2400/ 4763]
[2022-08-08 17:20:53,956] loss: 0.345473  [ 2880/ 4763]
[2022-08-08 17:21:06,381] loss: 0.085421  [ 3360/ 4763]
[2022-08-08 17:21:18,805] loss: 0.063872  [ 3840/ 4763]
[2022-08-08 17:21:31,230] loss: 0.080812  [ 4320/ 4763]
[2022-08-08 17:22:28,013] Train Error: Accuracy: 97.376%, Avg loss: 0.092053
[2022-08-08 17:22:47,591] Test  Error: Accuracy: 96.139%, Avg loss: 0.124978
[2022-08-08 17:22:47,592] Epoch 4---------------
[2022-08-08 17:22:47,593] lr: 1.714750e-03
[2022-08-08 17:22:48,423] loss: 0.092698  [    0/ 4763]
[2022-08-08 17:23:00,847] loss: 0.125987  [  480/ 4763]
[2022-08-08 17:23:13,268] loss: 0.105951  [  960/ 4763]
[2022-08-08 17:23:25,693] loss: 0.035206  [ 1440/ 4763]
[2022-08-08 17:23:38,117] loss: 0.252038  [ 1920/ 4763]
[2022-08-08 17:23:50,541] loss: 0.176556  [ 2400/ 4763]
[2022-08-08 17:24:02,965] loss: 0.096368  [ 2880/ 4763]
[2022-08-08 17:24:15,390] loss: 0.247310  [ 3360/ 4763]
[2022-08-08 17:24:27,814] loss: 0.160077  [ 3840/ 4763]
[2022-08-08 17:24:40,240] loss: 0.069985  [ 4320/ 4763]
[2022-08-08 17:25:37,036] Train Error: Accuracy: 95.192%, Avg loss: 0.140603
[2022-08-08 17:25:56,614] Test  Error: Accuracy: 93.020%, Avg loss: 0.186497
[2022-08-08 17:25:56,614] Epoch 5---------------
[2022-08-08 17:25:56,615] lr: 1.197474e-03
[2022-08-08 17:25:57,445] loss: 0.075554  [    0/ 4763]
[2022-08-08 17:26:09,869] loss: 0.022599  [  480/ 4763]
[2022-08-08 17:26:22,295] loss: 0.060757  [  960/ 4763]
[2022-08-08 17:26:34,719] loss: 0.045711  [ 1440/ 4763]
[2022-08-08 17:26:47,144] loss: 0.063318  [ 1920/ 4763]
[2022-08-08 17:26:59,570] loss: 0.013653  [ 2400/ 4763]
[2022-08-08 17:27:11,995] loss: 0.038986  [ 2880/ 4763]
[2022-08-08 17:27:24,419] loss: 0.011223  [ 3360/ 4763]
[2022-08-08 17:27:36,843] loss: 0.013915  [ 3840/ 4763]
[2022-08-08 17:27:49,268] loss: 0.120131  [ 4320/ 4763]
[2022-08-08 17:28:46,054] Train Error: Accuracy: 98.362%, Avg loss: 0.058773
[2022-08-08 17:29:05,633] Test  Error: Accuracy: 97.079%, Avg loss: 0.100987
[2022-08-08 17:29:05,634] Epoch 6---------------
[2022-08-08 17:29:05,635] lr: 1.137600e-03
[2022-08-08 17:29:06,466] loss: 0.031186  [    0/ 4763]
[2022-08-08 17:29:18,892] loss: 0.064089  [  480/ 4763]
[2022-08-08 17:29:31,316] loss: 0.027998  [  960/ 4763]
[2022-08-08 17:29:43,742] loss: 0.145982  [ 1440/ 4763]
[2022-08-08 17:29:56,167] loss: 0.098306  [ 1920/ 4763]
[2022-08-08 17:30:08,592] loss: 0.015265  [ 2400/ 4763]
[2022-08-08 17:30:21,017] loss: 0.076343  [ 2880/ 4763]
[2022-08-08 17:30:33,441] loss: 0.028573  [ 3360/ 4763]
[2022-08-08 17:30:45,866] loss: 0.039831  [ 3840/ 4763]
[2022-08-08 17:30:58,290] loss: 0.063192  [ 4320/ 4763]
[2022-08-08 17:31:55,059] Train Error: Accuracy: 98.215%, Avg loss: 0.055646
[2022-08-08 17:32:14,639] Test  Error: Accuracy: 97.327%, Avg loss: 0.083605
[2022-08-08 17:32:14,639] Epoch 7---------------
[2022-08-08 17:32:14,640] lr: 1.080720e-03
[2022-08-08 17:32:15,471] loss: 0.117222  [    0/ 4763]
[2022-08-08 17:32:27,896] loss: 0.020545  [  480/ 4763]
[2022-08-08 17:32:40,322] loss: 0.032484  [  960/ 4763]
[2022-08-08 17:32:52,749] loss: 0.020298  [ 1440/ 4763]
[2022-08-08 17:33:05,175] loss: 0.058424  [ 1920/ 4763]
[2022-08-08 17:33:17,601] loss: 0.066932  [ 2400/ 4763]
[2022-08-08 17:33:30,024] loss: 0.007017  [ 2880/ 4763]
[2022-08-08 17:33:42,451] loss: 0.117793  [ 3360/ 4763]
[2022-08-08 17:33:54,874] loss: 0.020320  [ 3840/ 4763]
[2022-08-08 17:34:07,299] loss: 0.047412  [ 4320/ 4763]
[2022-08-08 17:35:04,067] Train Error: Accuracy: 97.460%, Avg loss: 0.078258
[2022-08-08 17:35:23,644] Test  Error: Accuracy: 96.782%, Avg loss: 0.097381
[2022-08-08 17:35:23,645] Epoch 8---------------
[2022-08-08 17:35:23,646] lr: 8.362407e-04
[2022-08-08 17:35:24,476] loss: 0.117143  [    0/ 4763]
[2022-08-08 17:35:36,901] loss: 0.005056  [  480/ 4763]
[2022-08-08 17:35:49,327] loss: 0.031930  [  960/ 4763]
[2022-08-08 17:36:01,752] loss: 0.036868  [ 1440/ 4763]
[2022-08-08 17:36:14,175] loss: 0.104520  [ 1920/ 4763]
[2022-08-08 17:36:26,600] loss: 0.028621  [ 2400/ 4763]
[2022-08-08 17:36:39,023] loss: 0.037349  [ 2880/ 4763]
[2022-08-08 17:36:51,447] loss: 0.019968  [ 3360/ 4763]
[2022-08-08 17:37:03,871] loss: 0.006104  [ 3840/ 4763]
[2022-08-08 17:37:16,297] loss: 0.009205  [ 4320/ 4763]
[2022-08-08 17:38:13,066] Train Error: Accuracy: 99.202%, Avg loss: 0.030800
[2022-08-08 17:38:32,647] Test  Error: Accuracy: 98.416%, Avg loss: 0.059774
[2022-08-08 17:38:32,648] Epoch 9---------------
[2022-08-08 17:38:32,650] lr: 7.944286e-04
[2022-08-08 17:38:33,480] loss: 0.019318  [    0/ 4763]
[2022-08-08 17:38:45,916] loss: 0.010228  [  480/ 4763]
[2022-08-08 17:38:58,350] loss: 0.017061  [  960/ 4763]
[2022-08-08 17:39:10,786] loss: 0.018722  [ 1440/ 4763]
[2022-08-08 17:39:23,226] loss: 0.009087  [ 1920/ 4763]
[2022-08-08 17:39:35,667] loss: 0.018281  [ 2400/ 4763]
[2022-08-08 17:39:48,110] loss: 0.064505  [ 2880/ 4763]
[2022-08-08 17:40:00,553] loss: 0.017815  [ 3360/ 4763]
[2022-08-08 17:40:12,999] loss: 0.044320  [ 3840/ 4763]
[2022-08-08 17:40:25,445] loss: 0.075917  [ 4320/ 4763]
[2022-08-08 17:41:22,347] Train Error: Accuracy: 98.866%, Avg loss: 0.038634
[2022-08-08 17:41:41,978] Test  Error: Accuracy: 97.822%, Avg loss: 0.059895
[2022-08-08 17:41:41,979] Epoch 10---------------
[2022-08-08 17:41:41,980] lr: 6.811233e-04
[2022-08-08 17:41:42,813] loss: 0.019147  [    0/ 4763]
[2022-08-08 17:41:55,259] loss: 0.013784  [  480/ 4763]
[2022-08-08 17:42:07,705] loss: 0.049365  [  960/ 4763]
[2022-08-08 17:42:20,151] loss: 0.048194  [ 1440/ 4763]
[2022-08-08 17:42:32,596] loss: 0.086939  [ 1920/ 4763]
[2022-08-08 17:42:45,038] loss: 0.019907  [ 2400/ 4763]
[2022-08-08 17:42:57,482] loss: 0.020995  [ 2880/ 4763]
[2022-08-08 17:43:09,925] loss: 0.018737  [ 3360/ 4763]
[2022-08-08 17:43:22,368] loss: 0.022769  [ 3840/ 4763]
[2022-08-08 17:43:34,812] loss: 0.022951  [ 4320/ 4763]
[2022-08-08 17:44:31,697] Train Error: Accuracy: 99.664%, Avg loss: 0.014912
[2022-08-08 17:44:51,314] Test  Error: Accuracy: 99.059%, Avg loss: 0.032879
[2022-08-08 17:44:51,314] Epoch 11---------------
[2022-08-08 17:44:51,316] lr: 6.470671e-04
[2022-08-08 17:44:52,147] loss: 0.002820  [    0/ 4763]
[2022-08-08 17:45:04,592] loss: 0.005480  [  480/ 4763]
[2022-08-08 17:45:17,037] loss: 0.001189  [  960/ 4763]
[2022-08-08 17:45:29,483] loss: 0.008375  [ 1440/ 4763]
[2022-08-08 17:45:41,929] loss: 0.008083  [ 1920/ 4763]
[2022-08-08 17:45:54,378] loss: 0.006006  [ 2400/ 4763]
[2022-08-08 17:46:06,826] loss: 0.005618  [ 2880/ 4763]
[2022-08-08 17:46:19,274] loss: 0.007445  [ 3360/ 4763]
[2022-08-08 17:46:31,718] loss: 0.004147  [ 3840/ 4763]
[2022-08-08 17:46:44,166] loss: 0.011396  [ 4320/ 4763]
[2022-08-08 17:47:41,053] Train Error: Accuracy: 99.622%, Avg loss: 0.013866
[2022-08-08 17:48:00,672] Test  Error: Accuracy: 98.812%, Avg loss: 0.040537
[2022-08-08 17:48:00,673] Epoch 12---------------
[2022-08-08 17:48:00,674] lr: 5.006882e-04
[2022-08-08 17:48:01,506] loss: 0.002407  [    0/ 4763]
[2022-08-08 17:48:13,952] loss: 0.046964  [  480/ 4763]
[2022-08-08 17:48:26,398] loss: 0.004918  [  960/ 4763]
[2022-08-08 17:48:38,844] loss: 0.014920  [ 1440/ 4763]
[2022-08-08 17:48:51,287] loss: 0.002517  [ 1920/ 4763]
[2022-08-08 17:49:03,730] loss: 0.008417  [ 2400/ 4763]
[2022-08-08 17:49:16,180] loss: 0.009568  [ 2880/ 4763]
[2022-08-08 17:49:28,629] loss: 0.003189  [ 3360/ 4763]
[2022-08-08 17:49:41,076] loss: 0.000842  [ 3840/ 4763]
[2022-08-08 17:49:53,522] loss: 0.002534  [ 4320/ 4763]
[2022-08-08 17:50:50,402] Train Error: Accuracy: 99.706%, Avg loss: 0.011381
[2022-08-08 17:51:10,022] Test  Error: Accuracy: 98.663%, Avg loss: 0.040860
[2022-08-08 17:51:10,022] Epoch 13---------------
[2022-08-08 17:51:10,024] lr: 4.292775e-04
[2022-08-08 17:51:10,855] loss: 0.005881  [    0/ 4763]
[2022-08-08 17:51:23,300] loss: 0.007092  [  480/ 4763]
[2022-08-08 17:51:35,745] loss: 0.001678  [  960/ 4763]
[2022-08-08 17:51:48,194] loss: 0.007095  [ 1440/ 4763]
[2022-08-08 17:52:00,639] loss: 0.002483  [ 1920/ 4763]
[2022-08-08 17:52:13,083] loss: 0.002148  [ 2400/ 4763]
[2022-08-08 17:52:25,527] loss: 0.003148  [ 2880/ 4763]
[2022-08-08 17:52:37,972] loss: 0.001829  [ 3360/ 4763]
[2022-08-08 17:52:50,418] loss: 0.036650  [ 3840/ 4763]
[2022-08-08 17:53:02,865] loss: 0.003969  [ 4320/ 4763]
[2022-08-08 17:53:59,766] Train Error: Accuracy: 99.706%, Avg loss: 0.011256
[2022-08-08 17:54:19,387] Test  Error: Accuracy: 98.861%, Avg loss: 0.036225
[2022-08-08 17:54:19,387] Epoch 14---------------
[2022-08-08 17:54:19,388] lr: 4.078137e-04
[2022-08-08 17:54:20,220] loss: 0.008387  [    0/ 4763]
[2022-08-08 17:54:32,664] loss: 0.001094  [  480/ 4763]
[2022-08-08 17:54:45,112] loss: 0.001974  [  960/ 4763]
[2022-08-08 17:54:57,558] loss: 0.020199  [ 1440/ 4763]
[2022-08-08 17:55:10,003] loss: 0.001628  [ 1920/ 4763]
[2022-08-08 17:55:22,450] loss: 0.003095  [ 2400/ 4763]
[2022-08-08 17:55:34,893] loss: 0.006209  [ 2880/ 4763]
[2022-08-08 17:55:47,339] loss: 0.000622  [ 3360/ 4763]
[2022-08-08 17:55:59,785] loss: 0.040136  [ 3840/ 4763]
[2022-08-08 17:56:12,233] loss: 0.001631  [ 4320/ 4763]
[2022-08-08 17:57:09,138] Train Error: Accuracy: 99.748%, Avg loss: 0.008102
[2022-08-08 17:57:28,761] Test  Error: Accuracy: 98.861%, Avg loss: 0.036139
[2022-08-08 17:57:28,761] Epoch 15---------------
[2022-08-08 17:57:28,762] lr: 3.874230e-04
[2022-08-08 17:57:29,594] loss: 0.001766  [    0/ 4763]
[2022-08-08 17:57:42,039] loss: 0.001449  [  480/ 4763]
[2022-08-08 17:57:54,484] loss: 0.002413  [  960/ 4763]
[2022-08-08 17:58:06,930] loss: 0.001375  [ 1440/ 4763]
[2022-08-08 17:58:19,375] loss: 0.006170  [ 1920/ 4763]
[2022-08-08 17:58:31,822] loss: 0.000657  [ 2400/ 4763]
[2022-08-08 17:58:44,269] loss: 0.001820  [ 2880/ 4763]
[2022-08-08 17:58:56,714] loss: 0.000642  [ 3360/ 4763]
[2022-08-08 17:59:09,161] loss: 0.024292  [ 3840/ 4763]
[2022-08-08 17:59:21,609] loss: 0.068475  [ 4320/ 4763]
[2022-08-08 18:00:18,499] Train Error: Accuracy: 99.706%, Avg loss: 0.010654
[2022-08-08 18:00:38,122] Test  Error: Accuracy: 98.416%, Avg loss: 0.044468
[2022-08-08 18:00:38,123] Done!
[2022-08-08 18:00:38,128] Number of parameters:1656586
[2022-08-08 18:00:38,128] ## end time: 2022-08-08 18:00:38.123722
[2022-08-08 18:00:38,129] ## used time: 0:47:17.941204
