[2022-08-08 02:40:32,605] ## start time: 2022-08-08 02:40:32.460683
[2022-08-08 02:40:32,605] Using cuda device
[2022-08-08 02:40:32,606] In train:p&d10.npy.
[2022-08-08 02:40:32,607] One Channel
[2022-08-08 02:40:32,608] With Normal data.
[2022-08-08 02:40:32,608] Nunber of classes:10.
[2022-08-08 02:40:32,609] Nunber of ViT channels:1.
[2022-08-08 02:40:32,855] Totol epochs: 15
[2022-08-08 02:40:32,858] Epoch 1---------------
[2022-08-08 02:40:32,858] lr: 2.000000e-03
[2022-08-08 02:40:32,900] loss: 2.377169  [    0/ 4634]
[2022-08-08 02:40:33,434] loss: 1.820637  [  480/ 4634]
[2022-08-08 02:40:33,982] loss: 0.990095  [  960/ 4634]
[2022-08-08 02:40:34,510] loss: 0.781458  [ 1440/ 4634]
[2022-08-08 02:40:35,033] loss: 1.325047  [ 1920/ 4634]
[2022-08-08 02:40:35,554] loss: 0.188809  [ 2400/ 4634]
[2022-08-08 02:40:36,074] loss: 0.122246  [ 2880/ 4634]
[2022-08-08 02:40:36,599] loss: 0.014487  [ 3360/ 4634]
[2022-08-08 02:40:37,120] loss: 0.020878  [ 3840/ 4634]
[2022-08-08 02:40:37,641] loss: 0.009928  [ 4320/ 4634]
[2022-08-08 02:40:39,279] Train Error: Accuracy: 89.642%, Avg loss: 0.309089
[2022-08-08 02:40:39,900] Test  Error: Accuracy: 88.274%, Avg loss: 0.336336
[2022-08-08 02:40:39,901] Epoch 2---------------
[2022-08-08 02:40:39,902] lr: 1.900000e-03
[2022-08-08 02:40:39,939] loss: 0.487148  [    0/ 4634]
[2022-08-08 02:40:40,465] loss: 0.116147  [  480/ 4634]
[2022-08-08 02:40:40,986] loss: 0.015701  [  960/ 4634]
[2022-08-08 02:40:41,506] loss: 0.167160  [ 1440/ 4634]
[2022-08-08 02:40:42,026] loss: 0.006451  [ 1920/ 4634]
[2022-08-08 02:40:42,549] loss: 0.007538  [ 2400/ 4634]
[2022-08-08 02:40:43,077] loss: 0.013413  [ 2880/ 4634]
[2022-08-08 02:40:43,600] loss: 0.004934  [ 3360/ 4634]
[2022-08-08 02:40:44,118] loss: 0.004298  [ 3840/ 4634]
[2022-08-08 02:40:44,638] loss: 0.003675  [ 4320/ 4634]
[2022-08-08 02:40:46,290] Train Error: Accuracy: 100.000%, Avg loss: 0.003454
[2022-08-08 02:40:46,921] Test  Error: Accuracy: 100.000%, Avg loss: 0.003911
[2022-08-08 02:40:46,921] Epoch 3---------------
[2022-08-08 02:40:46,923] lr: 1.805000e-03
[2022-08-08 02:40:46,959] loss: 0.004376  [    0/ 4634]
[2022-08-08 02:40:47,484] loss: 0.002450  [  480/ 4634]
[2022-08-08 02:40:48,008] loss: 0.002588  [  960/ 4634]
[2022-08-08 02:40:48,533] loss: 0.002052  [ 1440/ 4634]
[2022-08-08 02:40:49,059] loss: 0.002041  [ 1920/ 4634]
[2022-08-08 02:40:49,585] loss: 0.001805  [ 2400/ 4634]
[2022-08-08 02:40:50,115] loss: 0.001900  [ 2880/ 4634]
[2022-08-08 02:40:50,636] loss: 0.001610  [ 3360/ 4634]
[2022-08-08 02:40:51,164] loss: 0.001651  [ 3840/ 4634]
[2022-08-08 02:40:51,685] loss: 0.002535  [ 4320/ 4634]
[2022-08-08 02:40:53,322] Train Error: Accuracy: 100.000%, Avg loss: 0.002448
[2022-08-08 02:40:53,940] Test  Error: Accuracy: 100.000%, Avg loss: 0.003438
[2022-08-08 02:40:53,941] Epoch 4---------------
[2022-08-08 02:40:53,942] lr: 1.714750e-03
[2022-08-08 02:40:53,982] loss: 0.001252  [    0/ 4634]
[2022-08-08 02:40:54,508] loss: 0.001937  [  480/ 4634]
[2022-08-08 02:40:55,036] loss: 0.001896  [  960/ 4634]
[2022-08-08 02:40:55,563] loss: 0.001518  [ 1440/ 4634]
[2022-08-08 02:40:56,086] loss: 0.001659  [ 1920/ 4634]
[2022-08-08 02:40:56,611] loss: 0.000964  [ 2400/ 4634]
[2022-08-08 02:40:57,133] loss: 0.000838  [ 2880/ 4634]
[2022-08-08 02:40:57,654] loss: 0.001168  [ 3360/ 4634]
[2022-08-08 02:40:58,172] loss: 0.000969  [ 3840/ 4634]
[2022-08-08 02:40:58,694] loss: 0.003480  [ 4320/ 4634]
[2022-08-08 02:41:00,326] Train Error: Accuracy: 100.000%, Avg loss: 0.001006
[2022-08-08 02:41:00,951] Test  Error: Accuracy: 100.000%, Avg loss: 0.001065
[2022-08-08 02:41:00,951] Epoch 5---------------
[2022-08-08 02:41:00,953] lr: 1.629012e-03
[2022-08-08 02:41:00,991] loss: 0.000764  [    0/ 4634]
[2022-08-08 02:41:01,514] loss: 0.001737  [  480/ 4634]
[2022-08-08 02:41:02,038] loss: 0.000768  [  960/ 4634]
[2022-08-08 02:41:02,560] loss: 0.000649  [ 1440/ 4634]
[2022-08-08 02:41:03,086] loss: 0.000825  [ 1920/ 4634]
[2022-08-08 02:41:03,612] loss: 0.000792  [ 2400/ 4634]
[2022-08-08 02:41:04,137] loss: 0.000655  [ 2880/ 4634]
[2022-08-08 02:41:04,661] loss: 0.000786  [ 3360/ 4634]
[2022-08-08 02:41:05,188] loss: 0.001166  [ 3840/ 4634]
[2022-08-08 02:41:05,710] loss: 0.000819  [ 4320/ 4634]
[2022-08-08 02:41:07,391] Train Error: Accuracy: 100.000%, Avg loss: 0.000742
[2022-08-08 02:41:08,013] Test  Error: Accuracy: 100.000%, Avg loss: 0.000844
[2022-08-08 02:41:08,013] Epoch 6---------------
[2022-08-08 02:41:08,014] lr: 1.547562e-03
[2022-08-08 02:41:08,053] loss: 0.000729  [    0/ 4634]
[2022-08-08 02:41:08,577] loss: 0.000534  [  480/ 4634]
[2022-08-08 02:41:09,098] loss: 0.000630  [  960/ 4634]
[2022-08-08 02:41:09,624] loss: 0.000962  [ 1440/ 4634]
[2022-08-08 02:41:10,145] loss: 0.000609  [ 1920/ 4634]
[2022-08-08 02:41:10,670] loss: 0.000670  [ 2400/ 4634]
[2022-08-08 02:41:11,196] loss: 0.000826  [ 2880/ 4634]
[2022-08-08 02:41:11,720] loss: 0.000581  [ 3360/ 4634]
[2022-08-08 02:41:12,242] loss: 0.000600  [ 3840/ 4634]
[2022-08-08 02:41:12,764] loss: 0.000588  [ 4320/ 4634]
[2022-08-08 02:41:14,405] Train Error: Accuracy: 100.000%, Avg loss: 0.000626
[2022-08-08 02:41:15,023] Test  Error: Accuracy: 100.000%, Avg loss: 0.000739
[2022-08-08 02:41:15,024] Epoch 7---------------
[2022-08-08 02:41:15,025] lr: 1.470184e-03
[2022-08-08 02:41:15,062] loss: 0.000676  [    0/ 4634]
[2022-08-08 02:41:15,589] loss: 0.000534  [  480/ 4634]
[2022-08-08 02:41:16,122] loss: 0.000502  [  960/ 4634]
[2022-08-08 02:41:16,663] loss: 0.000713  [ 1440/ 4634]
[2022-08-08 02:41:17,195] loss: 0.000819  [ 1920/ 4634]
[2022-08-08 02:41:17,725] loss: 0.000563  [ 2400/ 4634]
[2022-08-08 02:41:18,251] loss: 0.000627  [ 2880/ 4634]
[2022-08-08 02:41:18,781] loss: 0.000555  [ 3360/ 4634]
[2022-08-08 02:41:19,323] loss: 0.000570  [ 3840/ 4634]
[2022-08-08 02:41:19,850] loss: 0.000932  [ 4320/ 4634]
[2022-08-08 02:41:21,488] Train Error: Accuracy: 100.000%, Avg loss: 0.000560
[2022-08-08 02:41:22,115] Test  Error: Accuracy: 100.000%, Avg loss: 0.000627
[2022-08-08 02:41:22,115] Epoch 8---------------
[2022-08-08 02:41:22,116] lr: 1.396675e-03
[2022-08-08 02:41:22,154] loss: 0.000606  [    0/ 4634]
[2022-08-08 02:41:22,678] loss: 0.000476  [  480/ 4634]
[2022-08-08 02:41:23,203] loss: 0.000426  [  960/ 4634]
[2022-08-08 02:41:23,727] loss: 0.000532  [ 1440/ 4634]
[2022-08-08 02:41:24,253] loss: 0.000448  [ 1920/ 4634]
[2022-08-08 02:41:24,777] loss: 0.000848  [ 2400/ 4634]
[2022-08-08 02:41:25,304] loss: 0.000585  [ 2880/ 4634]
[2022-08-08 02:41:25,829] loss: 5.164425  [ 3360/ 4634]
[2022-08-08 02:41:26,353] loss: 1.322956  [ 3840/ 4634]
[2022-08-08 02:41:26,876] loss: 0.089398  [ 4320/ 4634]
[2022-08-08 02:41:28,519] Train Error: Accuracy: 83.276%, Avg loss: 0.655111
[2022-08-08 02:41:29,142] Test  Error: Accuracy: 82.922%, Avg loss: 0.679133
[2022-08-08 02:41:29,143] Epoch 9---------------
[2022-08-08 02:41:29,144] lr: 9.753500e-04
[2022-08-08 02:41:29,181] loss: 0.870683  [    0/ 4634]
[2022-08-08 02:41:29,710] loss: 0.028656  [  480/ 4634]
[2022-08-08 02:41:30,232] loss: 0.008268  [  960/ 4634]
[2022-08-08 02:41:30,759] loss: 0.008175  [ 1440/ 4634]
[2022-08-08 02:41:31,283] loss: 0.013098  [ 1920/ 4634]
[2022-08-08 02:41:31,806] loss: 0.004814  [ 2400/ 4634]
[2022-08-08 02:41:32,327] loss: 0.006885  [ 2880/ 4634]
[2022-08-08 02:41:32,853] loss: 0.008072  [ 3360/ 4634]
[2022-08-08 02:41:33,376] loss: 0.024109  [ 3840/ 4634]
[2022-08-08 02:41:33,900] loss: 0.006526  [ 4320/ 4634]
[2022-08-08 02:41:35,537] Train Error: Accuracy: 99.957%, Avg loss: 0.004524
[2022-08-08 02:41:36,160] Test  Error: Accuracy: 100.000%, Avg loss: 0.005985
[2022-08-08 02:41:36,160] Epoch 10---------------
[2022-08-08 02:41:36,161] lr: 9.265825e-04
[2022-08-08 02:41:36,198] loss: 0.004999  [    0/ 4634]
[2022-08-08 02:41:36,724] loss: 0.002287  [  480/ 4634]
[2022-08-08 02:41:37,257] loss: 0.002570  [  960/ 4634]
[2022-08-08 02:41:37,780] loss: 0.002105  [ 1440/ 4634]
[2022-08-08 02:41:38,304] loss: 0.001952  [ 1920/ 4634]
[2022-08-08 02:41:38,832] loss: 0.002918  [ 2400/ 4634]
[2022-08-08 02:41:39,361] loss: 0.002207  [ 2880/ 4634]
[2022-08-08 02:41:39,881] loss: 0.002981  [ 3360/ 4634]
[2022-08-08 02:41:40,400] loss: 0.001546  [ 3840/ 4634]
[2022-08-08 02:41:40,922] loss: 0.002532  [ 4320/ 4634]
[2022-08-08 02:41:42,559] Train Error: Accuracy: 100.000%, Avg loss: 0.002020
[2022-08-08 02:41:43,182] Test  Error: Accuracy: 100.000%, Avg loss: 0.003080
[2022-08-08 02:41:43,183] Epoch 11---------------
[2022-08-08 02:41:43,184] lr: 8.802533e-04
[2022-08-08 02:41:43,221] loss: 0.001528  [    0/ 4634]
[2022-08-08 02:41:43,744] loss: 0.002008  [  480/ 4634]
[2022-08-08 02:41:44,269] loss: 0.001369  [  960/ 4634]
[2022-08-08 02:41:44,793] loss: 0.001482  [ 1440/ 4634]
[2022-08-08 02:41:45,324] loss: 0.001891  [ 1920/ 4634]
[2022-08-08 02:41:45,846] loss: 0.001193  [ 2400/ 4634]
[2022-08-08 02:41:46,370] loss: 0.001784  [ 2880/ 4634]
[2022-08-08 02:41:46,892] loss: 0.001198  [ 3360/ 4634]
[2022-08-08 02:41:47,421] loss: 0.001275  [ 3840/ 4634]
[2022-08-08 02:41:47,941] loss: 0.002104  [ 4320/ 4634]
[2022-08-08 02:41:49,581] Train Error: Accuracy: 100.000%, Avg loss: 0.001514
[2022-08-08 02:41:50,200] Test  Error: Accuracy: 100.000%, Avg loss: 0.001818
[2022-08-08 02:41:50,200] Epoch 12---------------
[2022-08-08 02:41:50,201] lr: 8.362407e-04
[2022-08-08 02:41:50,239] loss: 0.001374  [    0/ 4634]
[2022-08-08 02:41:50,766] loss: 0.001914  [  480/ 4634]
[2022-08-08 02:41:51,288] loss: 0.003169  [  960/ 4634]
[2022-08-08 02:41:51,813] loss: 0.001538  [ 1440/ 4634]
[2022-08-08 02:41:52,336] loss: 0.001361  [ 1920/ 4634]
[2022-08-08 02:41:52,860] loss: 0.003468  [ 2400/ 4634]
[2022-08-08 02:41:53,385] loss: 0.000872  [ 2880/ 4634]
[2022-08-08 02:41:53,910] loss: 0.000929  [ 3360/ 4634]
[2022-08-08 02:41:54,434] loss: 0.001200  [ 3840/ 4634]
[2022-08-08 02:41:54,962] loss: 0.001276  [ 4320/ 4634]
[2022-08-08 02:41:56,598] Train Error: Accuracy: 100.000%, Avg loss: 0.001105
[2022-08-08 02:41:57,223] Test  Error: Accuracy: 99.953%, Avg loss: 0.002652
[2022-08-08 02:41:57,224] Epoch 13---------------
[2022-08-08 02:41:57,225] lr: 5.839780e-04
[2022-08-08 02:41:57,263] loss: 0.002156  [    0/ 4634]
[2022-08-08 02:41:57,784] loss: 0.000995  [  480/ 4634]
[2022-08-08 02:41:58,309] loss: 0.001322  [  960/ 4634]
[2022-08-08 02:41:58,830] loss: 0.001191  [ 1440/ 4634]
[2022-08-08 02:41:59,354] loss: 0.000780  [ 1920/ 4634]
[2022-08-08 02:41:59,878] loss: 0.001022  [ 2400/ 4634]
[2022-08-08 02:42:00,403] loss: 0.000936  [ 2880/ 4634]
[2022-08-08 02:42:00,926] loss: 0.001012  [ 3360/ 4634]
[2022-08-08 02:42:01,453] loss: 0.000674  [ 3840/ 4634]
[2022-08-08 02:42:01,978] loss: 0.000743  [ 4320/ 4634]
[2022-08-08 02:42:03,615] Train Error: Accuracy: 100.000%, Avg loss: 0.000951
[2022-08-08 02:42:04,234] Test  Error: Accuracy: 100.000%, Avg loss: 0.001193
[2022-08-08 02:42:04,235] Epoch 14---------------
[2022-08-08 02:42:04,237] lr: 5.547791e-04
[2022-08-08 02:42:04,274] loss: 0.001207  [    0/ 4634]
[2022-08-08 02:42:04,801] loss: 0.000677  [  480/ 4634]
[2022-08-08 02:42:05,323] loss: 0.000676  [  960/ 4634]
[2022-08-08 02:42:05,857] loss: 0.000651  [ 1440/ 4634]
[2022-08-08 02:42:06,377] loss: 0.001221  [ 1920/ 4634]
[2022-08-08 02:42:06,902] loss: 0.000734  [ 2400/ 4634]
[2022-08-08 02:42:07,424] loss: 0.000595  [ 2880/ 4634]
[2022-08-08 02:42:07,948] loss: 0.001113  [ 3360/ 4634]
[2022-08-08 02:42:08,468] loss: 0.000686  [ 3840/ 4634]
[2022-08-08 02:42:08,991] loss: 0.000561  [ 4320/ 4634]
[2022-08-08 02:42:10,621] Train Error: Accuracy: 100.000%, Avg loss: 0.000899
[2022-08-08 02:42:11,241] Test  Error: Accuracy: 100.000%, Avg loss: 0.001082
[2022-08-08 02:42:11,241] Epoch 15---------------
[2022-08-08 02:42:11,243] lr: 5.270402e-04
[2022-08-08 02:42:11,279] loss: 0.000747  [    0/ 4634]
[2022-08-08 02:42:11,801] loss: 0.000819  [  480/ 4634]
[2022-08-08 02:42:12,327] loss: 0.000698  [  960/ 4634]
[2022-08-08 02:42:12,850] loss: 0.000674  [ 1440/ 4634]
[2022-08-08 02:42:13,375] loss: 0.000626  [ 1920/ 4634]
[2022-08-08 02:42:13,898] loss: 0.000462  [ 2400/ 4634]
[2022-08-08 02:42:14,425] loss: 0.000622  [ 2880/ 4634]
[2022-08-08 02:42:14,962] loss: 0.000721  [ 3360/ 4634]
[2022-08-08 02:42:15,487] loss: 0.000787  [ 3840/ 4634]
[2022-08-08 02:42:16,013] loss: 0.000493  [ 4320/ 4634]
[2022-08-08 02:42:17,666] Train Error: Accuracy: 100.000%, Avg loss: 0.000739
[2022-08-08 02:42:18,286] Test  Error: Accuracy: 100.000%, Avg loss: 0.001028
[2022-08-08 02:42:18,286] Done!
[2022-08-08 02:42:18,290] Number of parameters:3686410
[2022-08-08 02:42:18,290] ## end time: 2022-08-08 02:42:18.286738
[2022-08-08 02:42:18,291] ## used time: 0:01:45.826055
