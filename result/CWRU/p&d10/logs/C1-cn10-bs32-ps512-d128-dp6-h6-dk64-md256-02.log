[2022-08-08 02:27:19,690] ## start time: 2022-08-08 02:27:19.544717
[2022-08-08 02:27:19,690] Using cuda device
[2022-08-08 02:27:19,692] In train:p&d10.npy.
[2022-08-08 02:27:19,693] One Channel
[2022-08-08 02:27:19,693] With Normal data.
[2022-08-08 02:27:19,694] Nunber of classes:10.
[2022-08-08 02:27:19,694] Nunber of ViT channels:1.
[2022-08-08 02:27:19,937] Totol epochs: 15
[2022-08-08 02:27:19,940] Epoch 1---------------
[2022-08-08 02:27:19,941] lr: 2.000000e-03
[2022-08-08 02:27:19,990] loss: 2.317728  [    0/ 4758]
[2022-08-08 02:27:20,600] loss: 1.666763  [  480/ 4758]
[2022-08-08 02:27:21,216] loss: 1.354630  [  960/ 4758]
[2022-08-08 02:27:21,819] loss: 0.997384  [ 1440/ 4758]
[2022-08-08 02:27:22,423] loss: 0.233488  [ 1920/ 4758]
[2022-08-08 02:27:23,026] loss: 0.093655  [ 2400/ 4758]
[2022-08-08 02:27:23,632] loss: 0.070040  [ 2880/ 4758]
[2022-08-08 02:27:24,235] loss: 0.008944  [ 3360/ 4758]
[2022-08-08 02:27:24,841] loss: 0.380508  [ 3840/ 4758]
[2022-08-08 02:27:25,446] loss: 0.045792  [ 4320/ 4758]
[2022-08-08 02:27:27,566] Train Error: Accuracy: 99.832%, Avg loss: 0.016056
[2022-08-08 02:27:28,254] Test  Error: Accuracy: 99.753%, Avg loss: 0.017209
[2022-08-08 02:27:28,254] Epoch 2---------------
[2022-08-08 02:27:28,256] lr: 1.900000e-03
[2022-08-08 02:27:28,297] loss: 0.005787  [    0/ 4758]
[2022-08-08 02:27:28,902] loss: 0.007416  [  480/ 4758]
[2022-08-08 02:27:29,505] loss: 0.008489  [  960/ 4758]
[2022-08-08 02:27:30,112] loss: 0.097083  [ 1440/ 4758]
[2022-08-08 02:27:30,716] loss: 0.006093  [ 1920/ 4758]
[2022-08-08 02:27:31,319] loss: 0.103992  [ 2400/ 4758]
[2022-08-08 02:27:31,923] loss: 0.011611  [ 2880/ 4758]
[2022-08-08 02:27:32,527] loss: 0.007800  [ 3360/ 4758]
[2022-08-08 02:27:33,131] loss: 0.005399  [ 3840/ 4758]
[2022-08-08 02:27:33,734] loss: 0.003769  [ 4320/ 4758]
[2022-08-08 02:27:35,857] Train Error: Accuracy: 99.958%, Avg loss: 0.005404
[2022-08-08 02:27:36,546] Test  Error: Accuracy: 99.951%, Avg loss: 0.005942
[2022-08-08 02:27:36,547] Epoch 3---------------
[2022-08-08 02:27:36,548] lr: 1.805000e-03
[2022-08-08 02:27:36,592] loss: 0.002676  [    0/ 4758]
[2022-08-08 02:27:37,196] loss: 0.002734  [  480/ 4758]
[2022-08-08 02:27:37,798] loss: 0.004054  [  960/ 4758]
[2022-08-08 02:27:38,402] loss: 0.001832  [ 1440/ 4758]
[2022-08-08 02:27:39,004] loss: 0.004381  [ 1920/ 4758]
[2022-08-08 02:27:39,608] loss: 0.001057  [ 2400/ 4758]
[2022-08-08 02:27:40,212] loss: 0.001289  [ 2880/ 4758]
[2022-08-08 02:27:40,817] loss: 1.819444  [ 3360/ 4758]
[2022-08-08 02:27:41,419] loss: 0.409000  [ 3840/ 4758]
[2022-08-08 02:27:42,025] loss: 0.022966  [ 4320/ 4758]
[2022-08-08 02:27:44,147] Train Error: Accuracy: 99.517%, Avg loss: 0.029329
[2022-08-08 02:27:44,838] Test  Error: Accuracy: 99.506%, Avg loss: 0.032023
[2022-08-08 02:27:44,839] Epoch 4---------------
[2022-08-08 02:27:44,840] lr: 1.260499e-03
[2022-08-08 02:27:44,882] loss: 0.031278  [    0/ 4758]
[2022-08-08 02:27:45,484] loss: 0.009215  [  480/ 4758]
[2022-08-08 02:27:46,086] loss: 0.006519  [  960/ 4758]
[2022-08-08 02:27:46,688] loss: 0.017472  [ 1440/ 4758]
[2022-08-08 02:27:47,288] loss: 0.003764  [ 1920/ 4758]
[2022-08-08 02:27:47,890] loss: 0.001816  [ 2400/ 4758]
[2022-08-08 02:27:48,493] loss: 0.054969  [ 2880/ 4758]
[2022-08-08 02:27:49,097] loss: 0.005277  [ 3360/ 4758]
[2022-08-08 02:27:49,700] loss: 0.004566  [ 3840/ 4758]
[2022-08-08 02:27:50,306] loss: 0.004459  [ 4320/ 4758]
[2022-08-08 02:27:52,425] Train Error: Accuracy: 100.000%, Avg loss: 0.003435
[2022-08-08 02:27:53,112] Test  Error: Accuracy: 100.000%, Avg loss: 0.004120
[2022-08-08 02:27:53,112] Epoch 5---------------
[2022-08-08 02:27:53,113] lr: 1.197474e-03
[2022-08-08 02:27:53,157] loss: 0.001930  [    0/ 4758]
[2022-08-08 02:27:53,763] loss: 0.001872  [  480/ 4758]
[2022-08-08 02:27:54,371] loss: 0.002522  [  960/ 4758]
[2022-08-08 02:27:54,977] loss: 0.001494  [ 1440/ 4758]
[2022-08-08 02:27:55,581] loss: 0.002446  [ 1920/ 4758]
[2022-08-08 02:27:56,186] loss: 0.004494  [ 2400/ 4758]
[2022-08-08 02:27:56,788] loss: 0.001300  [ 2880/ 4758]
[2022-08-08 02:27:57,390] loss: 0.001646  [ 3360/ 4758]
[2022-08-08 02:27:57,992] loss: 0.001750  [ 3840/ 4758]
[2022-08-08 02:27:58,597] loss: 0.001265  [ 4320/ 4758]
[2022-08-08 02:28:00,720] Train Error: Accuracy: 100.000%, Avg loss: 0.001163
[2022-08-08 02:28:01,409] Test  Error: Accuracy: 100.000%, Avg loss: 0.001370
[2022-08-08 02:28:01,410] Epoch 6---------------
[2022-08-08 02:28:01,411] lr: 1.137600e-03
[2022-08-08 02:28:01,453] loss: 0.001022  [    0/ 4758]
[2022-08-08 02:28:02,057] loss: 0.000891  [  480/ 4758]
[2022-08-08 02:28:02,660] loss: 0.000978  [  960/ 4758]
[2022-08-08 02:28:03,265] loss: 0.000734  [ 1440/ 4758]
[2022-08-08 02:28:03,870] loss: 0.001025  [ 1920/ 4758]
[2022-08-08 02:28:04,473] loss: 0.000912  [ 2400/ 4758]
[2022-08-08 02:28:05,079] loss: 0.000890  [ 2880/ 4758]
[2022-08-08 02:28:05,683] loss: 0.002684  [ 3360/ 4758]
[2022-08-08 02:28:06,289] loss: 0.001608  [ 3840/ 4758]
[2022-08-08 02:28:06,894] loss: 0.001337  [ 4320/ 4758]
[2022-08-08 02:28:09,009] Train Error: Accuracy: 100.000%, Avg loss: 0.001666
[2022-08-08 02:28:09,696] Test  Error: Accuracy: 99.951%, Avg loss: 0.002615
[2022-08-08 02:28:09,696] Epoch 7---------------
[2022-08-08 02:28:09,697] lr: 7.944286e-04
[2022-08-08 02:28:09,740] loss: 0.000787  [    0/ 4758]
[2022-08-08 02:28:10,343] loss: 0.000909  [  480/ 4758]
[2022-08-08 02:28:10,945] loss: 0.001209  [  960/ 4758]
[2022-08-08 02:28:11,547] loss: 0.000696  [ 1440/ 4758]
[2022-08-08 02:28:12,149] loss: 0.000667  [ 1920/ 4758]
[2022-08-08 02:28:12,753] loss: 0.000767  [ 2400/ 4758]
[2022-08-08 02:28:13,360] loss: 0.001036  [ 2880/ 4758]
[2022-08-08 02:28:13,963] loss: 0.002121  [ 3360/ 4758]
[2022-08-08 02:28:14,570] loss: 0.000824  [ 3840/ 4758]
[2022-08-08 02:28:15,180] loss: 0.000693  [ 4320/ 4758]
[2022-08-08 02:28:17,301] Train Error: Accuracy: 100.000%, Avg loss: 0.001130
[2022-08-08 02:28:17,993] Test  Error: Accuracy: 100.000%, Avg loss: 0.001695
[2022-08-08 02:28:17,994] Epoch 8---------------
[2022-08-08 02:28:17,995] lr: 7.547072e-04
[2022-08-08 02:28:18,038] loss: 0.000985  [    0/ 4758]
[2022-08-08 02:28:18,647] loss: 0.000641  [  480/ 4758]
[2022-08-08 02:28:19,255] loss: 0.000861  [  960/ 4758]
[2022-08-08 02:28:19,862] loss: 0.001143  [ 1440/ 4758]
[2022-08-08 02:28:20,473] loss: 0.000720  [ 1920/ 4758]
[2022-08-08 02:28:21,079] loss: 0.000653  [ 2400/ 4758]
[2022-08-08 02:28:21,685] loss: 0.001245  [ 2880/ 4758]
[2022-08-08 02:28:22,292] loss: 0.000553  [ 3360/ 4758]
[2022-08-08 02:28:22,895] loss: 0.000525  [ 3840/ 4758]
[2022-08-08 02:28:23,499] loss: 0.000539  [ 4320/ 4758]
[2022-08-08 02:28:25,621] Train Error: Accuracy: 100.000%, Avg loss: 0.000714
[2022-08-08 02:28:26,327] Test  Error: Accuracy: 99.951%, Avg loss: 0.001310
[2022-08-08 02:28:26,328] Epoch 9---------------
[2022-08-08 02:28:26,329] lr: 7.169718e-04
[2022-08-08 02:28:26,372] loss: 0.000436  [    0/ 4758]
[2022-08-08 02:28:26,975] loss: 0.000713  [  480/ 4758]
[2022-08-08 02:28:27,582] loss: 0.000538  [  960/ 4758]
[2022-08-08 02:28:28,186] loss: 0.000577  [ 1440/ 4758]
[2022-08-08 02:28:28,789] loss: 0.000643  [ 1920/ 4758]
[2022-08-08 02:28:29,391] loss: 0.000503  [ 2400/ 4758]
[2022-08-08 02:28:29,995] loss: 0.000617  [ 2880/ 4758]
[2022-08-08 02:28:30,599] loss: 0.000474  [ 3360/ 4758]
[2022-08-08 02:28:31,201] loss: 0.000746  [ 3840/ 4758]
[2022-08-08 02:28:31,803] loss: 0.000502  [ 4320/ 4758]
[2022-08-08 02:28:33,921] Train Error: Accuracy: 100.000%, Avg loss: 0.000591
[2022-08-08 02:28:34,610] Test  Error: Accuracy: 100.000%, Avg loss: 0.000627
[2022-08-08 02:28:34,610] Epoch 10---------------
[2022-08-08 02:28:34,612] lr: 6.811233e-04
[2022-08-08 02:28:34,653] loss: 0.000578  [    0/ 4758]
[2022-08-08 02:28:35,259] loss: 0.000357  [  480/ 4758]
[2022-08-08 02:28:35,864] loss: 0.000603  [  960/ 4758]
[2022-08-08 02:28:36,468] loss: 0.000434  [ 1440/ 4758]
[2022-08-08 02:28:37,071] loss: 0.000424  [ 1920/ 4758]
[2022-08-08 02:28:37,674] loss: 0.000368  [ 2400/ 4758]
[2022-08-08 02:28:38,279] loss: 0.000541  [ 2880/ 4758]
[2022-08-08 02:28:38,883] loss: 0.000608  [ 3360/ 4758]
[2022-08-08 02:28:39,488] loss: 0.000799  [ 3840/ 4758]
[2022-08-08 02:28:40,092] loss: 0.000361  [ 4320/ 4758]
[2022-08-08 02:28:42,219] Train Error: Accuracy: 100.000%, Avg loss: 0.000476
[2022-08-08 02:28:42,907] Test  Error: Accuracy: 100.000%, Avg loss: 0.000804
[2022-08-08 02:28:42,907] Epoch 11---------------
[2022-08-08 02:28:42,909] lr: 4.756538e-04
[2022-08-08 02:28:42,951] loss: 0.000354  [    0/ 4758]
[2022-08-08 02:28:43,555] loss: 0.000393  [  480/ 4758]
[2022-08-08 02:28:44,159] loss: 0.000485  [  960/ 4758]
[2022-08-08 02:28:44,764] loss: 0.000376  [ 1440/ 4758]
[2022-08-08 02:28:45,368] loss: 0.000403  [ 1920/ 4758]
[2022-08-08 02:28:45,972] loss: 0.000786  [ 2400/ 4758]
[2022-08-08 02:28:46,578] loss: 0.000468  [ 2880/ 4758]
[2022-08-08 02:28:47,180] loss: 0.000504  [ 3360/ 4758]
[2022-08-08 02:28:47,784] loss: 0.000368  [ 3840/ 4758]
[2022-08-08 02:28:48,388] loss: 0.000297  [ 4320/ 4758]
[2022-08-08 02:28:50,509] Train Error: Accuracy: 100.000%, Avg loss: 0.000457
[2022-08-08 02:28:51,195] Test  Error: Accuracy: 100.000%, Avg loss: 0.000590
[2022-08-08 02:28:51,196] Epoch 12---------------
[2022-08-08 02:28:51,197] lr: 4.518711e-04
[2022-08-08 02:28:51,240] loss: 0.000386  [    0/ 4758]
[2022-08-08 02:28:51,847] loss: 0.000373  [  480/ 4758]
[2022-08-08 02:28:52,451] loss: 0.000413  [  960/ 4758]
[2022-08-08 02:28:53,056] loss: 0.000325  [ 1440/ 4758]
[2022-08-08 02:28:53,661] loss: 0.000341  [ 1920/ 4758]
[2022-08-08 02:28:54,264] loss: 0.000304  [ 2400/ 4758]
[2022-08-08 02:28:54,869] loss: 0.000561  [ 2880/ 4758]
[2022-08-08 02:28:55,472] loss: 0.000307  [ 3360/ 4758]
[2022-08-08 02:28:56,076] loss: 0.000308  [ 3840/ 4758]
[2022-08-08 02:28:56,682] loss: 0.000349  [ 4320/ 4758]
[2022-08-08 02:28:58,805] Train Error: Accuracy: 100.000%, Avg loss: 0.000431
[2022-08-08 02:28:59,495] Test  Error: Accuracy: 100.000%, Avg loss: 0.000509
[2022-08-08 02:28:59,495] Epoch 13---------------
[2022-08-08 02:28:59,496] lr: 4.292775e-04
[2022-08-08 02:28:59,539] loss: 0.000479  [    0/ 4758]
[2022-08-08 02:29:00,144] loss: 0.000477  [  480/ 4758]
[2022-08-08 02:29:00,750] loss: 0.000389  [  960/ 4758]
[2022-08-08 02:29:01,355] loss: 0.000404  [ 1440/ 4758]
[2022-08-08 02:29:01,961] loss: 0.000479  [ 1920/ 4758]
[2022-08-08 02:29:02,566] loss: 0.000289  [ 2400/ 4758]
[2022-08-08 02:29:03,168] loss: 0.000316  [ 2880/ 4758]
[2022-08-08 02:29:03,774] loss: 0.000406  [ 3360/ 4758]
[2022-08-08 02:29:04,377] loss: 0.000348  [ 3840/ 4758]
[2022-08-08 02:29:04,982] loss: 0.000426  [ 4320/ 4758]
[2022-08-08 02:29:07,106] Train Error: Accuracy: 100.000%, Avg loss: 0.000400
[2022-08-08 02:29:07,795] Test  Error: Accuracy: 100.000%, Avg loss: 0.000473
[2022-08-08 02:29:07,795] Epoch 14---------------
[2022-08-08 02:29:07,796] lr: 4.078137e-04
[2022-08-08 02:29:07,839] loss: 0.000258  [    0/ 4758]
[2022-08-08 02:29:08,446] loss: 0.000306  [  480/ 4758]
[2022-08-08 02:29:09,051] loss: 0.000352  [  960/ 4758]
[2022-08-08 02:29:09,653] loss: 0.000378  [ 1440/ 4758]
[2022-08-08 02:29:10,254] loss: 0.000429  [ 1920/ 4758]
[2022-08-08 02:29:10,860] loss: 0.000334  [ 2400/ 4758]
[2022-08-08 02:29:11,466] loss: 0.000333  [ 2880/ 4758]
[2022-08-08 02:29:12,073] loss: 0.000357  [ 3360/ 4758]
[2022-08-08 02:29:12,679] loss: 0.000323  [ 3840/ 4758]
[2022-08-08 02:29:13,288] loss: 0.000415  [ 4320/ 4758]
[2022-08-08 02:29:15,411] Train Error: Accuracy: 100.000%, Avg loss: 0.000367
[2022-08-08 02:29:16,098] Test  Error: Accuracy: 100.000%, Avg loss: 0.000403
[2022-08-08 02:29:16,098] Epoch 15---------------
[2022-08-08 02:29:16,099] lr: 3.874230e-04
[2022-08-08 02:29:16,142] loss: 0.000334  [    0/ 4758]
[2022-08-08 02:29:16,748] loss: 0.000271  [  480/ 4758]
[2022-08-08 02:29:17,354] loss: 0.000274  [  960/ 4758]
[2022-08-08 02:29:17,958] loss: 0.000408  [ 1440/ 4758]
[2022-08-08 02:29:18,573] loss: 0.000268  [ 1920/ 4758]
[2022-08-08 02:29:19,187] loss: 0.000392  [ 2400/ 4758]
[2022-08-08 02:29:19,801] loss: 0.000362  [ 2880/ 4758]
[2022-08-08 02:29:20,419] loss: 0.000450  [ 3360/ 4758]
[2022-08-08 02:29:21,030] loss: 0.000340  [ 3840/ 4758]
[2022-08-08 02:29:21,641] loss: 0.000283  [ 4320/ 4758]
[2022-08-08 02:29:23,771] Train Error: Accuracy: 100.000%, Avg loss: 0.000363
[2022-08-08 02:29:24,458] Test  Error: Accuracy: 100.000%, Avg loss: 0.000438
[2022-08-08 02:29:24,458] Done!
[2022-08-08 02:29:24,463] Number of parameters:3293962
[2022-08-08 02:29:24,463] ## end time: 2022-08-08 02:29:24.458182
[2022-08-08 02:29:24,463] ## used time: 0:02:04.913465
