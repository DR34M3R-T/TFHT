[2022-08-08 02:25:15,552] ## start time: 2022-08-08 02:25:15.419655
[2022-08-08 02:25:15,552] Using cuda device
[2022-08-08 02:25:15,553] In train:p&d10.npy.
[2022-08-08 02:25:15,555] One Channel
[2022-08-08 02:25:15,555] With Normal data.
[2022-08-08 02:25:15,555] Nunber of classes:10.
[2022-08-08 02:25:15,556] Nunber of ViT channels:1.
[2022-08-08 02:25:15,795] Totol epochs: 15
[2022-08-08 02:25:15,799] Epoch 1---------------
[2022-08-08 02:25:15,799] lr: 2.000000e-03
[2022-08-08 02:25:15,842] loss: 2.286585  [    0/ 4728]
[2022-08-08 02:25:16,448] loss: 1.704498  [  480/ 4728]
[2022-08-08 02:25:17,060] loss: 1.167343  [  960/ 4728]
[2022-08-08 02:25:17,664] loss: 0.627627  [ 1440/ 4728]
[2022-08-08 02:25:18,267] loss: 0.259829  [ 1920/ 4728]
[2022-08-08 02:25:18,871] loss: 0.250238  [ 2400/ 4728]
[2022-08-08 02:25:19,477] loss: 0.045677  [ 2880/ 4728]
[2022-08-08 02:25:20,080] loss: 0.021705  [ 3360/ 4728]
[2022-08-08 02:25:20,685] loss: 0.275711  [ 3840/ 4728]
[2022-08-08 02:25:21,288] loss: 0.043062  [ 4320/ 4728]
[2022-08-08 02:25:23,357] Train Error: Accuracy: 99.704%, Avg loss: 0.021981
[2022-08-08 02:25:24,054] Test  Error: Accuracy: 99.659%, Avg loss: 0.023403
[2022-08-08 02:25:24,054] Epoch 2---------------
[2022-08-08 02:25:24,055] lr: 1.900000e-03
[2022-08-08 02:25:24,098] loss: 0.039170  [    0/ 4728]
[2022-08-08 02:25:24,705] loss: 0.142905  [  480/ 4728]
[2022-08-08 02:25:25,310] loss: 0.014962  [  960/ 4728]
[2022-08-08 02:25:25,914] loss: 0.005166  [ 1440/ 4728]
[2022-08-08 02:25:26,516] loss: 0.006674  [ 1920/ 4728]
[2022-08-08 02:25:27,120] loss: 0.014338  [ 2400/ 4728]
[2022-08-08 02:25:27,724] loss: 0.012254  [ 2880/ 4728]
[2022-08-08 02:25:28,327] loss: 0.003501  [ 3360/ 4728]
[2022-08-08 02:25:28,930] loss: 0.016184  [ 3840/ 4728]
[2022-08-08 02:25:29,539] loss: 0.003063  [ 4320/ 4728]
[2022-08-08 02:25:31,606] Train Error: Accuracy: 95.939%, Avg loss: 0.135070
[2022-08-08 02:25:32,302] Test  Error: Accuracy: 95.620%, Avg loss: 0.144644
[2022-08-08 02:25:32,303] Epoch 3---------------
[2022-08-08 02:25:32,304] lr: 1.326841e-03
[2022-08-08 02:25:32,347] loss: 0.066230  [    0/ 4728]
[2022-08-08 02:25:32,947] loss: 0.019274  [  480/ 4728]
[2022-08-08 02:25:33,552] loss: 0.009803  [  960/ 4728]
[2022-08-08 02:25:34,156] loss: 0.004244  [ 1440/ 4728]
[2022-08-08 02:25:34,760] loss: 0.002296  [ 1920/ 4728]
[2022-08-08 02:25:35,365] loss: 0.003382  [ 2400/ 4728]
[2022-08-08 02:25:35,966] loss: 0.008437  [ 2880/ 4728]
[2022-08-08 02:25:36,570] loss: 0.001912  [ 3360/ 4728]
[2022-08-08 02:25:37,171] loss: 0.003856  [ 3840/ 4728]
[2022-08-08 02:25:37,773] loss: 0.005117  [ 4320/ 4728]
[2022-08-08 02:25:39,838] Train Error: Accuracy: 99.979%, Avg loss: 0.003769
[2022-08-08 02:25:40,533] Test  Error: Accuracy: 99.854%, Avg loss: 0.010051
[2022-08-08 02:25:40,534] Epoch 4---------------
[2022-08-08 02:25:40,534] lr: 1.260499e-03
[2022-08-08 02:25:40,579] loss: 0.002283  [    0/ 4728]
[2022-08-08 02:25:41,182] loss: 0.002663  [  480/ 4728]
[2022-08-08 02:25:41,784] loss: 0.001345  [  960/ 4728]
[2022-08-08 02:25:42,387] loss: 0.001134  [ 1440/ 4728]
[2022-08-08 02:25:42,990] loss: 0.000966  [ 1920/ 4728]
[2022-08-08 02:25:43,593] loss: 0.001056  [ 2400/ 4728]
[2022-08-08 02:25:44,198] loss: 0.002546  [ 2880/ 4728]
[2022-08-08 02:25:44,801] loss: 0.001980  [ 3360/ 4728]
[2022-08-08 02:25:45,406] loss: 0.000774  [ 3840/ 4728]
[2022-08-08 02:25:46,009] loss: 0.000927  [ 4320/ 4728]
[2022-08-08 02:25:48,076] Train Error: Accuracy: 100.000%, Avg loss: 0.001005
[2022-08-08 02:25:48,774] Test  Error: Accuracy: 100.000%, Avg loss: 0.001523
[2022-08-08 02:25:48,775] Epoch 5---------------
[2022-08-08 02:25:48,776] lr: 1.197474e-03
[2022-08-08 02:25:48,818] loss: 0.000743  [    0/ 4728]
[2022-08-08 02:25:49,421] loss: 0.000856  [  480/ 4728]
[2022-08-08 02:25:50,023] loss: 0.000693  [  960/ 4728]
[2022-08-08 02:25:50,626] loss: 0.000692  [ 1440/ 4728]
[2022-08-08 02:25:51,228] loss: 0.000937  [ 1920/ 4728]
[2022-08-08 02:25:51,831] loss: 0.000951  [ 2400/ 4728]
[2022-08-08 02:25:52,432] loss: 0.000697  [ 2880/ 4728]
[2022-08-08 02:25:53,035] loss: 0.000699  [ 3360/ 4728]
[2022-08-08 02:25:53,636] loss: 0.000650  [ 3840/ 4728]
[2022-08-08 02:25:54,237] loss: 0.000561  [ 4320/ 4728]
[2022-08-08 02:25:56,302] Train Error: Accuracy: 100.000%, Avg loss: 0.000730
[2022-08-08 02:25:57,000] Test  Error: Accuracy: 100.000%, Avg loss: 0.001080
[2022-08-08 02:25:57,000] Epoch 6---------------
[2022-08-08 02:25:57,001] lr: 1.137600e-03
[2022-08-08 02:25:57,045] loss: 0.000631  [    0/ 4728]
[2022-08-08 02:25:57,648] loss: 0.000840  [  480/ 4728]
[2022-08-08 02:25:58,252] loss: 0.000525  [  960/ 4728]
[2022-08-08 02:25:58,857] loss: 0.000507  [ 1440/ 4728]
[2022-08-08 02:25:59,460] loss: 0.000464  [ 1920/ 4728]
[2022-08-08 02:26:00,063] loss: 0.000566  [ 2400/ 4728]
[2022-08-08 02:26:00,668] loss: 0.000635  [ 2880/ 4728]
[2022-08-08 02:26:01,274] loss: 0.000543  [ 3360/ 4728]
[2022-08-08 02:26:01,876] loss: 0.000764  [ 3840/ 4728]
[2022-08-08 02:26:02,478] loss: 0.000560  [ 4320/ 4728]
[2022-08-08 02:26:04,549] Train Error: Accuracy: 100.000%, Avg loss: 0.000537
[2022-08-08 02:26:05,244] Test  Error: Accuracy: 100.000%, Avg loss: 0.001034
[2022-08-08 02:26:05,245] Epoch 7---------------
[2022-08-08 02:26:05,246] lr: 1.080720e-03
[2022-08-08 02:26:05,288] loss: 0.000490  [    0/ 4728]
[2022-08-08 02:26:05,892] loss: 0.000551  [  480/ 4728]
[2022-08-08 02:26:06,495] loss: 0.000478  [  960/ 4728]
[2022-08-08 02:26:07,098] loss: 0.000390  [ 1440/ 4728]
[2022-08-08 02:26:07,705] loss: 0.000430  [ 1920/ 4728]
[2022-08-08 02:26:08,306] loss: 0.000559  [ 2400/ 4728]
[2022-08-08 02:26:08,910] loss: 0.000582  [ 2880/ 4728]
[2022-08-08 02:26:09,514] loss: 0.000588  [ 3360/ 4728]
[2022-08-08 02:26:10,117] loss: 0.000445  [ 3840/ 4728]
[2022-08-08 02:26:10,720] loss: 0.000457  [ 4320/ 4728]
[2022-08-08 02:26:12,789] Train Error: Accuracy: 100.000%, Avg loss: 0.000468
[2022-08-08 02:26:13,485] Test  Error: Accuracy: 100.000%, Avg loss: 0.000748
[2022-08-08 02:26:13,485] Epoch 8---------------
[2022-08-08 02:26:13,486] lr: 1.026684e-03
[2022-08-08 02:26:13,530] loss: 0.000459  [    0/ 4728]
[2022-08-08 02:26:14,135] loss: 0.000413  [  480/ 4728]
[2022-08-08 02:26:14,738] loss: 0.000439  [  960/ 4728]
[2022-08-08 02:26:15,341] loss: 0.000431  [ 1440/ 4728]
[2022-08-08 02:26:15,952] loss: 0.000415  [ 1920/ 4728]
[2022-08-08 02:26:16,557] loss: 0.000393  [ 2400/ 4728]
[2022-08-08 02:26:17,162] loss: 0.000417  [ 2880/ 4728]
[2022-08-08 02:26:17,770] loss: 0.000476  [ 3360/ 4728]
[2022-08-08 02:26:18,373] loss: 0.000542  [ 3840/ 4728]
[2022-08-08 02:26:18,974] loss: 0.000318  [ 4320/ 4728]
[2022-08-08 02:26:21,046] Train Error: Accuracy: 100.000%, Avg loss: 0.000427
[2022-08-08 02:26:21,744] Test  Error: Accuracy: 100.000%, Avg loss: 0.000595
[2022-08-08 02:26:21,744] Epoch 9---------------
[2022-08-08 02:26:21,745] lr: 9.753500e-04
[2022-08-08 02:26:21,788] loss: 0.000446  [    0/ 4728]
[2022-08-08 02:26:22,391] loss: 0.000347  [  480/ 4728]
[2022-08-08 02:26:22,997] loss: 0.000315  [  960/ 4728]
[2022-08-08 02:26:23,603] loss: 0.000411  [ 1440/ 4728]
[2022-08-08 02:26:24,207] loss: 0.000439  [ 1920/ 4728]
[2022-08-08 02:26:24,814] loss: 0.000321  [ 2400/ 4728]
[2022-08-08 02:26:25,416] loss: 0.000428  [ 2880/ 4728]
[2022-08-08 02:26:26,022] loss: 0.000526  [ 3360/ 4728]
[2022-08-08 02:26:26,627] loss: 0.000501  [ 3840/ 4728]
[2022-08-08 02:26:27,230] loss: 0.000327  [ 4320/ 4728]
[2022-08-08 02:26:29,297] Train Error: Accuracy: 100.000%, Avg loss: 0.000407
[2022-08-08 02:26:29,995] Test  Error: Accuracy: 99.951%, Avg loss: 0.001080
[2022-08-08 02:26:29,995] Epoch 10---------------
[2022-08-08 02:26:29,997] lr: 6.811233e-04
[2022-08-08 02:26:30,039] loss: 0.000357  [    0/ 4728]
[2022-08-08 02:26:30,642] loss: 0.000547  [  480/ 4728]
[2022-08-08 02:26:31,245] loss: 0.000361  [  960/ 4728]
[2022-08-08 02:26:31,846] loss: 0.000388  [ 1440/ 4728]
[2022-08-08 02:26:32,449] loss: 0.000475  [ 1920/ 4728]
[2022-08-08 02:26:33,053] loss: 0.000453  [ 2400/ 4728]
[2022-08-08 02:26:33,655] loss: 0.000319  [ 2880/ 4728]
[2022-08-08 02:26:34,260] loss: 0.000292  [ 3360/ 4728]
[2022-08-08 02:26:34,864] loss: 0.000436  [ 3840/ 4728]
[2022-08-08 02:26:35,468] loss: 0.000439  [ 4320/ 4728]
[2022-08-08 02:26:37,537] Train Error: Accuracy: 100.000%, Avg loss: 0.000408
[2022-08-08 02:26:38,233] Test  Error: Accuracy: 99.951%, Avg loss: 0.001551
[2022-08-08 02:26:38,233] Epoch 11---------------
[2022-08-08 02:26:38,234] lr: 4.756538e-04
[2022-08-08 02:26:38,278] loss: 0.000350  [    0/ 4728]
[2022-08-08 02:26:38,882] loss: 0.000310  [  480/ 4728]
[2022-08-08 02:26:39,485] loss: 0.000347  [  960/ 4728]
[2022-08-08 02:26:40,089] loss: 0.000318  [ 1440/ 4728]
[2022-08-08 02:26:40,691] loss: 0.000353  [ 1920/ 4728]
[2022-08-08 02:26:41,293] loss: 0.000327  [ 2400/ 4728]
[2022-08-08 02:26:41,898] loss: 0.000411  [ 2880/ 4728]
[2022-08-08 02:26:42,498] loss: 0.000396  [ 3360/ 4728]
[2022-08-08 02:26:43,103] loss: 0.000346  [ 3840/ 4728]
[2022-08-08 02:26:43,709] loss: 0.000281  [ 4320/ 4728]
[2022-08-08 02:26:45,775] Train Error: Accuracy: 100.000%, Avg loss: 0.000382
[2022-08-08 02:26:46,471] Test  Error: Accuracy: 100.000%, Avg loss: 0.000844
[2022-08-08 02:26:46,472] Epoch 12---------------
[2022-08-08 02:26:46,473] lr: 4.518711e-04
[2022-08-08 02:26:46,515] loss: 0.000445  [    0/ 4728]
[2022-08-08 02:26:47,120] loss: 0.000433  [  480/ 4728]
[2022-08-08 02:26:47,725] loss: 0.000540  [  960/ 4728]
[2022-08-08 02:26:48,329] loss: 0.000437  [ 1440/ 4728]
[2022-08-08 02:26:48,932] loss: 0.000527  [ 1920/ 4728]
[2022-08-08 02:26:49,536] loss: 0.000351  [ 2400/ 4728]
[2022-08-08 02:26:50,141] loss: 0.000453  [ 2880/ 4728]
[2022-08-08 02:26:50,745] loss: 0.000426  [ 3360/ 4728]
[2022-08-08 02:26:51,348] loss: 0.000298  [ 3840/ 4728]
[2022-08-08 02:26:51,951] loss: 0.000357  [ 4320/ 4728]
[2022-08-08 02:26:54,017] Train Error: Accuracy: 100.000%, Avg loss: 0.000375
[2022-08-08 02:26:54,715] Test  Error: Accuracy: 100.000%, Avg loss: 0.000572
[2022-08-08 02:26:54,722] Epoch 13---------------
[2022-08-08 02:26:54,723] lr: 4.292775e-04
[2022-08-08 02:26:54,766] loss: 0.000272  [    0/ 4728]
[2022-08-08 02:26:55,373] loss: 0.000404  [  480/ 4728]
[2022-08-08 02:26:55,980] loss: 0.000490  [  960/ 4728]
[2022-08-08 02:26:56,584] loss: 0.000335  [ 1440/ 4728]
[2022-08-08 02:26:57,186] loss: 0.000402  [ 1920/ 4728]
[2022-08-08 02:26:57,789] loss: 0.000341  [ 2400/ 4728]
[2022-08-08 02:26:58,393] loss: 0.000300  [ 2880/ 4728]
[2022-08-08 02:26:58,998] loss: 0.000479  [ 3360/ 4728]
[2022-08-08 02:26:59,601] loss: 0.000370  [ 3840/ 4728]
[2022-08-08 02:27:00,206] loss: 0.000578  [ 4320/ 4728]
[2022-08-08 02:27:02,274] Train Error: Accuracy: 100.000%, Avg loss: 0.000393
[2022-08-08 02:27:02,972] Test  Error: Accuracy: 100.000%, Avg loss: 0.000873
[2022-08-08 02:27:02,972] Epoch 14---------------
[2022-08-08 02:27:02,973] lr: 2.997805e-04
[2022-08-08 02:27:03,015] loss: 0.000375  [    0/ 4728]
[2022-08-08 02:27:03,618] loss: 0.000312  [  480/ 4728]
[2022-08-08 02:27:04,223] loss: 0.000598  [  960/ 4728]
[2022-08-08 02:27:04,827] loss: 0.000326  [ 1440/ 4728]
[2022-08-08 02:27:05,431] loss: 0.000288  [ 1920/ 4728]
[2022-08-08 02:27:06,035] loss: 0.000387  [ 2400/ 4728]
[2022-08-08 02:27:06,639] loss: 0.000333  [ 2880/ 4728]
[2022-08-08 02:27:07,244] loss: 0.000332  [ 3360/ 4728]
[2022-08-08 02:27:07,855] loss: 0.000331  [ 3840/ 4728]
[2022-08-08 02:27:08,461] loss: 0.000340  [ 4320/ 4728]
[2022-08-08 02:27:10,528] Train Error: Accuracy: 100.000%, Avg loss: 0.000397
[2022-08-08 02:27:11,223] Test  Error: Accuracy: 100.000%, Avg loss: 0.000792
[2022-08-08 02:27:11,225] Epoch 15---------------
[2022-08-08 02:27:11,226] lr: 2.847915e-04
[2022-08-08 02:27:11,267] loss: 0.000430  [    0/ 4728]
[2022-08-08 02:27:11,872] loss: 0.000304  [  480/ 4728]
[2022-08-08 02:27:12,478] loss: 0.001128  [  960/ 4728]
[2022-08-08 02:27:13,086] loss: 0.000260  [ 1440/ 4728]
[2022-08-08 02:27:13,690] loss: 0.000468  [ 1920/ 4728]
[2022-08-08 02:27:14,296] loss: 0.000336  [ 2400/ 4728]
[2022-08-08 02:27:14,903] loss: 0.000391  [ 2880/ 4728]
[2022-08-08 02:27:15,507] loss: 0.000344  [ 3360/ 4728]
[2022-08-08 02:27:16,112] loss: 0.000355  [ 3840/ 4728]
[2022-08-08 02:27:16,716] loss: 0.000303  [ 4320/ 4728]
[2022-08-08 02:27:18,788] Train Error: Accuracy: 100.000%, Avg loss: 0.000364
[2022-08-08 02:27:19,484] Test  Error: Accuracy: 100.000%, Avg loss: 0.000567
[2022-08-08 02:27:19,485] Done!
[2022-08-08 02:27:19,490] Number of parameters:3293962
[2022-08-08 02:27:19,490] ## end time: 2022-08-08 02:27:19.485703
[2022-08-08 02:27:19,491] ## used time: 0:02:04.066048
