[2022-08-07 16:07:26,353] ## start time: 2022-08-07 16:07:26.220192
[2022-08-07 16:07:26,353] Using cuda device
[2022-08-07 16:07:26,354] In train:p&d10.npy.
[2022-08-07 16:07:26,355] One Channel
[2022-08-07 16:07:26,355] With Normal data.
[2022-08-07 16:07:26,356] Nunber of classes:10.
[2022-08-07 16:07:26,356] Nunber of ViT channels:1.
[2022-08-07 16:07:26,582] Totol epochs: 15
[2022-08-07 16:07:26,584] Epoch 1---------------
[2022-08-07 16:07:26,584] lr: 2.000000e-03
[2022-08-07 16:07:27,579] loss: 2.616312  [    0/ 4818]
[2022-08-07 16:07:42,461] loss: 1.668181  [  960/ 4818]
[2022-08-07 16:07:57,342] loss: 1.711462  [ 1920/ 4818]
[2022-08-07 16:08:12,223] loss: 1.123345  [ 2880/ 4818]
[2022-08-07 16:08:27,104] loss: 0.499109  [ 3840/ 4818]
[2022-08-07 16:08:41,279] loss: 0.234455  [ 4800/ 4818]
[2022-08-07 16:09:08,350] Train Error: Accuracy: 86.260%, Avg loss: 0.414253
[2022-08-07 16:09:19,393] Test  Error: Accuracy: 85.598%, Avg loss: 0.438821
[2022-08-07 16:09:19,393] Epoch 2---------------
[2022-08-07 16:09:19,394] lr: 1.900000e-03
[2022-08-07 16:09:20,388] loss: 0.581905  [    0/ 4818]
[2022-08-07 16:09:35,268] loss: 0.279229  [  960/ 4818]
[2022-08-07 16:09:50,243] loss: 0.100180  [ 1920/ 4818]
[2022-08-07 16:10:05,231] loss: 0.114910  [ 2880/ 4818]
[2022-08-07 16:10:20,222] loss: 0.025508  [ 3840/ 4818]
[2022-08-07 16:10:34,501] loss: 0.040182  [ 4800/ 4818]
[2022-08-07 16:11:01,779] Train Error: Accuracy: 99.024%, Avg loss: 0.044480
[2022-08-07 16:11:12,903] Test  Error: Accuracy: 98.830%, Avg loss: 0.054540
[2022-08-07 16:11:12,904] Epoch 3---------------
[2022-08-07 16:11:12,904] lr: 1.805000e-03
[2022-08-07 16:11:13,906] loss: 0.029361  [    0/ 4818]
[2022-08-07 16:11:28,896] loss: 0.023091  [  960/ 4818]
[2022-08-07 16:11:43,885] loss: 0.068619  [ 1920/ 4818]
[2022-08-07 16:11:58,875] loss: 0.007111  [ 2880/ 4818]
[2022-08-07 16:12:13,864] loss: 0.011071  [ 3840/ 4818]
[2022-08-07 16:12:28,143] loss: 0.023400  [ 4800/ 4818]
[2022-08-07 16:12:55,409] Train Error: Accuracy: 99.585%, Avg loss: 0.019837
[2022-08-07 16:13:06,533] Test  Error: Accuracy: 99.135%, Avg loss: 0.031539
[2022-08-07 16:13:06,534] Epoch 4---------------
[2022-08-07 16:13:06,535] lr: 1.714750e-03
[2022-08-07 16:13:07,536] loss: 0.014736  [    0/ 4818]
[2022-08-07 16:13:22,527] loss: 0.023184  [  960/ 4818]
[2022-08-07 16:13:37,517] loss: 0.010048  [ 1920/ 4818]
[2022-08-07 16:13:52,507] loss: 0.007142  [ 2880/ 4818]
[2022-08-07 16:14:07,497] loss: 0.003964  [ 3840/ 4818]
[2022-08-07 16:14:21,777] loss: 0.955363  [ 4800/ 4818]
[2022-08-07 16:14:49,045] Train Error: Accuracy: 47.862%, Avg loss: 2.437637
[2022-08-07 16:15:00,174] Test  Error: Accuracy: 46.310%, Avg loss: 2.510020
[2022-08-07 16:15:00,174] Epoch 5---------------
[2022-08-07 16:15:00,175] lr: 1.197474e-03
[2022-08-07 16:15:01,176] loss: 2.899131  [    0/ 4818]
[2022-08-07 16:15:16,165] loss: 0.084790  [  960/ 4818]
[2022-08-07 16:15:31,155] loss: 0.036432  [ 1920/ 4818]
[2022-08-07 16:15:46,144] loss: 0.015688  [ 2880/ 4818]
[2022-08-07 16:16:01,133] loss: 0.027746  [ 3840/ 4818]
[2022-08-07 16:16:15,412] loss: 0.015865  [ 4800/ 4818]
[2022-08-07 16:16:42,679] Train Error: Accuracy: 99.689%, Avg loss: 0.015668
[2022-08-07 16:16:53,803] Test  Error: Accuracy: 99.440%, Avg loss: 0.024467
[2022-08-07 16:16:53,803] Epoch 6---------------
[2022-08-07 16:16:53,803] lr: 1.137600e-03
[2022-08-07 16:16:54,804] loss: 0.011540  [    0/ 4818]
[2022-08-07 16:17:09,794] loss: 0.027657  [  960/ 4818]
[2022-08-07 16:17:24,784] loss: 0.007491  [ 1920/ 4818]
[2022-08-07 16:17:39,772] loss: 0.008064  [ 2880/ 4818]
[2022-08-07 16:17:54,763] loss: 0.018498  [ 3840/ 4818]
[2022-08-07 16:18:09,043] loss: 0.022419  [ 4800/ 4818]
[2022-08-07 16:18:36,310] Train Error: Accuracy: 99.626%, Avg loss: 0.015340
[2022-08-07 16:18:47,431] Test  Error: Accuracy: 99.440%, Avg loss: 0.027059
[2022-08-07 16:18:47,432] Epoch 7---------------
[2022-08-07 16:18:47,432] lr: 9.753500e-04
[2022-08-07 16:18:48,433] loss: 0.044463  [    0/ 4818]
[2022-08-07 16:19:03,423] loss: 0.014763  [  960/ 4818]
[2022-08-07 16:19:18,413] loss: 0.005869  [ 1920/ 4818]
[2022-08-07 16:19:33,404] loss: 0.004128  [ 2880/ 4818]
[2022-08-07 16:19:48,396] loss: 0.006675  [ 3840/ 4818]
[2022-08-07 16:20:02,676] loss: 0.001643  [ 4800/ 4818]
[2022-08-07 16:20:29,954] Train Error: Accuracy: 99.979%, Avg loss: 0.003982
[2022-08-07 16:20:41,079] Test  Error: Accuracy: 99.746%, Avg loss: 0.012003
[2022-08-07 16:20:41,079] Epoch 8---------------
[2022-08-07 16:20:41,079] lr: 9.265825e-04
[2022-08-07 16:20:42,080] loss: 0.004767  [    0/ 4818]
[2022-08-07 16:20:57,070] loss: 0.003157  [  960/ 4818]
[2022-08-07 16:21:12,061] loss: 0.002142  [ 1920/ 4818]
[2022-08-07 16:21:27,051] loss: 0.003994  [ 2880/ 4818]
[2022-08-07 16:21:42,041] loss: 0.002783  [ 3840/ 4818]
[2022-08-07 16:21:56,321] loss: 0.002015  [ 4800/ 4818]
[2022-08-07 16:22:23,591] Train Error: Accuracy: 99.979%, Avg loss: 0.002648
[2022-08-07 16:22:34,716] Test  Error: Accuracy: 99.695%, Avg loss: 0.011156
[2022-08-07 16:22:34,716] Epoch 9---------------
[2022-08-07 16:22:34,716] lr: 8.802533e-04
[2022-08-07 16:22:35,717] loss: 0.001302  [    0/ 4818]
[2022-08-07 16:22:50,706] loss: 0.002145  [  960/ 4818]
[2022-08-07 16:23:05,696] loss: 0.001812  [ 1920/ 4818]
[2022-08-07 16:23:20,684] loss: 0.002724  [ 2880/ 4818]
[2022-08-07 16:23:35,674] loss: 0.000824  [ 3840/ 4818]
[2022-08-07 16:23:49,952] loss: 0.002873  [ 4800/ 4818]
[2022-08-07 16:24:17,224] Train Error: Accuracy: 99.938%, Avg loss: 0.004712
[2022-08-07 16:24:28,346] Test  Error: Accuracy: 99.695%, Avg loss: 0.014853
[2022-08-07 16:24:28,347] Epoch 10---------------
[2022-08-07 16:24:28,347] lr: 6.147137e-04
[2022-08-07 16:24:29,347] loss: 0.003606  [    0/ 4818]
[2022-08-07 16:24:44,336] loss: 0.002020  [  960/ 4818]
[2022-08-07 16:24:59,325] loss: 0.001923  [ 1920/ 4818]
[2022-08-07 16:25:14,314] loss: 0.000953  [ 2880/ 4818]
[2022-08-07 16:25:29,303] loss: 0.001516  [ 3840/ 4818]
[2022-08-07 16:25:43,581] loss: 0.001560  [ 4800/ 4818]
[2022-08-07 16:26:10,854] Train Error: Accuracy: 100.000%, Avg loss: 0.002835
[2022-08-07 16:26:21,976] Test  Error: Accuracy: 99.644%, Avg loss: 0.010494
[2022-08-07 16:26:21,977] Epoch 11---------------
[2022-08-07 16:26:21,977] lr: 5.839780e-04
[2022-08-07 16:26:22,977] loss: 0.011792  [    0/ 4818]
[2022-08-07 16:26:37,968] loss: 0.001141  [  960/ 4818]
[2022-08-07 16:26:52,957] loss: 0.001630  [ 1920/ 4818]
[2022-08-07 16:27:07,947] loss: 0.001393  [ 2880/ 4818]
[2022-08-07 16:27:22,938] loss: 0.001856  [ 3840/ 4818]
[2022-08-07 16:27:37,216] loss: 0.000527  [ 4800/ 4818]
[2022-08-07 16:28:04,487] Train Error: Accuracy: 100.000%, Avg loss: 0.001569
[2022-08-07 16:28:15,610] Test  Error: Accuracy: 99.847%, Avg loss: 0.007600
[2022-08-07 16:28:15,610] Epoch 12---------------
[2022-08-07 16:28:15,613] lr: 5.547791e-04
[2022-08-07 16:28:16,615] loss: 0.001004  [    0/ 4818]
[2022-08-07 16:28:31,606] loss: 0.000983  [  960/ 4818]
[2022-08-07 16:28:46,596] loss: 0.000845  [ 1920/ 4818]
[2022-08-07 16:29:01,587] loss: 0.004575  [ 2880/ 4818]
[2022-08-07 16:29:16,576] loss: 0.003330  [ 3840/ 4818]
[2022-08-07 16:29:30,855] loss: 0.000573  [ 4800/ 4818]
[2022-08-07 16:29:58,124] Train Error: Accuracy: 100.000%, Avg loss: 0.001482
[2022-08-07 16:30:09,249] Test  Error: Accuracy: 99.796%, Avg loss: 0.008204
[2022-08-07 16:30:09,249] Epoch 13---------------
[2022-08-07 16:30:09,250] lr: 4.756538e-04
[2022-08-07 16:30:10,252] loss: 0.001103  [    0/ 4818]
[2022-08-07 16:30:25,241] loss: 0.002094  [  960/ 4818]
[2022-08-07 16:30:41,661] loss: 0.001991  [ 1920/ 4818]
[2022-08-07 16:30:56,652] loss: 0.000951  [ 2880/ 4818]
[2022-08-07 16:31:11,641] loss: 0.001128  [ 3840/ 4818]
[2022-08-07 16:31:25,920] loss: 0.001856  [ 4800/ 4818]
[2022-08-07 16:31:53,187] Train Error: Accuracy: 100.000%, Avg loss: 0.001318
[2022-08-07 16:32:04,310] Test  Error: Accuracy: 99.847%, Avg loss: 0.006315
[2022-08-07 16:32:04,310] Epoch 14---------------
[2022-08-07 16:32:04,311] lr: 4.518711e-04
[2022-08-07 16:32:05,313] loss: 0.001117  [    0/ 4818]
[2022-08-07 16:32:20,301] loss: 0.000715  [  960/ 4818]
[2022-08-07 16:32:35,293] loss: 0.000857  [ 1920/ 4818]
[2022-08-07 16:32:50,283] loss: 0.002549  [ 2880/ 4818]
[2022-08-07 16:33:05,273] loss: 0.001468  [ 3840/ 4818]
[2022-08-07 16:33:19,552] loss: 0.000454  [ 4800/ 4818]
[2022-08-07 16:33:46,826] Train Error: Accuracy: 100.000%, Avg loss: 0.001209
[2022-08-07 16:33:57,951] Test  Error: Accuracy: 99.746%, Avg loss: 0.007288
[2022-08-07 16:33:57,952] Epoch 15---------------
[2022-08-07 16:33:57,952] lr: 3.496492e-04
[2022-08-07 16:33:58,953] loss: 0.001067  [    0/ 4818]
[2022-08-07 16:34:13,944] loss: 0.001179  [  960/ 4818]
[2022-08-07 16:34:28,937] loss: 0.000720  [ 1920/ 4818]
[2022-08-07 16:34:43,927] loss: 0.000670  [ 2880/ 4818]
[2022-08-07 16:34:58,919] loss: 0.001374  [ 3840/ 4818]
[2022-08-07 16:35:13,200] loss: 0.001448  [ 4800/ 4818]
[2022-08-07 16:35:40,483] Train Error: Accuracy: 100.000%, Avg loss: 0.000991
[2022-08-07 16:35:51,611] Test  Error: Accuracy: 99.847%, Avg loss: 0.005610
[2022-08-07 16:35:51,612] Done!
[2022-08-07 16:35:51,615] Number of parameters:2146058
[2022-08-07 16:35:51,615] ## end time: 2022-08-07 16:35:51.612080
[2022-08-07 16:35:51,617] ## used time: 0:28:25.391888
