[2022-08-08 20:02:07,060] ## start time: 2022-08-08 20:02:06.892876
[2022-08-08 20:02:07,061] Using cuda device
[2022-08-08 20:02:07,062] In train:p&d10.npy.
[2022-08-08 20:02:07,066] One Channel
[2022-08-08 20:02:07,066] With Normal data.
[2022-08-08 20:02:07,067] Nunber of classes:10.
[2022-08-08 20:02:07,067] Nunber of ViT channels:1.
[2022-08-08 20:02:07,353] Totol epochs: 15
[2022-08-08 20:02:07,356] Epoch 1---------------
[2022-08-08 20:02:07,356] lr: 2.000000e-03
[2022-08-08 20:02:08,670] loss: 2.407683  [    0/ 4725]
[2022-08-08 20:02:28,346] loss: 1.782685  [  480/ 4725]
[2022-08-08 20:02:48,023] loss: 1.677920  [  960/ 4725]
[2022-08-08 20:03:07,700] loss: 1.593334  [ 1440/ 4725]
[2022-08-08 20:03:27,376] loss: 1.442362  [ 1920/ 4725]
[2022-08-08 20:03:47,049] loss: 1.541107  [ 2400/ 4725]
[2022-08-08 20:04:06,725] loss: 0.839779  [ 2880/ 4725]
[2022-08-08 20:04:26,400] loss: 0.660753  [ 3360/ 4725]
[2022-08-08 20:04:46,075] loss: 1.159112  [ 3840/ 4725]
[2022-08-08 20:05:05,749] loss: 0.325355  [ 4320/ 4725]
[2022-08-08 20:06:33,589] Train Error: Accuracy: 74.772%, Avg loss: 0.714901
[2022-08-08 20:07:05,188] Test  Error: Accuracy: 73.081%, Avg loss: 0.726548
[2022-08-08 20:07:05,189] Epoch 2---------------
[2022-08-08 20:07:05,190] lr: 1.900000e-03
[2022-08-08 20:07:06,503] loss: 1.099610  [    0/ 4725]
[2022-08-08 20:07:26,181] loss: 0.368041  [  480/ 4725]
[2022-08-08 20:07:45,857] loss: 0.415914  [  960/ 4725]
[2022-08-08 20:08:05,534] loss: 0.528332  [ 1440/ 4725]
[2022-08-08 20:08:25,210] loss: 0.206324  [ 1920/ 4725]
[2022-08-08 20:08:44,886] loss: 0.363980  [ 2400/ 4725]
[2022-08-08 20:09:04,564] loss: 0.503590  [ 2880/ 4725]
[2022-08-08 20:09:24,240] loss: 0.318014  [ 3360/ 4725]
[2022-08-08 20:09:43,916] loss: 0.828569  [ 3840/ 4725]
[2022-08-08 20:10:03,591] loss: 0.622018  [ 4320/ 4725]
[2022-08-08 20:11:31,406] Train Error: Accuracy: 90.857%, Avg loss: 0.269959
[2022-08-08 20:12:02,998] Test  Error: Accuracy: 89.310%, Avg loss: 0.302481
[2022-08-08 20:12:02,999] Epoch 3---------------
[2022-08-08 20:12:03,000] lr: 1.805000e-03
[2022-08-08 20:12:04,313] loss: 0.350880  [    0/ 4725]
[2022-08-08 20:12:23,989] loss: 0.365484  [  480/ 4725]
[2022-08-08 20:12:43,662] loss: 0.131612  [  960/ 4725]
[2022-08-08 20:13:03,335] loss: 0.194768  [ 1440/ 4725]
[2022-08-08 20:13:23,010] loss: 0.234075  [ 1920/ 4725]
[2022-08-08 20:13:42,685] loss: 0.190067  [ 2400/ 4725]
[2022-08-08 20:14:02,360] loss: 0.175013  [ 2880/ 4725]
[2022-08-08 20:14:22,034] loss: 0.278815  [ 3360/ 4725]
[2022-08-08 20:14:41,709] loss: 0.160006  [ 3840/ 4725]
[2022-08-08 20:15:01,384] loss: 0.242863  [ 4320/ 4725]
[2022-08-08 20:16:29,212] Train Error: Accuracy: 90.116%, Avg loss: 0.276015
[2022-08-08 20:17:00,808] Test  Error: Accuracy: 87.901%, Avg loss: 0.337198
[2022-08-08 20:17:00,808] Epoch 4---------------
[2022-08-08 20:17:00,810] lr: 1.396675e-03
[2022-08-08 20:17:02,123] loss: 0.261779  [    0/ 4725]
[2022-08-08 20:17:21,800] loss: 0.072929  [  480/ 4725]
[2022-08-08 20:17:41,475] loss: 0.218072  [  960/ 4725]
[2022-08-08 20:18:01,151] loss: 0.104419  [ 1440/ 4725]
[2022-08-08 20:18:20,827] loss: 0.161027  [ 1920/ 4725]
[2022-08-08 20:18:40,501] loss: 0.138707  [ 2400/ 4725]
[2022-08-08 20:19:00,175] loss: 0.049505  [ 2880/ 4725]
[2022-08-08 20:19:19,852] loss: 0.121813  [ 3360/ 4725]
[2022-08-08 20:19:39,526] loss: 0.179259  [ 3840/ 4725]
[2022-08-08 20:19:59,199] loss: 0.034671  [ 4320/ 4725]
[2022-08-08 20:21:27,041] Train Error: Accuracy: 93.270%, Avg loss: 0.178647
[2022-08-08 20:21:58,656] Test  Error: Accuracy: 92.323%, Avg loss: 0.212129
[2022-08-08 20:21:58,656] Epoch 5---------------
[2022-08-08 20:21:58,658] lr: 1.326841e-03
[2022-08-08 20:21:59,972] loss: 0.043640  [    0/ 4725]
[2022-08-08 20:22:19,648] loss: 0.045976  [  480/ 4725]
[2022-08-08 20:22:39,324] loss: 0.062461  [  960/ 4725]
[2022-08-08 20:22:58,999] loss: 0.087052  [ 1440/ 4725]
[2022-08-08 20:23:18,672] loss: 0.015185  [ 1920/ 4725]
[2022-08-08 20:23:38,349] loss: 0.145381  [ 2400/ 4725]
[2022-08-08 20:23:58,024] loss: 0.060542  [ 2880/ 4725]
[2022-08-08 20:24:17,700] loss: 0.035955  [ 3360/ 4725]
[2022-08-08 20:24:37,377] loss: 0.176712  [ 3840/ 4725]
[2022-08-08 20:24:57,051] loss: 0.174049  [ 4320/ 4725]
[2022-08-08 20:26:24,891] Train Error: Accuracy: 97.587%, Avg loss: 0.075322
[2022-08-08 20:26:56,494] Test  Error: Accuracy: 96.647%, Avg loss: 0.094834
[2022-08-08 20:26:56,494] Epoch 6---------------
[2022-08-08 20:26:56,495] lr: 1.260499e-03
[2022-08-08 20:26:57,809] loss: 0.099571  [    0/ 4725]
[2022-08-08 20:27:17,485] loss: 0.006337  [  480/ 4725]
[2022-08-08 20:27:37,162] loss: 0.177059  [  960/ 4725]
[2022-08-08 20:27:56,840] loss: 0.033915  [ 1440/ 4725]
[2022-08-08 20:28:16,515] loss: 0.029551  [ 1920/ 4725]
[2022-08-08 20:28:36,192] loss: 0.031421  [ 2400/ 4725]
[2022-08-08 20:28:55,870] loss: 0.045273  [ 2880/ 4725]
[2022-08-08 20:29:15,546] loss: 0.045581  [ 3360/ 4725]
[2022-08-08 20:29:35,223] loss: 0.058918  [ 3840/ 4725]
[2022-08-08 20:29:54,897] loss: 0.048958  [ 4320/ 4725]
[2022-08-08 20:31:22,749] Train Error: Accuracy: 99.323%, Avg loss: 0.029608
[2022-08-08 20:31:54,366] Test  Error: Accuracy: 98.882%, Avg loss: 0.048931
[2022-08-08 20:31:54,367] Epoch 7---------------
[2022-08-08 20:31:54,368] lr: 1.197474e-03
[2022-08-08 20:31:55,681] loss: 0.111436  [    0/ 4725]
[2022-08-08 20:32:15,357] loss: 0.012294  [  480/ 4725]
[2022-08-08 20:32:35,034] loss: 0.190093  [  960/ 4725]
[2022-08-08 20:32:54,709] loss: 0.016063  [ 1440/ 4725]
[2022-08-08 20:33:14,383] loss: 0.022677  [ 1920/ 4725]
[2022-08-08 20:33:34,059] loss: 0.141458  [ 2400/ 4725]
[2022-08-08 20:33:53,733] loss: 0.157762  [ 2880/ 4725]
[2022-08-08 20:34:13,407] loss: 0.014983  [ 3360/ 4725]
[2022-08-08 20:34:33,082] loss: 0.094054  [ 3840/ 4725]
[2022-08-08 20:34:52,757] loss: 0.006782  [ 4320/ 4725]
[2022-08-08 20:36:20,593] Train Error: Accuracy: 97.884%, Avg loss: 0.062430
[2022-08-08 20:36:52,197] Test  Error: Accuracy: 96.599%, Avg loss: 0.092069
[2022-08-08 20:36:52,197] Epoch 8---------------
[2022-08-08 20:36:52,198] lr: 8.362407e-04
[2022-08-08 20:36:53,513] loss: 0.162757  [    0/ 4725]
[2022-08-08 20:37:13,190] loss: 0.008728  [  480/ 4725]
[2022-08-08 20:37:32,867] loss: 0.025132  [  960/ 4725]
[2022-08-08 20:37:52,541] loss: 0.019103  [ 1440/ 4725]
[2022-08-08 20:38:12,216] loss: 0.006178  [ 1920/ 4725]
[2022-08-08 20:38:31,889] loss: 0.004285  [ 2400/ 4725]
[2022-08-08 20:38:51,565] loss: 0.018886  [ 2880/ 4725]
[2022-08-08 20:39:11,240] loss: 0.010669  [ 3360/ 4725]
[2022-08-08 20:39:30,915] loss: 0.048539  [ 3840/ 4725]
[2022-08-08 20:39:50,590] loss: 0.009256  [ 4320/ 4725]
[2022-08-08 20:41:18,439] Train Error: Accuracy: 99.238%, Avg loss: 0.021907
[2022-08-08 20:41:50,039] Test  Error: Accuracy: 98.785%, Avg loss: 0.040191
[2022-08-08 20:41:50,039] Epoch 9---------------
[2022-08-08 20:41:50,040] lr: 7.944286e-04
[2022-08-08 20:41:51,353] loss: 0.022880  [    0/ 4725]
[2022-08-08 20:42:11,028] loss: 0.018633  [  480/ 4725]
[2022-08-08 20:42:30,704] loss: 0.004310  [  960/ 4725]
[2022-08-08 20:42:50,380] loss: 0.043633  [ 1440/ 4725]
[2022-08-08 20:43:10,056] loss: 0.011215  [ 1920/ 4725]
[2022-08-08 20:43:29,730] loss: 0.013015  [ 2400/ 4725]
[2022-08-08 20:43:49,404] loss: 0.035205  [ 2880/ 4725]
[2022-08-08 20:44:09,078] loss: 0.003857  [ 3360/ 4725]
[2022-08-08 20:44:28,753] loss: 0.006055  [ 3840/ 4725]
[2022-08-08 20:44:48,430] loss: 0.003726  [ 4320/ 4725]
[2022-08-08 20:46:16,273] Train Error: Accuracy: 99.810%, Avg loss: 0.012134
[2022-08-08 20:46:47,881] Test  Error: Accuracy: 99.028%, Avg loss: 0.034297
[2022-08-08 20:46:47,881] Epoch 10---------------
[2022-08-08 20:46:47,882] lr: 7.547072e-04
[2022-08-08 20:46:49,196] loss: 0.019549  [    0/ 4725]
[2022-08-08 20:47:08,872] loss: 0.001783  [  480/ 4725]
[2022-08-08 20:47:28,547] loss: 0.003999  [  960/ 4725]
[2022-08-08 20:47:48,223] loss: 0.017123  [ 1440/ 4725]
[2022-08-08 20:48:07,901] loss: 0.005101  [ 1920/ 4725]
[2022-08-08 20:48:27,578] loss: 0.001930  [ 2400/ 4725]
[2022-08-08 20:48:47,254] loss: 0.052076  [ 2880/ 4725]
[2022-08-08 20:49:06,930] loss: 0.080709  [ 3360/ 4725]
[2022-08-08 20:49:26,605] loss: 0.017101  [ 3840/ 4725]
[2022-08-08 20:49:46,283] loss: 0.016367  [ 4320/ 4725]
[2022-08-08 20:51:14,134] Train Error: Accuracy: 99.556%, Avg loss: 0.015503
[2022-08-08 20:51:45,742] Test  Error: Accuracy: 98.980%, Avg loss: 0.030614
[2022-08-08 20:51:45,743] Epoch 11---------------
[2022-08-08 20:51:45,745] lr: 7.169718e-04
[2022-08-08 20:51:47,058] loss: 0.010982  [    0/ 4725]
[2022-08-08 20:52:06,734] loss: 0.005368  [  480/ 4725]
[2022-08-08 20:52:26,409] loss: 0.002105  [  960/ 4725]
[2022-08-08 20:52:46,084] loss: 0.079067  [ 1440/ 4725]
[2022-08-08 20:53:05,759] loss: 0.008038  [ 1920/ 4725]
[2022-08-08 20:53:25,433] loss: 0.004139  [ 2400/ 4725]
[2022-08-08 20:53:45,107] loss: 0.007162  [ 2880/ 4725]
[2022-08-08 20:54:04,782] loss: 0.017866  [ 3360/ 4725]
[2022-08-08 20:54:24,459] loss: 0.017648  [ 3840/ 4725]
[2022-08-08 20:54:44,133] loss: 0.003323  [ 4320/ 4725]
[2022-08-08 20:56:11,969] Train Error: Accuracy: 99.725%, Avg loss: 0.009637
[2022-08-08 20:56:43,575] Test  Error: Accuracy: 98.980%, Avg loss: 0.029433
[2022-08-08 20:56:43,575] Epoch 12---------------
[2022-08-08 20:56:43,577] lr: 6.811233e-04
[2022-08-08 20:56:44,892] loss: 0.001073  [    0/ 4725]
[2022-08-08 20:57:04,569] loss: 0.010310  [  480/ 4725]
[2022-08-08 20:57:24,244] loss: 0.000456  [  960/ 4725]
[2022-08-08 20:57:43,921] loss: 0.005212  [ 1440/ 4725]
[2022-08-08 20:58:03,596] loss: 0.004622  [ 1920/ 4725]
[2022-08-08 20:58:23,272] loss: 0.002970  [ 2400/ 4725]
[2022-08-08 20:58:42,946] loss: 0.002612  [ 2880/ 4725]
[2022-08-08 20:59:02,621] loss: 0.007354  [ 3360/ 4725]
[2022-08-08 20:59:22,299] loss: 0.008759  [ 3840/ 4725]
[2022-08-08 20:59:41,974] loss: 0.002251  [ 4320/ 4725]
[2022-08-08 21:01:09,824] Train Error: Accuracy: 99.915%, Avg loss: 0.006378
[2022-08-08 21:01:41,418] Test  Error: Accuracy: 99.271%, Avg loss: 0.026028
[2022-08-08 21:01:41,419] Epoch 13---------------
[2022-08-08 21:01:41,420] lr: 6.470671e-04
[2022-08-08 21:01:42,733] loss: 0.051398  [    0/ 4725]
[2022-08-08 21:02:02,409] loss: 0.004593  [  480/ 4725]
[2022-08-08 21:02:22,086] loss: 0.001873  [  960/ 4725]
[2022-08-08 21:02:41,763] loss: 0.001260  [ 1440/ 4725]
[2022-08-08 21:03:01,439] loss: 0.004680  [ 1920/ 4725]
[2022-08-08 21:03:21,116] loss: 0.003203  [ 2400/ 4725]
[2022-08-08 21:03:40,792] loss: 0.000905  [ 2880/ 4725]
[2022-08-08 21:04:00,469] loss: 0.000418  [ 3360/ 4725]
[2022-08-08 21:04:20,146] loss: 0.000625  [ 3840/ 4725]
[2022-08-08 21:04:39,825] loss: 0.000341  [ 4320/ 4725]
[2022-08-08 21:06:07,649] Train Error: Accuracy: 99.810%, Avg loss: 0.007678
[2022-08-08 21:06:39,251] Test  Error: Accuracy: 99.466%, Avg loss: 0.020522
[2022-08-08 21:06:39,252] Epoch 14---------------
[2022-08-08 21:06:39,254] lr: 6.147137e-04
[2022-08-08 21:06:40,568] loss: 0.000671  [    0/ 4725]
[2022-08-08 21:07:00,246] loss: 0.060789  [  480/ 4725]
[2022-08-08 21:07:19,922] loss: 0.000929  [  960/ 4725]
[2022-08-08 21:07:39,601] loss: 0.000761  [ 1440/ 4725]
[2022-08-08 21:07:59,280] loss: 0.046742  [ 1920/ 4725]
[2022-08-08 21:08:18,960] loss: 0.007022  [ 2400/ 4725]
[2022-08-08 21:08:38,627] loss: 0.002347  [ 2880/ 4725]
[2022-08-08 21:08:58,301] loss: 0.004670  [ 3360/ 4725]
[2022-08-08 21:09:17,979] loss: 0.037652  [ 3840/ 4725]
[2022-08-08 21:09:37,652] loss: 0.017823  [ 4320/ 4725]
[2022-08-08 21:11:05,466] Train Error: Accuracy: 99.894%, Avg loss: 0.006323
[2022-08-08 21:11:37,054] Test  Error: Accuracy: 99.368%, Avg loss: 0.025891
[2022-08-08 21:11:37,055] Epoch 15---------------
[2022-08-08 21:11:37,055] lr: 4.292775e-04
[2022-08-08 21:11:38,371] loss: 0.015580  [    0/ 4725]
[2022-08-08 21:11:58,042] loss: 0.007551  [  480/ 4725]
[2022-08-08 21:12:17,715] loss: 0.001080  [  960/ 4725]
[2022-08-08 21:12:37,387] loss: 0.004973  [ 1440/ 4725]
[2022-08-08 21:12:57,060] loss: 0.001820  [ 1920/ 4725]
[2022-08-08 21:13:16,734] loss: 0.000715  [ 2400/ 4725]
[2022-08-08 21:13:36,407] loss: 0.045095  [ 2880/ 4725]
[2022-08-08 21:13:56,079] loss: 0.000458  [ 3360/ 4725]
[2022-08-08 21:14:15,753] loss: 0.000933  [ 3840/ 4725]
[2022-08-08 21:14:35,424] loss: 0.015725  [ 4320/ 4725]
[2022-08-08 21:16:03,208] Train Error: Accuracy: 99.852%, Avg loss: 0.007322
[2022-08-08 21:16:34,792] Test  Error: Accuracy: 99.320%, Avg loss: 0.021087
[2022-08-08 21:16:34,792] Done!
[2022-08-08 21:16:34,797] Number of parameters:2443018
[2022-08-08 21:16:34,798] ## end time: 2022-08-08 21:16:34.792554
[2022-08-08 21:16:34,798] ## used time: 1:14:27.899678
