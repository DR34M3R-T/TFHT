[2022-08-08 02:22:02,529] ## start time: 2022-08-08 02:22:02.389154
[2022-08-08 02:22:02,529] Using cuda device
[2022-08-08 02:22:02,531] In train:p&d10.npy.
[2022-08-08 02:22:02,531] One Channel
[2022-08-08 02:22:02,532] With Normal data.
[2022-08-08 02:22:02,532] Nunber of classes:10.
[2022-08-08 02:22:02,532] Nunber of ViT channels:1.
[2022-08-08 02:22:02,806] Totol epochs: 15
[2022-08-08 02:22:02,809] Epoch 1---------------
[2022-08-08 02:22:02,810] lr: 2.000000e-03
[2022-08-08 02:22:02,871] loss: 2.439914  [    0/ 4771]
[2022-08-08 02:22:03,776] loss: 1.872275  [  480/ 4771]
[2022-08-08 02:22:04,679] loss: 1.463547  [  960/ 4771]
[2022-08-08 02:22:05,582] loss: 0.659014  [ 1440/ 4771]
[2022-08-08 02:22:06,485] loss: 0.375214  [ 1920/ 4771]
[2022-08-08 02:22:07,387] loss: 0.031229  [ 2400/ 4771]
[2022-08-08 02:22:08,289] loss: 0.054335  [ 2880/ 4771]
[2022-08-08 02:22:09,191] loss: 0.057548  [ 3360/ 4771]
[2022-08-08 02:22:10,094] loss: 0.010784  [ 3840/ 4771]
[2022-08-08 02:22:10,995] loss: 0.091997  [ 4320/ 4771]
[2022-08-08 02:22:14,533] Train Error: Accuracy: 99.937%, Avg loss: 0.015848
[2022-08-08 02:22:15,700] Test  Error: Accuracy: 99.851%, Avg loss: 0.018777
[2022-08-08 02:22:15,700] Epoch 2---------------
[2022-08-08 02:22:15,701] lr: 1.900000e-03
[2022-08-08 02:22:15,764] loss: 0.016899  [    0/ 4771]
[2022-08-08 02:22:16,669] loss: 0.010383  [  480/ 4771]
[2022-08-08 02:22:17,574] loss: 0.005820  [  960/ 4771]
[2022-08-08 02:22:18,478] loss: 0.045002  [ 1440/ 4771]
[2022-08-08 02:22:19,383] loss: 0.050945  [ 1920/ 4771]
[2022-08-08 02:22:20,286] loss: 0.008497  [ 2400/ 4771]
[2022-08-08 02:22:21,190] loss: 0.006709  [ 2880/ 4771]
[2022-08-08 02:22:22,094] loss: 0.060337  [ 3360/ 4771]
[2022-08-08 02:22:22,998] loss: 0.003245  [ 3840/ 4771]
[2022-08-08 02:22:23,900] loss: 0.004324  [ 4320/ 4771]
[2022-08-08 02:22:27,407] Train Error: Accuracy: 99.937%, Avg loss: 0.006873
[2022-08-08 02:22:28,540] Test  Error: Accuracy: 99.801%, Avg loss: 0.008388
[2022-08-08 02:22:28,541] Epoch 3---------------
[2022-08-08 02:22:28,542] lr: 1.805000e-03
[2022-08-08 02:22:28,604] loss: 0.007238  [    0/ 4771]
[2022-08-08 02:22:29,507] loss: 0.002925  [  480/ 4771]
[2022-08-08 02:22:30,410] loss: 0.002220  [  960/ 4771]
[2022-08-08 02:22:31,313] loss: 0.003030  [ 1440/ 4771]
[2022-08-08 02:22:32,216] loss: 0.003555  [ 1920/ 4771]
[2022-08-08 02:22:33,118] loss: 0.002219  [ 2400/ 4771]
[2022-08-08 02:22:34,022] loss: 0.001929  [ 2880/ 4771]
[2022-08-08 02:22:34,925] loss: 0.001752  [ 3360/ 4771]
[2022-08-08 02:22:35,827] loss: 0.001422  [ 3840/ 4771]
[2022-08-08 02:22:36,729] loss: 0.001076  [ 4320/ 4771]
[2022-08-08 02:22:40,235] Train Error: Accuracy: 99.979%, Avg loss: 0.001398
[2022-08-08 02:22:41,367] Test  Error: Accuracy: 100.000%, Avg loss: 0.001337
[2022-08-08 02:22:41,368] Epoch 4---------------
[2022-08-08 02:22:41,369] lr: 1.714750e-03
[2022-08-08 02:22:41,431] loss: 0.000886  [    0/ 4771]
[2022-08-08 02:22:42,335] loss: 0.000848  [  480/ 4771]
[2022-08-08 02:22:43,239] loss: 0.000888  [  960/ 4771]
[2022-08-08 02:22:44,142] loss: 0.000874  [ 1440/ 4771]
[2022-08-08 02:22:45,046] loss: 0.000838  [ 1920/ 4771]
[2022-08-08 02:22:45,950] loss: 0.000850  [ 2400/ 4771]
[2022-08-08 02:22:46,853] loss: 0.000794  [ 2880/ 4771]
[2022-08-08 02:22:47,756] loss: 0.003891  [ 3360/ 4771]
[2022-08-08 02:22:48,660] loss: 0.000720  [ 3840/ 4771]
[2022-08-08 02:22:49,564] loss: 0.000917  [ 4320/ 4771]
[2022-08-08 02:22:53,074] Train Error: Accuracy: 100.000%, Avg loss: 0.000785
[2022-08-08 02:22:54,208] Test  Error: Accuracy: 100.000%, Avg loss: 0.000982
[2022-08-08 02:22:54,208] Epoch 5---------------
[2022-08-08 02:22:54,209] lr: 1.629012e-03
[2022-08-08 02:22:54,272] loss: 0.000809  [    0/ 4771]
[2022-08-08 02:22:55,174] loss: 0.001237  [  480/ 4771]
[2022-08-08 02:22:56,076] loss: 0.001029  [  960/ 4771]
[2022-08-08 02:22:56,980] loss: 0.000706  [ 1440/ 4771]
[2022-08-08 02:22:57,881] loss: 0.000686  [ 1920/ 4771]
[2022-08-08 02:22:58,783] loss: 0.000555  [ 2400/ 4771]
[2022-08-08 02:22:59,685] loss: 0.000562  [ 2880/ 4771]
[2022-08-08 02:23:00,587] loss: 0.000627  [ 3360/ 4771]
[2022-08-08 02:23:01,491] loss: 0.000506  [ 3840/ 4771]
[2022-08-08 02:23:02,392] loss: 0.000539  [ 4320/ 4771]
[2022-08-08 02:23:05,898] Train Error: Accuracy: 100.000%, Avg loss: 0.000553
[2022-08-08 02:23:07,030] Test  Error: Accuracy: 100.000%, Avg loss: 0.000647
[2022-08-08 02:23:07,030] Epoch 6---------------
[2022-08-08 02:23:07,031] lr: 1.547562e-03
[2022-08-08 02:23:07,094] loss: 0.000557  [    0/ 4771]
[2022-08-08 02:23:07,996] loss: 0.000395  [  480/ 4771]
[2022-08-08 02:23:08,900] loss: 0.000561  [  960/ 4771]
[2022-08-08 02:23:09,803] loss: 0.000942  [ 1440/ 4771]
[2022-08-08 02:23:10,706] loss: 0.000594  [ 1920/ 4771]
[2022-08-08 02:23:11,611] loss: 0.000386  [ 2400/ 4771]
[2022-08-08 02:23:12,516] loss: 0.000491  [ 2880/ 4771]
[2022-08-08 02:23:13,419] loss: 0.000419  [ 3360/ 4771]
[2022-08-08 02:23:14,321] loss: 0.000424  [ 3840/ 4771]
[2022-08-08 02:23:15,225] loss: 0.000592  [ 4320/ 4771]
[2022-08-08 02:23:18,731] Train Error: Accuracy: 100.000%, Avg loss: 0.000448
[2022-08-08 02:23:19,865] Test  Error: Accuracy: 100.000%, Avg loss: 0.000548
[2022-08-08 02:23:19,865] Epoch 7---------------
[2022-08-08 02:23:19,868] lr: 1.470184e-03
[2022-08-08 02:23:19,930] loss: 0.000458  [    0/ 4771]
[2022-08-08 02:23:20,832] loss: 0.000400  [  480/ 4771]
[2022-08-08 02:23:21,734] loss: 0.000363  [  960/ 4771]
[2022-08-08 02:23:22,637] loss: 0.000331  [ 1440/ 4771]
[2022-08-08 02:23:23,539] loss: 0.000481  [ 1920/ 4771]
[2022-08-08 02:23:24,441] loss: 0.000380  [ 2400/ 4771]
[2022-08-08 02:23:25,343] loss: 0.000428  [ 2880/ 4771]
[2022-08-08 02:23:26,245] loss: 0.000492  [ 3360/ 4771]
[2022-08-08 02:23:27,147] loss: 0.000425  [ 3840/ 4771]
[2022-08-08 02:23:28,049] loss: 0.000421  [ 4320/ 4771]
[2022-08-08 02:23:31,554] Train Error: Accuracy: 100.000%, Avg loss: 0.000418
[2022-08-08 02:23:32,684] Test  Error: Accuracy: 100.000%, Avg loss: 0.000631
[2022-08-08 02:23:32,685] Epoch 8---------------
[2022-08-08 02:23:32,686] lr: 1.137600e-03
[2022-08-08 02:23:32,748] loss: 0.000444  [    0/ 4771]
[2022-08-08 02:23:33,652] loss: 0.000373  [  480/ 4771]
[2022-08-08 02:23:34,555] loss: 0.000378  [  960/ 4771]
[2022-08-08 02:23:35,460] loss: 0.000313  [ 1440/ 4771]
[2022-08-08 02:23:36,363] loss: 0.000398  [ 1920/ 4771]
[2022-08-08 02:23:37,267] loss: 0.000393  [ 2400/ 4771]
[2022-08-08 02:23:38,171] loss: 0.000510  [ 2880/ 4771]
[2022-08-08 02:23:39,074] loss: 0.000323  [ 3360/ 4771]
[2022-08-08 02:23:39,978] loss: 0.000438  [ 3840/ 4771]
[2022-08-08 02:23:40,881] loss: 0.000383  [ 4320/ 4771]
[2022-08-08 02:23:44,387] Train Error: Accuracy: 100.000%, Avg loss: 0.000394
[2022-08-08 02:23:45,520] Test  Error: Accuracy: 100.000%, Avg loss: 0.000559
[2022-08-08 02:23:45,520] Epoch 9---------------
[2022-08-08 02:23:45,522] lr: 1.080720e-03
[2022-08-08 02:23:45,584] loss: 0.000337  [    0/ 4771]
[2022-08-08 02:23:46,487] loss: 0.000334  [  480/ 4771]
[2022-08-08 02:23:47,389] loss: 0.000325  [  960/ 4771]
[2022-08-08 02:23:48,291] loss: 0.000588  [ 1440/ 4771]
[2022-08-08 02:23:49,194] loss: 0.000418  [ 1920/ 4771]
[2022-08-08 02:23:50,097] loss: 0.000495  [ 2400/ 4771]
[2022-08-08 02:23:50,998] loss: 0.000305  [ 2880/ 4771]
[2022-08-08 02:23:51,900] loss: 0.000511  [ 3360/ 4771]
[2022-08-08 02:23:52,802] loss: 0.000312  [ 3840/ 4771]
[2022-08-08 02:23:53,705] loss: 0.000427  [ 4320/ 4771]
[2022-08-08 02:23:57,212] Train Error: Accuracy: 100.000%, Avg loss: 0.000383
[2022-08-08 02:23:58,344] Test  Error: Accuracy: 100.000%, Avg loss: 0.000564
[2022-08-08 02:23:58,344] Epoch 10---------------
[2022-08-08 02:23:58,345] lr: 9.265825e-04
[2022-08-08 02:23:58,407] loss: 0.000427  [    0/ 4771]
[2022-08-08 02:23:59,311] loss: 0.000446  [  480/ 4771]
[2022-08-08 02:24:00,215] loss: 0.000372  [  960/ 4771]
[2022-08-08 02:24:01,118] loss: 0.000485  [ 1440/ 4771]
[2022-08-08 02:24:02,021] loss: 0.000328  [ 1920/ 4771]
[2022-08-08 02:24:02,924] loss: 0.000327  [ 2400/ 4771]
[2022-08-08 02:24:03,826] loss: 0.000354  [ 2880/ 4771]
[2022-08-08 02:24:04,730] loss: 0.000292  [ 3360/ 4771]
[2022-08-08 02:24:05,633] loss: 0.000377  [ 3840/ 4771]
[2022-08-08 02:24:06,537] loss: 0.000414  [ 4320/ 4771]
[2022-08-08 02:24:10,045] Train Error: Accuracy: 100.000%, Avg loss: 0.000383
[2022-08-08 02:24:11,178] Test  Error: Accuracy: 100.000%, Avg loss: 0.000513
[2022-08-08 02:24:11,179] Epoch 11---------------
[2022-08-08 02:24:11,180] lr: 8.802533e-04
[2022-08-08 02:24:11,243] loss: 0.000378  [    0/ 4771]
[2022-08-08 02:24:12,145] loss: 0.000349  [  480/ 4771]
[2022-08-08 02:24:13,047] loss: 0.000367  [  960/ 4771]
[2022-08-08 02:24:13,949] loss: 0.000286  [ 1440/ 4771]
[2022-08-08 02:24:14,852] loss: 0.000348  [ 1920/ 4771]
[2022-08-08 02:24:15,754] loss: 0.000365  [ 2400/ 4771]
[2022-08-08 02:24:16,657] loss: 0.000322  [ 2880/ 4771]
[2022-08-08 02:24:17,560] loss: 0.000465  [ 3360/ 4771]
[2022-08-08 02:24:18,463] loss: 0.000301  [ 3840/ 4771]
[2022-08-08 02:24:19,365] loss: 0.000335  [ 4320/ 4771]
[2022-08-08 02:24:22,876] Train Error: Accuracy: 100.000%, Avg loss: 0.000365
[2022-08-08 02:24:24,008] Test  Error: Accuracy: 100.000%, Avg loss: 0.000533
[2022-08-08 02:24:24,008] Epoch 12---------------
[2022-08-08 02:24:24,009] lr: 7.547072e-04
[2022-08-08 02:24:24,072] loss: 0.000262  [    0/ 4771]
[2022-08-08 02:24:24,976] loss: 0.000229  [  480/ 4771]
[2022-08-08 02:24:25,879] loss: 0.000423  [  960/ 4771]
[2022-08-08 02:24:26,783] loss: 0.000324  [ 1440/ 4771]
[2022-08-08 02:24:27,687] loss: 0.000334  [ 1920/ 4771]
[2022-08-08 02:24:28,590] loss: 0.000298  [ 2400/ 4771]
[2022-08-08 02:24:29,495] loss: 0.000286  [ 2880/ 4771]
[2022-08-08 02:24:30,397] loss: 0.000624  [ 3360/ 4771]
[2022-08-08 02:24:31,300] loss: 0.000551  [ 3840/ 4771]
[2022-08-08 02:24:32,203] loss: 0.000289  [ 4320/ 4771]
[2022-08-08 02:24:35,708] Train Error: Accuracy: 100.000%, Avg loss: 0.000374
[2022-08-08 02:24:36,840] Test  Error: Accuracy: 100.000%, Avg loss: 0.000575
[2022-08-08 02:24:36,841] Epoch 13---------------
[2022-08-08 02:24:36,841] lr: 6.470671e-04
[2022-08-08 02:24:36,904] loss: 0.000362  [    0/ 4771]
[2022-08-08 02:24:37,806] loss: 0.000331  [  480/ 4771]
[2022-08-08 02:24:38,708] loss: 0.000311  [  960/ 4771]
[2022-08-08 02:24:39,611] loss: 0.000346  [ 1440/ 4771]
[2022-08-08 02:24:40,513] loss: 0.000377  [ 1920/ 4771]
[2022-08-08 02:24:41,416] loss: 0.000302  [ 2400/ 4771]
[2022-08-08 02:24:42,318] loss: 0.000419  [ 2880/ 4771]
[2022-08-08 02:24:43,221] loss: 0.000296  [ 3360/ 4771]
[2022-08-08 02:24:44,123] loss: 0.000233  [ 3840/ 4771]
[2022-08-08 02:24:45,027] loss: 0.000383  [ 4320/ 4771]
[2022-08-08 02:24:48,532] Train Error: Accuracy: 100.000%, Avg loss: 0.000354
[2022-08-08 02:24:49,664] Test  Error: Accuracy: 100.000%, Avg loss: 0.000518
[2022-08-08 02:24:49,665] Epoch 14---------------
[2022-08-08 02:24:49,666] lr: 6.147137e-04
[2022-08-08 02:24:49,729] loss: 0.000700  [    0/ 4771]
[2022-08-08 02:24:50,631] loss: 0.000277  [  480/ 4771]
[2022-08-08 02:24:51,535] loss: 0.000316  [  960/ 4771]
[2022-08-08 02:24:52,438] loss: 0.000493  [ 1440/ 4771]
[2022-08-08 02:24:53,341] loss: 0.000247  [ 1920/ 4771]
[2022-08-08 02:24:54,245] loss: 0.000351  [ 2400/ 4771]
[2022-08-08 02:24:55,148] loss: 0.000873  [ 2880/ 4771]
[2022-08-08 02:24:56,051] loss: 0.000426  [ 3360/ 4771]
[2022-08-08 02:24:56,954] loss: 0.000340  [ 3840/ 4771]
[2022-08-08 02:24:57,859] loss: 0.000871  [ 4320/ 4771]
[2022-08-08 02:25:01,365] Train Error: Accuracy: 100.000%, Avg loss: 0.000352
[2022-08-08 02:25:02,497] Test  Error: Accuracy: 100.000%, Avg loss: 0.000608
[2022-08-08 02:25:02,497] Epoch 15---------------
[2022-08-08 02:25:02,498] lr: 4.756538e-04
[2022-08-08 02:25:02,560] loss: 0.000364  [    0/ 4771]
[2022-08-08 02:25:03,462] loss: 0.000370  [  480/ 4771]
[2022-08-08 02:25:04,364] loss: 0.000309  [  960/ 4771]
[2022-08-08 02:25:05,267] loss: 0.000319  [ 1440/ 4771]
[2022-08-08 02:25:06,170] loss: 0.000288  [ 1920/ 4771]
[2022-08-08 02:25:07,073] loss: 0.000413  [ 2400/ 4771]
[2022-08-08 02:25:07,976] loss: 0.000331  [ 2880/ 4771]
[2022-08-08 02:25:08,877] loss: 0.000395  [ 3360/ 4771]
[2022-08-08 02:25:09,779] loss: 0.000283  [ 3840/ 4771]
[2022-08-08 02:25:10,682] loss: 0.000268  [ 4320/ 4771]
[2022-08-08 02:25:14,196] Train Error: Accuracy: 100.000%, Avg loss: 0.000340
[2022-08-08 02:25:15,328] Test  Error: Accuracy: 100.000%, Avg loss: 0.000673
[2022-08-08 02:25:15,329] Done!
[2022-08-08 02:25:15,333] Number of parameters:3229450
[2022-08-08 02:25:15,333] ## end time: 2022-08-08 02:25:15.329864
[2022-08-08 02:25:15,333] ## used time: 0:03:12.940710
