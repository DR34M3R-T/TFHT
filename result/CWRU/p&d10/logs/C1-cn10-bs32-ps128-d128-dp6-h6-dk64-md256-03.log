[2022-08-08 02:09:56,146] ## start time: 2022-08-08 02:09:56.015225
[2022-08-08 02:09:56,146] Using cuda device
[2022-08-08 02:09:56,147] In train:p&d10.npy.
[2022-08-08 02:09:56,149] One Channel
[2022-08-08 02:09:56,149] With Normal data.
[2022-08-08 02:09:56,149] Nunber of classes:10.
[2022-08-08 02:09:56,150] Nunber of ViT channels:1.
[2022-08-08 02:09:56,384] Totol epochs: 15
[2022-08-08 02:09:56,386] Epoch 1---------------
[2022-08-08 02:09:56,386] lr: 2.000000e-03
[2022-08-08 02:09:56,488] loss: 2.419948  [    0/ 4830]
[2022-08-08 02:09:57,993] loss: 2.220317  [  480/ 4830]
[2022-08-08 02:09:59,498] loss: 1.726801  [  960/ 4830]
[2022-08-08 02:10:01,004] loss: 1.123322  [ 1440/ 4830]
[2022-08-08 02:10:02,512] loss: 0.521271  [ 1920/ 4830]
[2022-08-08 02:10:04,017] loss: 0.383795  [ 2400/ 4830]
[2022-08-08 02:10:05,523] loss: 0.032723  [ 2880/ 4830]
[2022-08-08 02:10:07,031] loss: 0.086627  [ 3360/ 4830]
[2022-08-08 02:10:08,537] loss: 0.164175  [ 3840/ 4830]
[2022-08-08 02:10:10,045] loss: 0.130550  [ 4320/ 4830]
[2022-08-08 02:10:11,549] loss: 0.021970  [ 4800/ 4830]
[2022-08-08 02:10:16,889] Train Error: Accuracy: 99.379%, Avg loss: 0.033677
[2022-08-08 02:10:19,052] Test  Error: Accuracy: 99.283%, Avg loss: 0.037659
[2022-08-08 02:10:19,053] Epoch 2---------------
[2022-08-08 02:10:19,055] lr: 1.900000e-03
[2022-08-08 02:10:19,157] loss: 0.014830  [    0/ 4830]
[2022-08-08 02:10:20,664] loss: 0.021977  [  480/ 4830]
[2022-08-08 02:10:22,173] loss: 0.005687  [  960/ 4830]
[2022-08-08 02:10:23,677] loss: 0.007639  [ 1440/ 4830]
[2022-08-08 02:10:25,186] loss: 0.004147  [ 1920/ 4830]
[2022-08-08 02:10:26,692] loss: 0.004251  [ 2400/ 4830]
[2022-08-08 02:10:28,196] loss: 0.004351  [ 2880/ 4830]
[2022-08-08 02:10:29,702] loss: 0.007513  [ 3360/ 4830]
[2022-08-08 02:10:31,208] loss: 0.180704  [ 3840/ 4830]
[2022-08-08 02:10:32,717] loss: 0.072106  [ 4320/ 4830]
[2022-08-08 02:10:34,221] loss: 0.012351  [ 4800/ 4830]
[2022-08-08 02:10:39,551] Train Error: Accuracy: 99.959%, Avg loss: 0.012365
[2022-08-08 02:10:41,714] Test  Error: Accuracy: 100.000%, Avg loss: 0.012467
[2022-08-08 02:10:41,714] Epoch 3---------------
[2022-08-08 02:10:41,715] lr: 1.805000e-03
[2022-08-08 02:10:41,818] loss: 0.016241  [    0/ 4830]
[2022-08-08 02:10:43,327] loss: 0.008759  [  480/ 4830]
[2022-08-08 02:10:44,833] loss: 0.011750  [  960/ 4830]
[2022-08-08 02:10:46,340] loss: 0.005833  [ 1440/ 4830]
[2022-08-08 02:10:47,848] loss: 0.002961  [ 1920/ 4830]
[2022-08-08 02:10:49,357] loss: 0.002223  [ 2400/ 4830]
[2022-08-08 02:10:50,864] loss: 0.004800  [ 2880/ 4830]
[2022-08-08 02:10:52,371] loss: 3.240082  [ 3360/ 4830]
[2022-08-08 02:10:53,877] loss: 0.062497  [ 3840/ 4830]
[2022-08-08 02:10:55,387] loss: 0.102414  [ 4320/ 4830]
[2022-08-08 02:10:56,893] loss: 0.144973  [ 4800/ 4830]
[2022-08-08 02:11:02,209] Train Error: Accuracy: 99.234%, Avg loss: 0.042802
[2022-08-08 02:11:04,371] Test  Error: Accuracy: 98.720%, Avg loss: 0.050919
[2022-08-08 02:11:04,371] Epoch 4---------------
[2022-08-08 02:11:04,372] lr: 1.260499e-03
[2022-08-08 02:11:04,475] loss: 0.043647  [    0/ 4830]
[2022-08-08 02:11:05,983] loss: 0.013016  [  480/ 4830]
[2022-08-08 02:11:07,490] loss: 0.019576  [  960/ 4830]
[2022-08-08 02:11:08,997] loss: 0.006052  [ 1440/ 4830]
[2022-08-08 02:11:10,506] loss: 0.015637  [ 1920/ 4830]
[2022-08-08 02:11:12,012] loss: 0.005791  [ 2400/ 4830]
[2022-08-08 02:11:13,516] loss: 0.023616  [ 2880/ 4830]
[2022-08-08 02:11:15,024] loss: 0.002953  [ 3360/ 4830]
[2022-08-08 02:11:16,529] loss: 0.002561  [ 3840/ 4830]
[2022-08-08 02:11:18,031] loss: 0.029465  [ 4320/ 4830]
[2022-08-08 02:11:19,534] loss: 0.001535  [ 4800/ 4830]
[2022-08-08 02:11:24,861] Train Error: Accuracy: 99.979%, Avg loss: 0.003195
[2022-08-08 02:11:27,021] Test  Error: Accuracy: 99.898%, Avg loss: 0.005140
[2022-08-08 02:11:27,021] Epoch 5---------------
[2022-08-08 02:11:27,023] lr: 1.197474e-03
[2022-08-08 02:11:27,124] loss: 0.001505  [    0/ 4830]
[2022-08-08 02:11:28,632] loss: 0.001393  [  480/ 4830]
[2022-08-08 02:11:30,137] loss: 0.001243  [  960/ 4830]
[2022-08-08 02:11:31,644] loss: 0.004053  [ 1440/ 4830]
[2022-08-08 02:11:33,150] loss: 0.007043  [ 1920/ 4830]
[2022-08-08 02:11:34,657] loss: 0.001558  [ 2400/ 4830]
[2022-08-08 02:11:36,163] loss: 0.001085  [ 2880/ 4830]
[2022-08-08 02:11:37,669] loss: 0.003027  [ 3360/ 4830]
[2022-08-08 02:11:39,177] loss: 0.025293  [ 3840/ 4830]
[2022-08-08 02:11:40,686] loss: 0.001233  [ 4320/ 4830]
[2022-08-08 02:11:42,189] loss: 0.007827  [ 4800/ 4830]
[2022-08-08 02:11:47,503] Train Error: Accuracy: 100.000%, Avg loss: 0.001928
[2022-08-08 02:11:49,670] Test  Error: Accuracy: 99.898%, Avg loss: 0.003355
[2022-08-08 02:11:49,671] Epoch 6---------------
[2022-08-08 02:11:49,672] lr: 1.137600e-03
[2022-08-08 02:11:49,774] loss: 0.001361  [    0/ 4830]
[2022-08-08 02:11:51,278] loss: 0.000956  [  480/ 4830]
[2022-08-08 02:11:52,784] loss: 0.000671  [  960/ 4830]
[2022-08-08 02:11:54,290] loss: 0.000986  [ 1440/ 4830]
[2022-08-08 02:11:55,796] loss: 0.000789  [ 1920/ 4830]
[2022-08-08 02:11:57,305] loss: 0.002647  [ 2400/ 4830]
[2022-08-08 02:11:58,810] loss: 0.000767  [ 2880/ 4830]
[2022-08-08 02:12:00,318] loss: 0.000581  [ 3360/ 4830]
[2022-08-08 02:12:01,823] loss: 0.000594  [ 3840/ 4830]
[2022-08-08 02:12:03,330] loss: 0.000704  [ 4320/ 4830]
[2022-08-08 02:12:04,833] loss: 0.000512  [ 4800/ 4830]
[2022-08-08 02:12:10,159] Train Error: Accuracy: 100.000%, Avg loss: 0.000909
[2022-08-08 02:12:12,315] Test  Error: Accuracy: 99.949%, Avg loss: 0.001669
[2022-08-08 02:12:12,315] Epoch 7---------------
[2022-08-08 02:12:12,316] lr: 1.080720e-03
[2022-08-08 02:12:12,419] loss: 0.001281  [    0/ 4830]
[2022-08-08 02:12:13,921] loss: 0.000608  [  480/ 4830]
[2022-08-08 02:12:15,429] loss: 0.000907  [  960/ 4830]
[2022-08-08 02:12:16,935] loss: 0.000523  [ 1440/ 4830]
[2022-08-08 02:12:18,439] loss: 0.002073  [ 1920/ 4830]
[2022-08-08 02:12:19,945] loss: 0.000530  [ 2400/ 4830]
[2022-08-08 02:12:21,449] loss: 0.000523  [ 2880/ 4830]
[2022-08-08 02:12:22,958] loss: 0.000498  [ 3360/ 4830]
[2022-08-08 02:12:24,466] loss: 0.000490  [ 3840/ 4830]
[2022-08-08 02:12:25,971] loss: 0.000458  [ 4320/ 4830]
[2022-08-08 02:12:27,475] loss: 0.000896  [ 4800/ 4830]
[2022-08-08 02:12:32,781] Train Error: Accuracy: 100.000%, Avg loss: 0.001285
[2022-08-08 02:12:34,930] Test  Error: Accuracy: 99.949%, Avg loss: 0.002543
[2022-08-08 02:12:34,931] Epoch 8---------------
[2022-08-08 02:12:34,932] lr: 7.547072e-04
[2022-08-08 02:12:35,034] loss: 0.000895  [    0/ 4830]
[2022-08-08 02:12:36,542] loss: 0.001163  [  480/ 4830]
[2022-08-08 02:12:38,049] loss: 0.000559  [  960/ 4830]
[2022-08-08 02:12:39,558] loss: 0.000598  [ 1440/ 4830]
[2022-08-08 02:12:41,064] loss: 0.000708  [ 1920/ 4830]
[2022-08-08 02:12:42,572] loss: 0.000643  [ 2400/ 4830]
[2022-08-08 02:12:44,080] loss: 0.000665  [ 2880/ 4830]
[2022-08-08 02:12:45,585] loss: 0.001063  [ 3360/ 4830]
[2022-08-08 02:12:47,093] loss: 0.000562  [ 3840/ 4830]
[2022-08-08 02:12:48,600] loss: 0.000752  [ 4320/ 4830]
[2022-08-08 02:12:50,102] loss: 0.000599  [ 4800/ 4830]
[2022-08-08 02:12:55,407] Train Error: Accuracy: 100.000%, Avg loss: 0.000608
[2022-08-08 02:12:57,561] Test  Error: Accuracy: 100.000%, Avg loss: 0.001313
[2022-08-08 02:12:57,562] Epoch 9---------------
[2022-08-08 02:12:57,563] lr: 7.169718e-04
[2022-08-08 02:12:57,665] loss: 0.000815  [    0/ 4830]
[2022-08-08 02:12:59,175] loss: 0.000507  [  480/ 4830]
[2022-08-08 02:13:00,681] loss: 0.000781  [  960/ 4830]
[2022-08-08 02:13:02,188] loss: 0.002470  [ 1440/ 4830]
[2022-08-08 02:13:03,693] loss: 0.000298  [ 1920/ 4830]
[2022-08-08 02:13:05,199] loss: 0.000568  [ 2400/ 4830]
[2022-08-08 02:13:06,706] loss: 0.000385  [ 2880/ 4830]
[2022-08-08 02:13:08,211] loss: 0.000315  [ 3360/ 4830]
[2022-08-08 02:13:09,716] loss: 0.000815  [ 3840/ 4830]
[2022-08-08 02:13:11,223] loss: 0.000408  [ 4320/ 4830]
[2022-08-08 02:13:12,725] loss: 0.000639  [ 4800/ 4830]
[2022-08-08 02:13:18,027] Train Error: Accuracy: 100.000%, Avg loss: 0.000510
[2022-08-08 02:13:20,177] Test  Error: Accuracy: 99.949%, Avg loss: 0.001354
[2022-08-08 02:13:20,178] Epoch 10---------------
[2022-08-08 02:13:20,178] lr: 6.147137e-04
[2022-08-08 02:13:20,281] loss: 0.000414  [    0/ 4830]
[2022-08-08 02:13:21,785] loss: 0.000404  [  480/ 4830]
[2022-08-08 02:13:23,292] loss: 0.000375  [  960/ 4830]
[2022-08-08 02:13:24,796] loss: 0.000497  [ 1440/ 4830]
[2022-08-08 02:13:26,302] loss: 0.000557  [ 1920/ 4830]
[2022-08-08 02:13:27,809] loss: 0.000475  [ 2400/ 4830]
[2022-08-08 02:13:29,317] loss: 0.000273  [ 2880/ 4830]
[2022-08-08 02:13:30,824] loss: 0.000410  [ 3360/ 4830]
[2022-08-08 02:13:32,332] loss: 0.000940  [ 3840/ 4830]
[2022-08-08 02:13:33,835] loss: 0.000759  [ 4320/ 4830]
[2022-08-08 02:13:35,336] loss: 0.000535  [ 4800/ 4830]
[2022-08-08 02:13:40,629] Train Error: Accuracy: 100.000%, Avg loss: 0.000556
[2022-08-08 02:13:42,780] Test  Error: Accuracy: 99.949%, Avg loss: 0.001262
[2022-08-08 02:13:42,780] Epoch 11---------------
[2022-08-08 02:13:42,781] lr: 5.839780e-04
[2022-08-08 02:13:42,883] loss: 0.000315  [    0/ 4830]
[2022-08-08 02:13:44,394] loss: 0.000581  [  480/ 4830]
[2022-08-08 02:13:45,902] loss: 0.000397  [  960/ 4830]
[2022-08-08 02:13:47,406] loss: 0.000275  [ 1440/ 4830]
[2022-08-08 02:13:48,913] loss: 0.000463  [ 1920/ 4830]
[2022-08-08 02:13:50,418] loss: 0.000282  [ 2400/ 4830]
[2022-08-08 02:13:51,923] loss: 0.000504  [ 2880/ 4830]
[2022-08-08 02:13:53,432] loss: 0.000287  [ 3360/ 4830]
[2022-08-08 02:13:54,943] loss: 0.000282  [ 3840/ 4830]
[2022-08-08 02:13:56,449] loss: 0.000425  [ 4320/ 4830]
[2022-08-08 02:13:57,953] loss: 0.000447  [ 4800/ 4830]
[2022-08-08 02:14:03,255] Train Error: Accuracy: 100.000%, Avg loss: 0.000446
[2022-08-08 02:14:05,406] Test  Error: Accuracy: 99.949%, Avg loss: 0.001339
[2022-08-08 02:14:05,407] Epoch 12---------------
[2022-08-08 02:14:05,408] lr: 5.006882e-04
[2022-08-08 02:14:05,510] loss: 0.000398  [    0/ 4830]
[2022-08-08 02:14:07,016] loss: 0.000452  [  480/ 4830]
[2022-08-08 02:14:08,521] loss: 0.000323  [  960/ 4830]
[2022-08-08 02:14:10,022] loss: 0.000389  [ 1440/ 4830]
[2022-08-08 02:14:11,530] loss: 0.000447  [ 1920/ 4830]
[2022-08-08 02:14:13,039] loss: 0.000440  [ 2400/ 4830]
[2022-08-08 02:14:14,543] loss: 0.000327  [ 2880/ 4830]
[2022-08-08 02:14:16,048] loss: 0.000630  [ 3360/ 4830]
[2022-08-08 02:14:17,555] loss: 0.000479  [ 3840/ 4830]
[2022-08-08 02:14:19,061] loss: 0.000326  [ 4320/ 4830]
[2022-08-08 02:14:20,567] loss: 0.000272  [ 4800/ 4830]
[2022-08-08 02:14:25,883] Train Error: Accuracy: 100.000%, Avg loss: 0.000425
[2022-08-08 02:14:28,036] Test  Error: Accuracy: 100.000%, Avg loss: 0.001011
[2022-08-08 02:14:28,036] Epoch 13---------------
[2022-08-08 02:14:28,037] lr: 4.756538e-04
[2022-08-08 02:14:28,140] loss: 0.000397  [    0/ 4830]
[2022-08-08 02:14:29,652] loss: 0.000272  [  480/ 4830]
[2022-08-08 02:14:31,160] loss: 0.000442  [  960/ 4830]
[2022-08-08 02:14:32,667] loss: 0.000351  [ 1440/ 4830]
[2022-08-08 02:14:34,171] loss: 0.000530  [ 1920/ 4830]
[2022-08-08 02:14:35,675] loss: 0.000299  [ 2400/ 4830]
[2022-08-08 02:14:37,182] loss: 0.000344  [ 2880/ 4830]
[2022-08-08 02:14:38,691] loss: 0.000410  [ 3360/ 4830]
[2022-08-08 02:14:40,196] loss: 0.000280  [ 3840/ 4830]
[2022-08-08 02:14:41,701] loss: 0.000254  [ 4320/ 4830]
[2022-08-08 02:14:43,201] loss: 0.000421  [ 4800/ 4830]
[2022-08-08 02:14:48,506] Train Error: Accuracy: 100.000%, Avg loss: 0.000416
[2022-08-08 02:14:50,662] Test  Error: Accuracy: 99.898%, Avg loss: 0.002997
[2022-08-08 02:14:50,662] Epoch 14---------------
[2022-08-08 02:14:50,663] lr: 3.321668e-04
[2022-08-08 02:14:50,766] loss: 0.000438  [    0/ 4830]
[2022-08-08 02:14:52,269] loss: 0.000324  [  480/ 4830]
[2022-08-08 02:14:53,777] loss: 0.000557  [  960/ 4830]
[2022-08-08 02:14:55,281] loss: 0.000402  [ 1440/ 4830]
[2022-08-08 02:14:56,790] loss: 0.000403  [ 1920/ 4830]
[2022-08-08 02:14:58,300] loss: 0.000369  [ 2400/ 4830]
[2022-08-08 02:14:59,806] loss: 0.000283  [ 2880/ 4830]
[2022-08-08 02:15:01,313] loss: 0.000313  [ 3360/ 4830]
[2022-08-08 02:15:02,816] loss: 0.000292  [ 3840/ 4830]
[2022-08-08 02:15:04,323] loss: 0.000321  [ 4320/ 4830]
[2022-08-08 02:15:05,829] loss: 0.000434  [ 4800/ 4830]
[2022-08-08 02:15:11,140] Train Error: Accuracy: 100.000%, Avg loss: 0.000426
[2022-08-08 02:15:13,296] Test  Error: Accuracy: 100.000%, Avg loss: 0.000946
[2022-08-08 02:15:13,297] Epoch 15---------------
[2022-08-08 02:15:13,298] lr: 3.155584e-04
[2022-08-08 02:15:13,399] loss: 0.000296  [    0/ 4830]
[2022-08-08 02:15:14,909] loss: 0.000335  [  480/ 4830]
[2022-08-08 02:15:16,411] loss: 0.000381  [  960/ 4830]
[2022-08-08 02:15:17,918] loss: 0.000415  [ 1440/ 4830]
[2022-08-08 02:15:19,425] loss: 0.000571  [ 1920/ 4830]
[2022-08-08 02:15:20,935] loss: 0.000404  [ 2400/ 4830]
[2022-08-08 02:15:22,439] loss: 0.000342  [ 2880/ 4830]
[2022-08-08 02:15:23,946] loss: 0.001116  [ 3360/ 4830]
[2022-08-08 02:15:25,452] loss: 0.000647  [ 3840/ 4830]
[2022-08-08 02:15:26,961] loss: 0.000206  [ 4320/ 4830]
[2022-08-08 02:15:28,465] loss: 0.000256  [ 4800/ 4830]
[2022-08-08 02:15:33,774] Train Error: Accuracy: 100.000%, Avg loss: 0.000399
[2022-08-08 02:15:35,927] Test  Error: Accuracy: 99.949%, Avg loss: 0.001392
[2022-08-08 02:15:35,928] Done!
[2022-08-08 02:15:35,932] Number of parameters:3198730
[2022-08-08 02:15:35,932] ## end time: 2022-08-08 02:15:35.928813
[2022-08-08 02:15:35,933] ## used time: 0:05:39.913588
