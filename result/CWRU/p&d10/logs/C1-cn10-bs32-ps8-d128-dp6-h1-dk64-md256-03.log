[2022-08-08 15:54:31,454] ## start time: 2022-08-08 15:54:31.326491
[2022-08-08 15:54:31,454] Using cuda device
[2022-08-08 15:54:31,455] In train:p&d10.npy.
[2022-08-08 15:54:31,457] One Channel
[2022-08-08 15:54:31,457] With Normal data.
[2022-08-08 15:54:31,458] Nunber of classes:10.
[2022-08-08 15:54:31,458] Nunber of ViT channels:1.
[2022-08-08 15:54:31,680] Totol epochs: 15
[2022-08-08 15:54:31,683] Epoch 1---------------
[2022-08-08 15:54:31,683] lr: 2.000000e-03
[2022-08-08 15:54:32,242] loss: 2.378734  [    0/ 4732]
[2022-08-08 15:54:40,604] loss: 2.123214  [  480/ 4732]
[2022-08-08 15:54:48,966] loss: 1.852042  [  960/ 4732]
[2022-08-08 15:54:57,326] loss: 1.624543  [ 1440/ 4732]
[2022-08-08 15:55:05,688] loss: 1.279766  [ 1920/ 4732]
[2022-08-08 15:55:14,049] loss: 0.980842  [ 2400/ 4732]
[2022-08-08 15:55:22,412] loss: 1.165320  [ 2880/ 4732]
[2022-08-08 15:55:30,773] loss: 2.196089  [ 3360/ 4732]
[2022-08-08 15:55:39,136] loss: 1.263727  [ 3840/ 4732]
[2022-08-08 15:55:47,499] loss: 0.911398  [ 4320/ 4732]
[2022-08-08 15:56:26,007] Train Error: Accuracy: 64.772%, Avg loss: 0.892717
[2022-08-08 15:56:39,840] Test  Error: Accuracy: 62.847%, Avg loss: 0.927456
[2022-08-08 15:56:39,840] Epoch 2---------------
[2022-08-08 15:56:39,841] lr: 1.900000e-03
[2022-08-08 15:56:40,401] loss: 0.830478  [    0/ 4732]
[2022-08-08 15:56:48,763] loss: 0.675340  [  480/ 4732]
[2022-08-08 15:56:57,177] loss: 0.976794  [  960/ 4732]
[2022-08-08 15:57:05,594] loss: 0.518940  [ 1440/ 4732]
[2022-08-08 15:57:14,012] loss: 1.038536  [ 1920/ 4732]
[2022-08-08 15:57:22,430] loss: 0.706589  [ 2400/ 4732]
[2022-08-08 15:57:30,848] loss: 1.155853  [ 2880/ 4732]
[2022-08-08 15:57:39,266] loss: 0.649999  [ 3360/ 4732]
[2022-08-08 15:57:47,684] loss: 0.712471  [ 3840/ 4732]
[2022-08-08 15:57:56,101] loss: 0.863635  [ 4320/ 4732]
[2022-08-08 15:58:34,866] Train Error: Accuracy: 75.676%, Avg loss: 0.687590
[2022-08-08 15:58:48,791] Test  Error: Accuracy: 74.988%, Avg loss: 0.712726
[2022-08-08 15:58:48,791] Epoch 3---------------
[2022-08-08 15:58:48,792] lr: 1.805000e-03
[2022-08-08 15:58:49,356] loss: 1.036975  [    0/ 4732]
[2022-08-08 15:58:57,775] loss: 0.514076  [  480/ 4732]
[2022-08-08 15:59:06,195] loss: 0.567208  [  960/ 4732]
[2022-08-08 15:59:14,613] loss: 0.672968  [ 1440/ 4732]
[2022-08-08 15:59:23,032] loss: 0.426187  [ 1920/ 4732]
[2022-08-08 15:59:31,450] loss: 0.808254  [ 2400/ 4732]
[2022-08-08 15:59:39,869] loss: 0.258344  [ 2880/ 4732]
[2022-08-08 15:59:48,289] loss: 0.265525  [ 3360/ 4732]
[2022-08-08 15:59:56,709] loss: 0.803994  [ 3840/ 4732]
[2022-08-08 16:00:05,128] loss: 0.481066  [ 4320/ 4732]
[2022-08-08 16:00:43,897] Train Error: Accuracy: 77.747%, Avg loss: 0.614306
[2022-08-08 16:00:57,826] Test  Error: Accuracy: 76.694%, Avg loss: 0.662067
[2022-08-08 16:00:57,826] Epoch 4---------------
[2022-08-08 16:00:57,829] lr: 1.714750e-03
[2022-08-08 16:00:58,394] loss: 0.750150  [    0/ 4732]
[2022-08-08 16:01:06,811] loss: 0.384872  [  480/ 4732]
[2022-08-08 16:01:15,229] loss: 0.293348  [  960/ 4732]
[2022-08-08 16:01:23,649] loss: 0.243376  [ 1440/ 4732]
[2022-08-08 16:01:32,067] loss: 0.492580  [ 1920/ 4732]
[2022-08-08 16:01:40,485] loss: 0.238578  [ 2400/ 4732]
[2022-08-08 16:01:48,904] loss: 0.672462  [ 2880/ 4732]
[2022-08-08 16:01:57,323] loss: 0.300159  [ 3360/ 4732]
[2022-08-08 16:02:05,742] loss: 0.315981  [ 3840/ 4732]
[2022-08-08 16:02:14,160] loss: 0.129589  [ 4320/ 4732]
[2022-08-08 16:02:52,938] Train Error: Accuracy: 89.560%, Avg loss: 0.324043
[2022-08-08 16:03:06,867] Test  Error: Accuracy: 87.031%, Avg loss: 0.402480
[2022-08-08 16:03:06,867] Epoch 5---------------
[2022-08-08 16:03:06,869] lr: 1.629012e-03
[2022-08-08 16:03:07,431] loss: 0.243240  [    0/ 4732]
[2022-08-08 16:03:15,850] loss: 0.297538  [  480/ 4732]
[2022-08-08 16:03:24,269] loss: 0.397237  [  960/ 4732]
[2022-08-08 16:03:32,687] loss: 0.397478  [ 1440/ 4732]
[2022-08-08 16:03:41,104] loss: 0.595072  [ 1920/ 4732]
[2022-08-08 16:03:49,521] loss: 0.389147  [ 2400/ 4732]
[2022-08-08 16:03:57,939] loss: 0.319491  [ 2880/ 4732]
[2022-08-08 16:04:06,358] loss: 0.182053  [ 3360/ 4732]
[2022-08-08 16:04:14,777] loss: 0.252941  [ 3840/ 4732]
[2022-08-08 16:04:23,195] loss: 0.529266  [ 4320/ 4732]
[2022-08-08 16:05:01,964] Train Error: Accuracy: 87.828%, Avg loss: 0.354703
[2022-08-08 16:05:15,887] Test  Error: Accuracy: 84.788%, Avg loss: 0.435970
[2022-08-08 16:05:15,888] Epoch 6---------------
[2022-08-08 16:05:15,889] lr: 1.396675e-03
[2022-08-08 16:05:16,453] loss: 0.439256  [    0/ 4732]
[2022-08-08 16:05:24,870] loss: 0.241473  [  480/ 4732]
[2022-08-08 16:05:33,288] loss: 0.187975  [  960/ 4732]
[2022-08-08 16:05:41,707] loss: 0.115321  [ 1440/ 4732]
[2022-08-08 16:05:50,126] loss: 0.253548  [ 1920/ 4732]
[2022-08-08 16:05:58,545] loss: 0.281902  [ 2400/ 4732]
[2022-08-08 16:06:06,962] loss: 0.117730  [ 2880/ 4732]
[2022-08-08 16:06:15,382] loss: 0.351436  [ 3360/ 4732]
[2022-08-08 16:06:23,800] loss: 0.255884  [ 3840/ 4732]
[2022-08-08 16:06:32,219] loss: 0.168197  [ 4320/ 4732]
[2022-08-08 16:07:10,982] Train Error: Accuracy: 88.123%, Avg loss: 0.514977
[2022-08-08 16:07:24,909] Test  Error: Accuracy: 87.665%, Avg loss: 0.571387
[2022-08-08 16:07:24,909] Epoch 7---------------
[2022-08-08 16:07:24,910] lr: 9.753500e-04
[2022-08-08 16:07:25,474] loss: 0.455866  [    0/ 4732]
[2022-08-08 16:07:33,892] loss: 0.025895  [  480/ 4732]
[2022-08-08 16:07:42,311] loss: 0.106332  [  960/ 4732]
[2022-08-08 16:07:50,729] loss: 0.040288  [ 1440/ 4732]
[2022-08-08 16:07:59,147] loss: 0.085350  [ 1920/ 4732]
[2022-08-08 16:08:07,565] loss: 0.077441  [ 2400/ 4732]
[2022-08-08 16:08:15,984] loss: 0.193282  [ 2880/ 4732]
[2022-08-08 16:08:24,401] loss: 0.069225  [ 3360/ 4732]
[2022-08-08 16:08:32,820] loss: 0.099842  [ 3840/ 4732]
[2022-08-08 16:08:41,237] loss: 0.087428  [ 4320/ 4732]
[2022-08-08 16:09:19,998] Train Error: Accuracy: 95.287%, Avg loss: 0.142360
[2022-08-08 16:09:33,920] Test  Error: Accuracy: 93.954%, Avg loss: 0.168132
[2022-08-08 16:09:33,921] Epoch 8---------------
[2022-08-08 16:09:33,922] lr: 9.265825e-04
[2022-08-08 16:09:34,485] loss: 0.256026  [    0/ 4732]
[2022-08-08 16:09:42,903] loss: 0.059487  [  480/ 4732]
[2022-08-08 16:09:51,321] loss: 0.198951  [  960/ 4732]
[2022-08-08 16:09:59,741] loss: 0.236358  [ 1440/ 4732]
[2022-08-08 16:10:08,159] loss: 0.216432  [ 1920/ 4732]
[2022-08-08 16:10:16,577] loss: 0.128537  [ 2400/ 4732]
[2022-08-08 16:10:24,996] loss: 0.046066  [ 2880/ 4732]
[2022-08-08 16:10:33,414] loss: 0.105897  [ 3360/ 4732]
[2022-08-08 16:10:41,832] loss: 0.083314  [ 3840/ 4732]
[2022-08-08 16:10:50,250] loss: 0.132176  [ 4320/ 4732]
[2022-08-08 16:11:29,009] Train Error: Accuracy: 94.992%, Avg loss: 0.138674
[2022-08-08 16:11:42,929] Test  Error: Accuracy: 93.857%, Avg loss: 0.181586
[2022-08-08 16:11:42,930] Epoch 9---------------
[2022-08-08 16:11:42,930] lr: 7.944286e-04
[2022-08-08 16:11:43,494] loss: 0.342349  [    0/ 4732]
[2022-08-08 16:11:51,913] loss: 0.062167  [  480/ 4732]
[2022-08-08 16:12:00,331] loss: 0.183298  [  960/ 4732]
[2022-08-08 16:12:08,748] loss: 0.075783  [ 1440/ 4732]
[2022-08-08 16:12:17,167] loss: 0.270706  [ 1920/ 4732]
[2022-08-08 16:12:25,583] loss: 0.015739  [ 2400/ 4732]
[2022-08-08 16:12:34,000] loss: 0.309315  [ 2880/ 4732]
[2022-08-08 16:12:42,419] loss: 0.071431  [ 3360/ 4732]
[2022-08-08 16:12:50,837] loss: 0.096304  [ 3840/ 4732]
[2022-08-08 16:12:59,254] loss: 0.052853  [ 4320/ 4732]
[2022-08-08 16:13:38,007] Train Error: Accuracy: 96.577%, Avg loss: 0.094240
[2022-08-08 16:13:51,924] Test  Error: Accuracy: 95.661%, Avg loss: 0.133965
[2022-08-08 16:13:51,924] Epoch 10---------------
[2022-08-08 16:13:51,927] lr: 7.547072e-04
[2022-08-08 16:13:52,489] loss: 0.119614  [    0/ 4732]
[2022-08-08 16:14:00,908] loss: 0.096184  [  480/ 4732]
[2022-08-08 16:14:09,326] loss: 0.144807  [  960/ 4732]
[2022-08-08 16:14:17,744] loss: 0.181146  [ 1440/ 4732]
[2022-08-08 16:14:26,161] loss: 0.038083  [ 1920/ 4732]
[2022-08-08 16:14:34,579] loss: 0.007813  [ 2400/ 4732]
[2022-08-08 16:14:42,998] loss: 0.168401  [ 2880/ 4732]
[2022-08-08 16:14:51,417] loss: 0.026725  [ 3360/ 4732]
[2022-08-08 16:14:59,835] loss: 0.026278  [ 3840/ 4732]
[2022-08-08 16:15:08,253] loss: 0.429839  [ 4320/ 4732]
[2022-08-08 16:15:47,009] Train Error: Accuracy: 98.521%, Avg loss: 0.050845
[2022-08-08 16:16:00,931] Test  Error: Accuracy: 97.123%, Avg loss: 0.090221
[2022-08-08 16:16:00,931] Epoch 11---------------
[2022-08-08 16:16:00,932] lr: 7.169718e-04
[2022-08-08 16:16:01,496] loss: 0.016430  [    0/ 4732]
[2022-08-08 16:16:09,914] loss: 0.043918  [  480/ 4732]
[2022-08-08 16:16:18,332] loss: 0.112502  [  960/ 4732]
[2022-08-08 16:16:26,749] loss: 0.094051  [ 1440/ 4732]
[2022-08-08 16:16:35,167] loss: 0.034888  [ 1920/ 4732]
[2022-08-08 16:16:43,584] loss: 0.048696  [ 2400/ 4732]
[2022-08-08 16:16:52,003] loss: 0.026869  [ 2880/ 4732]
[2022-08-08 16:17:00,421] loss: 0.010271  [ 3360/ 4732]
[2022-08-08 16:17:08,840] loss: 0.126110  [ 3840/ 4732]
[2022-08-08 16:17:17,258] loss: 0.028885  [ 4320/ 4732]
[2022-08-08 16:17:56,012] Train Error: Accuracy: 97.739%, Avg loss: 0.068799
[2022-08-08 16:18:09,934] Test  Error: Accuracy: 96.051%, Avg loss: 0.118982
[2022-08-08 16:18:09,934] Epoch 12---------------
[2022-08-08 16:18:09,935] lr: 5.006882e-04
[2022-08-08 16:18:10,499] loss: 0.026383  [    0/ 4732]
[2022-08-08 16:18:18,917] loss: 0.145012  [  480/ 4732]
[2022-08-08 16:18:27,336] loss: 0.037210  [  960/ 4732]
[2022-08-08 16:18:35,753] loss: 0.027840  [ 1440/ 4732]
[2022-08-08 16:18:44,173] loss: 0.085511  [ 1920/ 4732]
[2022-08-08 16:18:52,591] loss: 0.221514  [ 2400/ 4732]
[2022-08-08 16:19:01,008] loss: 0.105003  [ 2880/ 4732]
[2022-08-08 16:19:09,426] loss: 0.052972  [ 3360/ 4732]
[2022-08-08 16:19:17,844] loss: 0.089304  [ 3840/ 4732]
[2022-08-08 16:19:26,262] loss: 0.036680  [ 4320/ 4732]
[2022-08-08 16:20:05,019] Train Error: Accuracy: 98.521%, Avg loss: 0.050636
[2022-08-08 16:20:18,943] Test  Error: Accuracy: 97.660%, Avg loss: 0.078436
[2022-08-08 16:20:18,943] Epoch 13---------------
[2022-08-08 16:20:18,944] lr: 4.756538e-04
[2022-08-08 16:20:19,508] loss: 0.045288  [    0/ 4732]
[2022-08-08 16:20:27,927] loss: 0.011330  [  480/ 4732]
[2022-08-08 16:20:36,346] loss: 0.133357  [  960/ 4732]
[2022-08-08 16:20:44,765] loss: 0.027893  [ 1440/ 4732]
[2022-08-08 16:20:53,182] loss: 0.006776  [ 1920/ 4732]
[2022-08-08 16:21:01,601] loss: 0.025750  [ 2400/ 4732]
[2022-08-08 16:21:10,018] loss: 0.020079  [ 2880/ 4732]
[2022-08-08 16:21:18,434] loss: 0.031469  [ 3360/ 4732]
[2022-08-08 16:21:26,852] loss: 0.004256  [ 3840/ 4732]
[2022-08-08 16:21:35,269] loss: 0.007748  [ 4320/ 4732]
[2022-08-08 16:22:14,020] Train Error: Accuracy: 98.457%, Avg loss: 0.047633
[2022-08-08 16:22:27,940] Test  Error: Accuracy: 96.977%, Avg loss: 0.085638
[2022-08-08 16:22:27,941] Epoch 14---------------
[2022-08-08 16:22:27,942] lr: 4.078137e-04
[2022-08-08 16:22:28,505] loss: 0.061984  [    0/ 4732]
[2022-08-08 16:22:36,924] loss: 0.022756  [  480/ 4732]
[2022-08-08 16:22:45,340] loss: 0.036810  [  960/ 4732]
[2022-08-08 16:22:53,759] loss: 0.061970  [ 1440/ 4732]
[2022-08-08 16:23:02,177] loss: 0.037640  [ 1920/ 4732]
[2022-08-08 16:23:10,595] loss: 0.071483  [ 2400/ 4732]
[2022-08-08 16:23:19,013] loss: 0.016301  [ 2880/ 4732]
[2022-08-08 16:23:27,430] loss: 0.091430  [ 3360/ 4732]
[2022-08-08 16:23:35,849] loss: 0.040299  [ 3840/ 4732]
[2022-08-08 16:23:44,266] loss: 0.005728  [ 4320/ 4732]
[2022-08-08 16:24:23,015] Train Error: Accuracy: 99.028%, Avg loss: 0.032973
[2022-08-08 16:24:36,935] Test  Error: Accuracy: 97.757%, Avg loss: 0.066220
[2022-08-08 16:24:36,936] Epoch 15---------------
[2022-08-08 16:24:36,937] lr: 3.874230e-04
[2022-08-08 16:24:37,500] loss: 0.003426  [    0/ 4732]
[2022-08-08 16:24:45,918] loss: 0.028508  [  480/ 4732]
[2022-08-08 16:24:54,337] loss: 0.022313  [  960/ 4732]
[2022-08-08 16:25:02,756] loss: 0.006940  [ 1440/ 4732]
[2022-08-08 16:25:11,176] loss: 0.012062  [ 1920/ 4732]
[2022-08-08 16:25:19,595] loss: 0.017197  [ 2400/ 4732]
[2022-08-08 16:25:28,012] loss: 0.007448  [ 2880/ 4732]
[2022-08-08 16:25:36,432] loss: 0.010385  [ 3360/ 4732]
[2022-08-08 16:25:44,850] loss: 0.052485  [ 3840/ 4732]
[2022-08-08 16:25:53,270] loss: 0.018535  [ 4320/ 4732]
[2022-08-08 16:26:32,027] Train Error: Accuracy: 99.091%, Avg loss: 0.027193
[2022-08-08 16:26:45,948] Test  Error: Accuracy: 98.001%, Avg loss: 0.068043
[2022-08-08 16:26:45,948] Done!
[2022-08-08 16:26:45,952] Number of parameters:1263370
[2022-08-08 16:26:45,953] ## end time: 2022-08-08 16:26:45.948013
[2022-08-08 16:26:45,953] ## used time: 0:32:14.621522
