[2022-08-06 15:10:19,939] ## start time: 2022-08-06 15:10:19.804185
[2022-08-06 15:10:19,940] Using cuda device
[2022-08-06 15:10:19,941] In train:p&d10.npy.
[2022-08-06 15:10:19,942] One Channel
[2022-08-06 15:10:19,942] With Normal data.
[2022-08-06 15:10:19,943] Nunber of classes:10.
[2022-08-06 15:10:19,944] Nunber of ViT channels:1.
[2022-08-06 15:10:20,143] Totol epochs: 10
[2022-08-06 15:10:20,144] Epoch 1---------------
[2022-08-06 15:10:20,145] lr: 2.000000e-03
[2022-08-06 15:10:20,158] loss: 2.503757  [    0/ 4697]
[2022-08-06 15:10:20,319] loss: 1.855693  [  960/ 4697]
[2022-08-06 15:10:20,482] loss: 1.604616  [ 1920/ 4697]
[2022-08-06 15:10:20,641] loss: 0.957444  [ 2880/ 4697]
[2022-08-06 15:10:20,809] loss: 0.701121  [ 3840/ 4697]
[2022-08-06 15:10:21,236] Train Error: Accuracy: 89.504%, Avg loss: 0.414037
[2022-08-06 15:10:21,361] Test  Error: Accuracy: 87.632%, Avg loss: 0.432390
[2022-08-06 15:10:21,361] Epoch 2---------------
[2022-08-06 15:10:21,363] lr: 1.900000e-03
[2022-08-06 15:10:21,374] loss: 0.370786  [    0/ 4697]
[2022-08-06 15:10:21,532] loss: 0.284587  [  960/ 4697]
[2022-08-06 15:10:21,692] loss: 0.281117  [ 1920/ 4697]
[2022-08-06 15:10:21,845] loss: 0.114917  [ 2880/ 4697]
[2022-08-06 15:10:21,997] loss: 0.148119  [ 3840/ 4697]
[2022-08-06 15:10:22,402] Train Error: Accuracy: 94.592%, Avg loss: 0.191683
[2022-08-06 15:10:22,526] Test  Error: Accuracy: 93.768%, Avg loss: 0.215484
[2022-08-06 15:10:22,527] Epoch 3---------------
[2022-08-06 15:10:22,528] lr: 1.805000e-03
[2022-08-06 15:10:22,540] loss: 0.212441  [    0/ 4697]
[2022-08-06 15:10:22,693] loss: 0.081628  [  960/ 4697]
[2022-08-06 15:10:22,851] loss: 0.061073  [ 1920/ 4697]
[2022-08-06 15:10:23,004] loss: 0.070237  [ 2880/ 4697]
[2022-08-06 15:10:23,155] loss: 0.036879  [ 3840/ 4697]
[2022-08-06 15:10:23,554] Train Error: Accuracy: 96.956%, Avg loss: 0.114006
[2022-08-06 15:10:23,674] Test  Error: Accuracy: 96.069%, Avg loss: 0.131822
[2022-08-06 15:10:23,674] Epoch 4---------------
[2022-08-06 15:10:23,675] lr: 1.714750e-03
[2022-08-06 15:10:23,687] loss: 0.095184  [    0/ 4697]
[2022-08-06 15:10:23,842] loss: 0.026638  [  960/ 4697]
[2022-08-06 15:10:23,992] loss: 0.025535  [ 1920/ 4697]
[2022-08-06 15:10:24,142] loss: 0.033947  [ 2880/ 4697]
[2022-08-06 15:10:24,295] loss: 0.068754  [ 3840/ 4697]
[2022-08-06 15:10:24,703] Train Error: Accuracy: 99.659%, Avg loss: 0.026581
[2022-08-06 15:10:24,823] Test  Error: Accuracy: 99.425%, Avg loss: 0.034848
[2022-08-06 15:10:24,824] Epoch 5---------------
[2022-08-06 15:10:24,825] lr: 1.629012e-03
[2022-08-06 15:10:24,837] loss: 0.035099  [    0/ 4697]
[2022-08-06 15:10:24,994] loss: 0.018876  [  960/ 4697]
[2022-08-06 15:10:25,145] loss: 0.022703  [ 1920/ 4697]
[2022-08-06 15:10:25,294] loss: 0.014420  [ 2880/ 4697]
[2022-08-06 15:10:25,444] loss: 0.007002  [ 3840/ 4697]
[2022-08-06 15:10:25,846] Train Error: Accuracy: 99.894%, Avg loss: 0.012045
[2022-08-06 15:10:25,970] Test  Error: Accuracy: 99.712%, Avg loss: 0.018511
[2022-08-06 15:10:25,970] Epoch 6---------------
[2022-08-06 15:10:25,971] lr: 1.547562e-03
[2022-08-06 15:10:25,983] loss: 0.008041  [    0/ 4697]
[2022-08-06 15:10:26,136] loss: 0.010100  [  960/ 4697]
[2022-08-06 15:10:26,286] loss: 0.008681  [ 1920/ 4697]
[2022-08-06 15:10:26,437] loss: 0.008534  [ 2880/ 4697]
[2022-08-06 15:10:26,586] loss: 0.007089  [ 3840/ 4697]
[2022-08-06 15:10:26,984] Train Error: Accuracy: 99.894%, Avg loss: 0.009484
[2022-08-06 15:10:27,105] Test  Error: Accuracy: 99.616%, Avg loss: 0.017137
[2022-08-06 15:10:27,105] Epoch 7---------------
[2022-08-06 15:10:27,107] lr: 1.470184e-03
[2022-08-06 15:10:27,118] loss: 0.010549  [    0/ 4697]
[2022-08-06 15:10:27,268] loss: 0.004179  [  960/ 4697]
[2022-08-06 15:10:27,418] loss: 0.005241  [ 1920/ 4697]
[2022-08-06 15:10:27,570] loss: 0.005225  [ 2880/ 4697]
[2022-08-06 15:10:27,723] loss: 0.022261  [ 3840/ 4697]
[2022-08-06 15:10:28,124] Train Error: Accuracy: 98.063%, Avg loss: 0.064631
[2022-08-06 15:10:28,249] Test  Error: Accuracy: 97.124%, Avg loss: 0.093699
[2022-08-06 15:10:28,250] Epoch 8---------------
[2022-08-06 15:10:28,251] lr: 1.026684e-03
[2022-08-06 15:10:28,263] loss: 0.036450  [    0/ 4697]
[2022-08-06 15:10:28,414] loss: 0.004727  [  960/ 4697]
[2022-08-06 15:10:28,566] loss: 0.005270  [ 1920/ 4697]
[2022-08-06 15:10:28,717] loss: 0.006354  [ 2880/ 4697]
[2022-08-06 15:10:28,868] loss: 0.007523  [ 3840/ 4697]
[2022-08-06 15:10:29,266] Train Error: Accuracy: 99.957%, Avg loss: 0.005592
[2022-08-06 15:10:29,384] Test  Error: Accuracy: 99.856%, Avg loss: 0.009363
[2022-08-06 15:10:29,384] Epoch 9---------------
[2022-08-06 15:10:29,386] lr: 9.753500e-04
[2022-08-06 15:10:29,397] loss: 0.003552  [    0/ 4697]
[2022-08-06 15:10:29,553] loss: 0.002848  [  960/ 4697]
[2022-08-06 15:10:29,705] loss: 0.002095  [ 1920/ 4697]
[2022-08-06 15:10:29,855] loss: 0.005878  [ 2880/ 4697]
[2022-08-06 15:10:30,007] loss: 0.021383  [ 3840/ 4697]
[2022-08-06 15:10:30,404] Train Error: Accuracy: 99.915%, Avg loss: 0.007591
[2022-08-06 15:10:30,527] Test  Error: Accuracy: 99.616%, Avg loss: 0.018532
[2022-08-06 15:10:30,527] Epoch 10---------------
[2022-08-06 15:10:30,528] lr: 6.811233e-04
[2022-08-06 15:10:30,540] loss: 0.005580  [    0/ 4697]
[2022-08-06 15:10:30,697] loss: 0.008636  [  960/ 4697]
[2022-08-06 15:10:30,847] loss: 0.003395  [ 1920/ 4697]
[2022-08-06 15:10:30,997] loss: 0.007506  [ 2880/ 4697]
[2022-08-06 15:10:31,148] loss: 0.002242  [ 3840/ 4697]
[2022-08-06 15:10:31,544] Train Error: Accuracy: 99.936%, Avg loss: 0.006688
[2022-08-06 15:10:31,663] Test  Error: Accuracy: 99.712%, Avg loss: 0.013054
[2022-08-06 15:10:31,663] Done!
[2022-08-06 15:10:31,665] Number of parameters:59338
[2022-08-06 15:10:31,665] ## end time: 2022-08-06 15:10:31.663699
[2022-08-06 15:10:31,665] ## used time: 0:00:11.859514
