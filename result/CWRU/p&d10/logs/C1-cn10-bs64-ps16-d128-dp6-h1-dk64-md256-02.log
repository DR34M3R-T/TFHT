[2022-08-06 16:10:52,460] ## start time: 2022-08-06 16:10:52.322597
[2022-08-06 16:10:52,460] Using cuda device
[2022-08-06 16:10:52,461] In train:p&d10.npy.
[2022-08-06 16:10:52,462] One Channel
[2022-08-06 16:10:52,463] With Normal data.
[2022-08-06 16:10:52,463] Nunber of classes:10.
[2022-08-06 16:10:52,464] Nunber of ViT channels:1.
[2022-08-06 16:10:52,684] Totol epochs: 15
[2022-08-06 16:10:52,687] Epoch 1---------------
[2022-08-06 16:10:52,687] lr: 2.000000e-03
[2022-08-06 16:10:53,191] loss: 2.441643  [    0/ 4798]
[2022-08-06 16:11:00,731] loss: 1.853670  [  960/ 4798]
[2022-08-06 16:11:08,284] loss: 1.593166  [ 1920/ 4798]
[2022-08-06 16:11:15,853] loss: 1.119555  [ 2880/ 4798]
[2022-08-06 16:11:23,423] loss: 0.727707  [ 3840/ 4798]
[2022-08-06 16:11:45,176] Train Error: Accuracy: 86.515%, Avg loss: 0.447893
[2022-08-06 16:11:51,266] Test  Error: Accuracy: 85.793%, Avg loss: 0.450692
[2022-08-06 16:11:51,266] Epoch 2---------------
[2022-08-06 16:11:51,267] lr: 1.900000e-03
[2022-08-06 16:11:51,774] loss: 0.430954  [    0/ 4798]
[2022-08-06 16:11:59,345] loss: 0.286228  [  960/ 4798]
[2022-08-06 16:12:06,918] loss: 0.259979  [ 1920/ 4798]
[2022-08-06 16:12:14,487] loss: 0.233028  [ 2880/ 4798]
[2022-08-06 16:12:22,057] loss: 0.179320  [ 3840/ 4798]
[2022-08-06 16:12:43,813] Train Error: Accuracy: 97.999%, Avg loss: 0.077496
[2022-08-06 16:12:49,901] Test  Error: Accuracy: 96.927%, Avg loss: 0.102673
[2022-08-06 16:12:49,902] Epoch 3---------------
[2022-08-06 16:12:49,904] lr: 1.805000e-03
[2022-08-06 16:12:50,410] loss: 0.112497  [    0/ 4798]
[2022-08-06 16:12:57,982] loss: 0.128214  [  960/ 4798]
[2022-08-06 16:13:05,551] loss: 0.333413  [ 1920/ 4798]
[2022-08-06 16:13:13,121] loss: 0.074949  [ 2880/ 4798]
[2022-08-06 16:13:20,690] loss: 0.024707  [ 3840/ 4798]
[2022-08-06 16:13:42,436] Train Error: Accuracy: 98.916%, Avg loss: 0.047804
[2022-08-06 16:13:48,522] Test  Error: Accuracy: 98.438%, Avg loss: 0.054916
[2022-08-06 16:13:48,522] Epoch 4---------------
[2022-08-06 16:13:48,523] lr: 1.714750e-03
[2022-08-06 16:13:49,031] loss: 0.049212  [    0/ 4798]
[2022-08-06 16:13:56,601] loss: 0.027063  [  960/ 4798]
[2022-08-06 16:14:04,170] loss: 0.016637  [ 1920/ 4798]
[2022-08-06 16:14:11,739] loss: 0.167181  [ 2880/ 4798]
[2022-08-06 16:14:19,309] loss: 0.022335  [ 3840/ 4798]
[2022-08-06 16:14:41,054] Train Error: Accuracy: 99.000%, Avg loss: 0.034476
[2022-08-06 16:14:47,136] Test  Error: Accuracy: 98.892%, Avg loss: 0.043071
[2022-08-06 16:14:47,137] Epoch 5---------------
[2022-08-06 16:14:47,138] lr: 1.629012e-03
[2022-08-06 16:14:47,645] loss: 0.014386  [    0/ 4798]
[2022-08-06 16:14:55,216] loss: 0.053410  [  960/ 4798]
[2022-08-06 16:15:02,785] loss: 0.009558  [ 1920/ 4798]
[2022-08-06 16:15:10,354] loss: 0.019994  [ 2880/ 4798]
[2022-08-06 16:15:17,922] loss: 0.005923  [ 3840/ 4798]
[2022-08-06 16:15:39,663] Train Error: Accuracy: 99.250%, Avg loss: 0.030403
[2022-08-06 16:15:45,747] Test  Error: Accuracy: 98.589%, Avg loss: 0.046666
[2022-08-06 16:15:45,747] Epoch 6---------------
[2022-08-06 16:15:45,748] lr: 1.396675e-03
[2022-08-06 16:15:46,255] loss: 0.014317  [    0/ 4798]
[2022-08-06 16:15:53,823] loss: 0.061810  [  960/ 4798]
[2022-08-06 16:16:01,392] loss: 0.021731  [ 1920/ 4798]
[2022-08-06 16:16:08,963] loss: 0.006988  [ 2880/ 4798]
[2022-08-06 16:16:16,533] loss: 0.006673  [ 3840/ 4798]
[2022-08-06 16:16:38,280] Train Error: Accuracy: 98.312%, Avg loss: 0.051799
[2022-08-06 16:16:44,365] Test  Error: Accuracy: 97.683%, Avg loss: 0.069988
[2022-08-06 16:16:44,366] Epoch 7---------------
[2022-08-06 16:16:44,366] lr: 9.753500e-04
[2022-08-06 16:16:44,872] loss: 0.024129  [    0/ 4798]
[2022-08-06 16:16:52,444] loss: 0.016023  [  960/ 4798]
[2022-08-06 16:17:00,012] loss: 0.006056  [ 1920/ 4798]
[2022-08-06 16:17:07,584] loss: 0.048991  [ 2880/ 4798]
[2022-08-06 16:17:15,155] loss: 0.040078  [ 3840/ 4798]
[2022-08-06 16:17:36,898] Train Error: Accuracy: 99.562%, Avg loss: 0.019612
[2022-08-06 16:17:42,985] Test  Error: Accuracy: 99.244%, Avg loss: 0.027381
[2022-08-06 16:17:42,986] Epoch 8---------------
[2022-08-06 16:17:42,987] lr: 9.265825e-04
[2022-08-06 16:17:43,494] loss: 0.012853  [    0/ 4798]
[2022-08-06 16:17:51,064] loss: 0.005198  [  960/ 4798]
[2022-08-06 16:17:58,633] loss: 0.008282  [ 1920/ 4798]
[2022-08-06 16:18:06,203] loss: 0.014956  [ 2880/ 4798]
[2022-08-06 16:18:13,774] loss: 0.003157  [ 3840/ 4798]
[2022-08-06 16:18:35,527] Train Error: Accuracy: 99.792%, Avg loss: 0.008785
[2022-08-06 16:18:41,614] Test  Error: Accuracy: 99.496%, Avg loss: 0.018003
[2022-08-06 16:18:41,615] Epoch 9---------------
[2022-08-06 16:18:41,616] lr: 8.802533e-04
[2022-08-06 16:18:42,124] loss: 0.006929  [    0/ 4798]
[2022-08-06 16:18:49,693] loss: 0.001139  [  960/ 4798]
[2022-08-06 16:18:57,265] loss: 0.003301  [ 1920/ 4798]
[2022-08-06 16:19:04,838] loss: 0.002209  [ 2880/ 4798]
[2022-08-06 16:19:12,411] loss: 0.003593  [ 3840/ 4798]
[2022-08-06 16:19:34,157] Train Error: Accuracy: 99.625%, Avg loss: 0.011968
[2022-08-06 16:19:40,245] Test  Error: Accuracy: 99.496%, Avg loss: 0.016146
[2022-08-06 16:19:40,246] Epoch 10---------------
[2022-08-06 16:19:40,247] lr: 8.362407e-04
[2022-08-06 16:19:40,755] loss: 0.024470  [    0/ 4798]
[2022-08-06 16:19:48,326] loss: 0.005253  [  960/ 4798]
[2022-08-06 16:19:55,898] loss: 0.071178  [ 1920/ 4798]
[2022-08-06 16:20:03,468] loss: 0.006503  [ 2880/ 4798]
[2022-08-06 16:20:11,038] loss: 0.005374  [ 3840/ 4798]
[2022-08-06 16:20:32,792] Train Error: Accuracy: 99.937%, Avg loss: 0.005208
[2022-08-06 16:20:38,878] Test  Error: Accuracy: 99.849%, Avg loss: 0.007441
[2022-08-06 16:20:38,878] Epoch 11---------------
[2022-08-06 16:20:38,880] lr: 7.944286e-04
[2022-08-06 16:20:39,386] loss: 0.008770  [    0/ 4798]
[2022-08-06 16:20:46,957] loss: 0.004442  [  960/ 4798]
[2022-08-06 16:20:54,527] loss: 0.001793  [ 1920/ 4798]
[2022-08-06 16:21:02,098] loss: 0.003307  [ 2880/ 4798]
[2022-08-06 16:21:09,667] loss: 0.001815  [ 3840/ 4798]
[2022-08-06 16:21:31,422] Train Error: Accuracy: 99.937%, Avg loss: 0.004120
[2022-08-06 16:21:37,510] Test  Error: Accuracy: 99.950%, Avg loss: 0.005970
[2022-08-06 16:21:37,510] Epoch 12---------------
[2022-08-06 16:21:37,511] lr: 7.547072e-04
[2022-08-06 16:21:38,019] loss: 0.005070  [    0/ 4798]
[2022-08-06 16:21:45,590] loss: 0.001085  [  960/ 4798]
[2022-08-06 16:21:53,160] loss: 0.006505  [ 1920/ 4798]
[2022-08-06 16:22:00,731] loss: 0.002194  [ 2880/ 4798]
[2022-08-06 16:22:08,299] loss: 0.005911  [ 3840/ 4798]
[2022-08-06 16:22:30,055] Train Error: Accuracy: 99.958%, Avg loss: 0.002784
[2022-08-06 16:22:36,141] Test  Error: Accuracy: 99.748%, Avg loss: 0.010735
[2022-08-06 16:22:36,141] Epoch 13---------------
[2022-08-06 16:22:36,144] lr: 5.270402e-04
[2022-08-06 16:22:36,650] loss: 0.005414  [    0/ 4798]
[2022-08-06 16:22:44,219] loss: 0.001184  [  960/ 4798]
[2022-08-06 16:22:51,789] loss: 0.001880  [ 1920/ 4798]
[2022-08-06 16:22:59,360] loss: 0.001582  [ 2880/ 4798]
[2022-08-06 16:23:06,930] loss: 0.002307  [ 3840/ 4798]
[2022-08-06 16:23:28,680] Train Error: Accuracy: 100.000%, Avg loss: 0.001539
[2022-08-06 16:23:34,766] Test  Error: Accuracy: 99.798%, Avg loss: 0.008885
[2022-08-06 16:23:34,766] Epoch 14---------------
[2022-08-06 16:23:34,767] lr: 5.006882e-04
[2022-08-06 16:23:35,275] loss: 0.001153  [    0/ 4798]
[2022-08-06 16:23:42,846] loss: 0.003571  [  960/ 4798]
[2022-08-06 16:23:50,416] loss: 0.002165  [ 1920/ 4798]
[2022-08-06 16:23:57,986] loss: 0.000887  [ 2880/ 4798]
[2022-08-06 16:24:05,555] loss: 0.000971  [ 3840/ 4798]
[2022-08-06 16:24:27,308] Train Error: Accuracy: 99.979%, Avg loss: 0.001891
[2022-08-06 16:24:33,396] Test  Error: Accuracy: 99.748%, Avg loss: 0.007740
[2022-08-06 16:24:33,397] Epoch 15---------------
[2022-08-06 16:24:33,398] lr: 4.756538e-04
[2022-08-06 16:24:33,906] loss: 0.006756  [    0/ 4798]
[2022-08-06 16:24:41,475] loss: 0.000989  [  960/ 4798]
[2022-08-06 16:24:49,046] loss: 0.001851  [ 1920/ 4798]
[2022-08-06 16:24:56,616] loss: 0.001544  [ 2880/ 4798]
[2022-08-06 16:25:04,185] loss: 0.000860  [ 3840/ 4798]
[2022-08-06 16:25:25,939] Train Error: Accuracy: 100.000%, Avg loss: 0.001374
[2022-08-06 16:25:32,028] Test  Error: Accuracy: 99.748%, Avg loss: 0.005409
[2022-08-06 16:25:32,029] Done!
[2022-08-06 16:25:32,033] Number of parameters:1232650
[2022-08-06 16:25:32,033] ## end time: 2022-08-06 16:25:32.029763
[2022-08-06 16:25:32,033] ## used time: 0:14:39.707166
