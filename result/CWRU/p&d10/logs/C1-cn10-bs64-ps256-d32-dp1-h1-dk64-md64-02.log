[2022-08-06 15:16:42,858] ## start time: 2022-08-06 15:16:42.713360
[2022-08-06 15:16:42,859] Using cuda device
[2022-08-06 15:16:42,861] In train:p&d10.npy.
[2022-08-06 15:16:42,861] One Channel
[2022-08-06 15:16:42,862] With Normal data.
[2022-08-06 15:16:42,862] Nunber of classes:10.
[2022-08-06 15:16:42,862] Nunber of ViT channels:1.
[2022-08-06 15:16:43,049] Totol epochs: 10
[2022-08-06 15:16:43,051] Epoch 1---------------
[2022-08-06 15:16:43,051] lr: 2.000000e-03
[2022-08-06 15:16:43,064] loss: 2.483814  [    0/ 4798]
[2022-08-06 15:16:43,221] loss: 1.987159  [  960/ 4798]
[2022-08-06 15:16:43,378] loss: 1.787105  [ 1920/ 4798]
[2022-08-06 15:16:43,533] loss: 1.233234  [ 2880/ 4798]
[2022-08-06 15:16:43,684] loss: 1.136780  [ 3840/ 4798]
[2022-08-06 15:16:44,095] Train Error: Accuracy: 70.238%, Avg loss: 0.888215
[2022-08-06 15:16:44,212] Test  Error: Accuracy: 69.169%, Avg loss: 0.895133
[2022-08-06 15:16:44,212] Epoch 2---------------
[2022-08-06 15:16:44,213] lr: 1.900000e-03
[2022-08-06 15:16:44,224] loss: 0.715383  [    0/ 4798]
[2022-08-06 15:16:44,378] loss: 0.767654  [  960/ 4798]
[2022-08-06 15:16:44,533] loss: 0.834651  [ 1920/ 4798]
[2022-08-06 15:16:44,685] loss: 0.473938  [ 2880/ 4798]
[2022-08-06 15:16:44,832] loss: 0.402206  [ 3840/ 4798]
[2022-08-06 15:16:45,225] Train Error: Accuracy: 87.411%, Avg loss: 0.411584
[2022-08-06 15:16:45,332] Test  Error: Accuracy: 84.786%, Avg loss: 0.461008
[2022-08-06 15:16:45,333] Epoch 3---------------
[2022-08-06 15:16:45,334] lr: 1.805000e-03
[2022-08-06 15:16:45,345] loss: 0.391414  [    0/ 4798]
[2022-08-06 15:16:45,493] loss: 0.310118  [  960/ 4798]
[2022-08-06 15:16:45,643] loss: 0.380196  [ 1920/ 4798]
[2022-08-06 15:16:45,789] loss: 0.209805  [ 2880/ 4798]
[2022-08-06 15:16:45,937] loss: 0.219277  [ 3840/ 4798]
[2022-08-06 15:16:46,328] Train Error: Accuracy: 94.706%, Avg loss: 0.216364
[2022-08-06 15:16:46,437] Test  Error: Accuracy: 92.997%, Avg loss: 0.251989
[2022-08-06 15:16:46,437] Epoch 4---------------
[2022-08-06 15:16:46,438] lr: 1.714750e-03
[2022-08-06 15:16:46,450] loss: 0.171045  [    0/ 4798]
[2022-08-06 15:16:46,602] loss: 0.171978  [  960/ 4798]
[2022-08-06 15:16:46,754] loss: 0.157796  [ 1920/ 4798]
[2022-08-06 15:16:46,905] loss: 0.143152  [ 2880/ 4798]
[2022-08-06 15:16:47,053] loss: 0.179100  [ 3840/ 4798]
[2022-08-06 15:16:47,446] Train Error: Accuracy: 95.748%, Avg loss: 0.155408
[2022-08-06 15:16:47,553] Test  Error: Accuracy: 95.013%, Avg loss: 0.181560
[2022-08-06 15:16:47,554] Epoch 5---------------
[2022-08-06 15:16:47,555] lr: 1.629012e-03
[2022-08-06 15:16:47,567] loss: 0.248049  [    0/ 4798]
[2022-08-06 15:16:47,715] loss: 0.055699  [  960/ 4798]
[2022-08-06 15:16:47,865] loss: 0.090136  [ 1920/ 4798]
[2022-08-06 15:16:48,012] loss: 0.119353  [ 2880/ 4798]
[2022-08-06 15:16:48,160] loss: 0.153848  [ 3840/ 4798]
[2022-08-06 15:16:48,557] Train Error: Accuracy: 96.978%, Avg loss: 0.114333
[2022-08-06 15:16:48,666] Test  Error: Accuracy: 95.063%, Avg loss: 0.155368
[2022-08-06 15:16:48,666] Epoch 6---------------
[2022-08-06 15:16:48,667] lr: 1.547562e-03
[2022-08-06 15:16:48,678] loss: 0.129982  [    0/ 4798]
[2022-08-06 15:16:48,828] loss: 0.061426  [  960/ 4798]
[2022-08-06 15:16:48,976] loss: 0.092312  [ 1920/ 4798]
[2022-08-06 15:16:49,124] loss: 0.098485  [ 2880/ 4798]
[2022-08-06 15:16:49,271] loss: 0.073141  [ 3840/ 4798]
[2022-08-06 15:16:49,673] Train Error: Accuracy: 98.249%, Avg loss: 0.065001
[2022-08-06 15:16:49,782] Test  Error: Accuracy: 97.531%, Avg loss: 0.097339
[2022-08-06 15:16:49,782] Epoch 7---------------
[2022-08-06 15:16:49,783] lr: 1.470184e-03
[2022-08-06 15:16:49,794] loss: 0.040609  [    0/ 4798]
[2022-08-06 15:16:49,948] loss: 0.115233  [  960/ 4798]
[2022-08-06 15:16:50,097] loss: 0.019406  [ 1920/ 4798]
[2022-08-06 15:16:50,243] loss: 0.074929  [ 2880/ 4798]
[2022-08-06 15:16:50,390] loss: 0.091458  [ 3840/ 4798]
[2022-08-06 15:16:50,785] Train Error: Accuracy: 98.270%, Avg loss: 0.064022
[2022-08-06 15:16:50,893] Test  Error: Accuracy: 96.322%, Avg loss: 0.120618
[2022-08-06 15:16:50,894] Epoch 8---------------
[2022-08-06 15:16:50,895] lr: 1.137600e-03
[2022-08-06 15:16:50,907] loss: 0.099689  [    0/ 4798]
[2022-08-06 15:16:51,059] loss: 0.117290  [  960/ 4798]
[2022-08-06 15:16:51,207] loss: 0.117259  [ 1920/ 4798]
[2022-08-06 15:16:51,355] loss: 0.060281  [ 2880/ 4798]
[2022-08-06 15:16:51,502] loss: 0.054939  [ 3840/ 4798]
[2022-08-06 15:16:51,905] Train Error: Accuracy: 99.083%, Avg loss: 0.041533
[2022-08-06 15:16:52,014] Test  Error: Accuracy: 97.834%, Avg loss: 0.144919
[2022-08-06 15:16:52,014] Epoch 9---------------
[2022-08-06 15:16:52,015] lr: 8.802533e-04
[2022-08-06 15:16:52,028] loss: 0.013331  [    0/ 4798]
[2022-08-06 15:16:52,178] loss: 0.021637  [  960/ 4798]
[2022-08-06 15:16:52,326] loss: 0.072128  [ 1920/ 4798]
[2022-08-06 15:16:52,473] loss: 0.053477  [ 2880/ 4798]
[2022-08-06 15:16:52,621] loss: 0.029943  [ 3840/ 4798]
[2022-08-06 15:16:53,021] Train Error: Accuracy: 99.187%, Avg loss: 0.034886
[2022-08-06 15:16:53,131] Test  Error: Accuracy: 98.438%, Avg loss: 0.070494
[2022-08-06 15:16:53,131] Epoch 10---------------
[2022-08-06 15:16:53,132] lr: 8.362407e-04
[2022-08-06 15:16:53,144] loss: 0.064321  [    0/ 4798]
[2022-08-06 15:16:53,290] loss: 0.045780  [  960/ 4798]
[2022-08-06 15:16:53,437] loss: 0.106440  [ 1920/ 4798]
[2022-08-06 15:16:53,586] loss: 0.024177  [ 2880/ 4798]
[2022-08-06 15:16:53,736] loss: 0.016735  [ 3840/ 4798]
[2022-08-06 15:16:54,131] Train Error: Accuracy: 98.958%, Avg loss: 0.041399
[2022-08-06 15:16:54,243] Test  Error: Accuracy: 98.186%, Avg loss: 0.068698
[2022-08-06 15:16:54,243] Done!
[2022-08-06 15:16:54,245] Number of parameters:42954
[2022-08-06 15:16:54,245] ## end time: 2022-08-06 15:16:54.243217
[2022-08-06 15:16:54,246] ## used time: 0:00:11.529857
