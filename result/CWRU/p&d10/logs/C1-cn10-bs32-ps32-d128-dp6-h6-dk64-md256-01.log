[2022-08-08 00:29:12,661] ## start time: 2022-08-08 00:29:12.537203
[2022-08-08 00:29:12,661] Using cuda device
[2022-08-08 00:29:12,662] In train:p&d10.npy.
[2022-08-08 00:29:12,664] One Channel
[2022-08-08 00:29:12,664] With Normal data.
[2022-08-08 00:29:12,665] Nunber of classes:10.
[2022-08-08 00:29:12,665] Nunber of ViT channels:1.
[2022-08-08 00:29:12,915] Totol epochs: 15
[2022-08-08 00:29:12,918] Epoch 1---------------
[2022-08-08 00:29:12,918] lr: 2.000000e-03
[2022-08-08 00:29:13,276] loss: 2.486661  [    0/ 4740]
[2022-08-08 00:29:18,618] loss: 1.846442  [  480/ 4740]
[2022-08-08 00:29:23,958] loss: 1.617532  [  960/ 4740]
[2022-08-08 00:29:29,300] loss: 1.189517  [ 1440/ 4740]
[2022-08-08 00:29:34,642] loss: 0.839714  [ 1920/ 4740]
[2022-08-08 00:29:39,983] loss: 0.521055  [ 2400/ 4740]
[2022-08-08 00:29:45,326] loss: 0.224150  [ 2880/ 4740]
[2022-08-08 00:29:50,668] loss: 0.139292  [ 3360/ 4740]
[2022-08-08 00:29:56,011] loss: 0.278320  [ 3840/ 4740]
[2022-08-08 00:30:01,354] loss: 0.029199  [ 4320/ 4740]
[2022-08-08 00:30:24,170] Train Error: Accuracy: 98.418%, Avg loss: 0.074576
[2022-08-08 00:30:32,136] Test  Error: Accuracy: 98.483%, Avg loss: 0.071076
[2022-08-08 00:30:32,136] Epoch 2---------------
[2022-08-08 00:30:32,137] lr: 1.900000e-03
[2022-08-08 00:30:32,495] loss: 0.074748  [    0/ 4740]
[2022-08-08 00:30:37,840] loss: 0.444103  [  480/ 4740]
[2022-08-08 00:30:43,183] loss: 0.080176  [  960/ 4740]
[2022-08-08 00:30:48,522] loss: 0.069119  [ 1440/ 4740]
[2022-08-08 00:30:53,863] loss: 0.201913  [ 1920/ 4740]
[2022-08-08 00:30:59,206] loss: 0.027515  [ 2400/ 4740]
[2022-08-08 00:31:04,547] loss: 0.264374  [ 2880/ 4740]
[2022-08-08 00:31:09,890] loss: 0.076302  [ 3360/ 4740]
[2022-08-08 00:31:15,230] loss: 0.010052  [ 3840/ 4740]
[2022-08-08 00:31:20,573] loss: 0.016356  [ 4320/ 4740]
[2022-08-08 00:31:43,402] Train Error: Accuracy: 99.494%, Avg loss: 0.025356
[2022-08-08 00:31:51,371] Test  Error: Accuracy: 99.413%, Avg loss: 0.026536
[2022-08-08 00:31:51,372] Epoch 3---------------
[2022-08-08 00:31:51,373] lr: 1.805000e-03
[2022-08-08 00:31:51,731] loss: 0.008375  [    0/ 4740]
[2022-08-08 00:31:57,073] loss: 0.008976  [  480/ 4740]
[2022-08-08 00:32:02,420] loss: 0.006496  [  960/ 4740]
[2022-08-08 00:32:07,764] loss: 0.151187  [ 1440/ 4740]
[2022-08-08 00:32:13,106] loss: 0.006487  [ 1920/ 4740]
[2022-08-08 00:32:18,445] loss: 0.024867  [ 2400/ 4740]
[2022-08-08 00:32:23,788] loss: 0.003652  [ 2880/ 4740]
[2022-08-08 00:32:29,131] loss: 0.378377  [ 3360/ 4740]
[2022-08-08 00:32:34,472] loss: 0.015171  [ 3840/ 4740]
[2022-08-08 00:32:39,814] loss: 0.016212  [ 4320/ 4740]
[2022-08-08 00:33:02,626] Train Error: Accuracy: 96.730%, Avg loss: 0.114519
[2022-08-08 00:33:10,587] Test  Error: Accuracy: 97.161%, Avg loss: 0.109226
[2022-08-08 00:33:10,589] Epoch 4---------------
[2022-08-08 00:33:10,591] lr: 1.260499e-03
[2022-08-08 00:33:10,949] loss: 0.049397  [    0/ 4740]
[2022-08-08 00:33:16,288] loss: 0.010490  [  480/ 4740]
[2022-08-08 00:33:21,633] loss: 0.184288  [  960/ 4740]
[2022-08-08 00:33:26,976] loss: 0.010153  [ 1440/ 4740]
[2022-08-08 00:33:32,319] loss: 0.010439  [ 1920/ 4740]
[2022-08-08 00:33:37,665] loss: 0.015054  [ 2400/ 4740]
[2022-08-08 00:33:43,008] loss: 0.004999  [ 2880/ 4740]
[2022-08-08 00:33:48,349] loss: 0.003853  [ 3360/ 4740]
[2022-08-08 00:33:53,693] loss: 0.007822  [ 3840/ 4740]
[2022-08-08 00:33:59,035] loss: 0.005001  [ 4320/ 4740]
[2022-08-08 00:34:21,850] Train Error: Accuracy: 96.603%, Avg loss: 0.121908
[2022-08-08 00:34:29,813] Test  Error: Accuracy: 95.595%, Avg loss: 0.167232
[2022-08-08 00:34:29,813] Epoch 5---------------
[2022-08-08 00:34:29,814] lr: 8.802533e-04
[2022-08-08 00:34:30,172] loss: 0.002170  [    0/ 4740]
[2022-08-08 00:34:35,517] loss: 0.002882  [  480/ 4740]
[2022-08-08 00:34:40,862] loss: 0.008353  [  960/ 4740]
[2022-08-08 00:34:46,206] loss: 0.008414  [ 1440/ 4740]
[2022-08-08 00:34:51,548] loss: 0.001869  [ 1920/ 4740]
[2022-08-08 00:34:56,891] loss: 0.012598  [ 2400/ 4740]
[2022-08-08 00:35:02,234] loss: 0.051563  [ 2880/ 4740]
[2022-08-08 00:35:07,576] loss: 0.005728  [ 3360/ 4740]
[2022-08-08 00:35:12,918] loss: 0.005757  [ 3840/ 4740]
[2022-08-08 00:35:18,261] loss: 0.003817  [ 4320/ 4740]
[2022-08-08 00:35:41,088] Train Error: Accuracy: 99.916%, Avg loss: 0.004124
[2022-08-08 00:35:49,051] Test  Error: Accuracy: 99.902%, Avg loss: 0.005991
[2022-08-08 00:35:49,051] Epoch 6---------------
[2022-08-08 00:35:49,054] lr: 8.362407e-04
[2022-08-08 00:35:49,413] loss: 0.008035  [    0/ 4740]
[2022-08-08 00:35:54,754] loss: 0.002653  [  480/ 4740]
[2022-08-08 00:36:00,099] loss: 0.003585  [  960/ 4740]
[2022-08-08 00:36:05,444] loss: 0.021882  [ 1440/ 4740]
[2022-08-08 00:36:10,785] loss: 0.004222  [ 1920/ 4740]
[2022-08-08 00:36:16,129] loss: 0.001367  [ 2400/ 4740]
[2022-08-08 00:36:21,471] loss: 0.001912  [ 2880/ 4740]
[2022-08-08 00:36:26,817] loss: 0.001313  [ 3360/ 4740]
[2022-08-08 00:36:32,158] loss: 0.021094  [ 3840/ 4740]
[2022-08-08 00:36:37,500] loss: 0.004356  [ 4320/ 4740]
[2022-08-08 00:37:00,320] Train Error: Accuracy: 99.979%, Avg loss: 0.001929
[2022-08-08 00:37:08,275] Test  Error: Accuracy: 99.951%, Avg loss: 0.003728
[2022-08-08 00:37:08,276] Epoch 7---------------
[2022-08-08 00:37:08,277] lr: 7.944286e-04
[2022-08-08 00:37:08,634] loss: 0.001124  [    0/ 4740]
[2022-08-08 00:37:13,976] loss: 0.000720  [  480/ 4740]
[2022-08-08 00:37:19,317] loss: 0.000811  [  960/ 4740]
[2022-08-08 00:37:24,658] loss: 0.000675  [ 1440/ 4740]
[2022-08-08 00:37:29,999] loss: 0.001215  [ 1920/ 4740]
[2022-08-08 00:37:35,341] loss: 0.002317  [ 2400/ 4740]
[2022-08-08 00:37:40,682] loss: 0.003166  [ 2880/ 4740]
[2022-08-08 00:37:46,023] loss: 0.001810  [ 3360/ 4740]
[2022-08-08 00:37:51,367] loss: 0.001249  [ 3840/ 4740]
[2022-08-08 00:37:56,704] loss: 0.000449  [ 4320/ 4740]
[2022-08-08 00:38:19,503] Train Error: Accuracy: 100.000%, Avg loss: 0.001131
[2022-08-08 00:38:27,458] Test  Error: Accuracy: 99.951%, Avg loss: 0.003046
[2022-08-08 00:38:27,459] Epoch 8---------------
[2022-08-08 00:38:27,460] lr: 7.547072e-04
[2022-08-08 00:38:27,819] loss: 0.000745  [    0/ 4740]
[2022-08-08 00:38:33,159] loss: 0.000718  [  480/ 4740]
[2022-08-08 00:38:38,503] loss: 0.000371  [  960/ 4740]
[2022-08-08 00:38:43,845] loss: 0.000534  [ 1440/ 4740]
[2022-08-08 00:38:49,190] loss: 0.000744  [ 1920/ 4740]
[2022-08-08 00:38:54,532] loss: 0.000629  [ 2400/ 4740]
[2022-08-08 00:38:59,872] loss: 0.000517  [ 2880/ 4740]
[2022-08-08 00:39:05,215] loss: 0.000348  [ 3360/ 4740]
[2022-08-08 00:39:10,556] loss: 0.000714  [ 3840/ 4740]
[2022-08-08 00:39:15,898] loss: 0.000991  [ 4320/ 4740]
[2022-08-08 00:39:38,692] Train Error: Accuracy: 100.000%, Avg loss: 0.000740
[2022-08-08 00:39:46,644] Test  Error: Accuracy: 100.000%, Avg loss: 0.001727
[2022-08-08 00:39:46,645] Epoch 9---------------
[2022-08-08 00:39:46,646] lr: 7.169718e-04
[2022-08-08 00:39:47,004] loss: 0.000553  [    0/ 4740]
[2022-08-08 00:39:52,344] loss: 0.000518  [  480/ 4740]
[2022-08-08 00:39:57,685] loss: 0.000483  [  960/ 4740]
[2022-08-08 00:40:03,029] loss: 0.000483  [ 1440/ 4740]
[2022-08-08 00:40:08,370] loss: 0.000564  [ 1920/ 4740]
[2022-08-08 00:40:13,712] loss: 0.000715  [ 2400/ 4740]
[2022-08-08 00:40:19,054] loss: 0.001412  [ 2880/ 4740]
[2022-08-08 00:40:24,398] loss: 0.000542  [ 3360/ 4740]
[2022-08-08 00:40:29,741] loss: 0.000438  [ 3840/ 4740]
[2022-08-08 00:40:35,082] loss: 0.001093  [ 4320/ 4740]
[2022-08-08 00:40:57,878] Train Error: Accuracy: 100.000%, Avg loss: 0.000776
[2022-08-08 00:41:05,837] Test  Error: Accuracy: 99.853%, Avg loss: 0.004754
[2022-08-08 00:41:05,837] Epoch 10---------------
[2022-08-08 00:41:05,838] lr: 5.006882e-04
[2022-08-08 00:41:06,196] loss: 0.001406  [    0/ 4740]
[2022-08-08 00:41:11,540] loss: 0.001107  [  480/ 4740]
[2022-08-08 00:41:16,885] loss: 0.000494  [  960/ 4740]
[2022-08-08 00:41:22,225] loss: 0.000432  [ 1440/ 4740]
[2022-08-08 00:41:27,567] loss: 0.000289  [ 1920/ 4740]
[2022-08-08 00:41:32,909] loss: 0.000333  [ 2400/ 4740]
[2022-08-08 00:41:38,251] loss: 0.000534  [ 2880/ 4740]
[2022-08-08 00:41:43,593] loss: 0.000398  [ 3360/ 4740]
[2022-08-08 00:41:48,936] loss: 0.001728  [ 3840/ 4740]
[2022-08-08 00:41:54,279] loss: 0.000406  [ 4320/ 4740]
[2022-08-08 00:42:17,079] Train Error: Accuracy: 100.000%, Avg loss: 0.000710
[2022-08-08 00:42:25,034] Test  Error: Accuracy: 99.951%, Avg loss: 0.001904
[2022-08-08 00:42:25,035] Epoch 11---------------
[2022-08-08 00:42:25,036] lr: 4.756538e-04
[2022-08-08 00:42:25,394] loss: 0.000712  [    0/ 4740]
[2022-08-08 00:42:30,738] loss: 0.000271  [  480/ 4740]
[2022-08-08 00:42:36,082] loss: 0.000756  [  960/ 4740]
[2022-08-08 00:42:41,428] loss: 0.000343  [ 1440/ 4740]
[2022-08-08 00:42:46,770] loss: 0.000339  [ 1920/ 4740]
[2022-08-08 00:42:52,112] loss: 0.000715  [ 2400/ 4740]
[2022-08-08 00:42:57,455] loss: 0.000430  [ 2880/ 4740]
[2022-08-08 00:43:02,798] loss: 0.002172  [ 3360/ 4740]
[2022-08-08 00:43:08,140] loss: 0.000754  [ 3840/ 4740]
[2022-08-08 00:43:13,483] loss: 0.000658  [ 4320/ 4740]
[2022-08-08 00:43:36,278] Train Error: Accuracy: 100.000%, Avg loss: 0.000738
[2022-08-08 00:43:44,232] Test  Error: Accuracy: 99.951%, Avg loss: 0.001752
[2022-08-08 00:43:44,232] Epoch 12---------------
[2022-08-08 00:43:44,233] lr: 4.518711e-04
[2022-08-08 00:43:44,592] loss: 0.000597  [    0/ 4740]
[2022-08-08 00:43:49,935] loss: 0.000370  [  480/ 4740]
[2022-08-08 00:43:55,275] loss: 0.000769  [  960/ 4740]
[2022-08-08 00:44:00,617] loss: 0.000454  [ 1440/ 4740]
[2022-08-08 00:44:05,956] loss: 0.000733  [ 1920/ 4740]
[2022-08-08 00:44:11,300] loss: 0.000301  [ 2400/ 4740]
[2022-08-08 00:44:16,643] loss: 0.000570  [ 2880/ 4740]
[2022-08-08 00:44:21,979] loss: 0.000550  [ 3360/ 4740]
[2022-08-08 00:44:27,315] loss: 0.001123  [ 3840/ 4740]
[2022-08-08 00:44:32,651] loss: 0.000799  [ 4320/ 4740]
[2022-08-08 00:44:55,443] Train Error: Accuracy: 100.000%, Avg loss: 0.001036
[2022-08-08 00:45:03,399] Test  Error: Accuracy: 99.902%, Avg loss: 0.003741
[2022-08-08 00:45:03,399] Epoch 13---------------
[2022-08-08 00:45:03,400] lr: 3.155584e-04
[2022-08-08 00:45:03,759] loss: 0.000415  [    0/ 4740]
[2022-08-08 00:45:09,098] loss: 0.005219  [  480/ 4740]
[2022-08-08 00:45:14,437] loss: 0.000296  [  960/ 4740]
[2022-08-08 00:45:19,774] loss: 0.000375  [ 1440/ 4740]
[2022-08-08 00:45:25,116] loss: 0.000217  [ 1920/ 4740]
[2022-08-08 00:45:30,460] loss: 0.000530  [ 2400/ 4740]
[2022-08-08 00:45:35,802] loss: 0.000591  [ 2880/ 4740]
[2022-08-08 00:45:41,143] loss: 0.001006  [ 3360/ 4740]
[2022-08-08 00:45:46,485] loss: 0.000459  [ 3840/ 4740]
[2022-08-08 00:45:51,829] loss: 0.003480  [ 4320/ 4740]
[2022-08-08 00:46:14,624] Train Error: Accuracy: 99.958%, Avg loss: 0.001404
[2022-08-08 00:46:22,582] Test  Error: Accuracy: 99.902%, Avg loss: 0.004429
[2022-08-08 00:46:22,582] Epoch 14---------------
[2022-08-08 00:46:22,583] lr: 2.441731e-04
[2022-08-08 00:46:22,941] loss: 0.000645  [    0/ 4740]
[2022-08-08 00:46:28,282] loss: 0.000945  [  480/ 4740]
[2022-08-08 00:46:33,622] loss: 0.000698  [  960/ 4740]
[2022-08-08 00:46:38,964] loss: 0.000309  [ 1440/ 4740]
[2022-08-08 00:46:44,307] loss: 0.000597  [ 1920/ 4740]
[2022-08-08 00:46:49,650] loss: 0.000252  [ 2400/ 4740]
[2022-08-08 00:46:54,991] loss: 0.000462  [ 2880/ 4740]
[2022-08-08 00:47:00,332] loss: 0.001685  [ 3360/ 4740]
[2022-08-08 00:47:05,674] loss: 0.000267  [ 3840/ 4740]
[2022-08-08 00:47:11,016] loss: 0.000562  [ 4320/ 4740]
[2022-08-08 00:47:33,818] Train Error: Accuracy: 100.000%, Avg loss: 0.000641
[2022-08-08 00:47:41,772] Test  Error: Accuracy: 99.902%, Avg loss: 0.003595
[2022-08-08 00:47:41,773] Epoch 15---------------
[2022-08-08 00:47:41,774] lr: 2.319644e-04
[2022-08-08 00:47:42,132] loss: 0.000477  [    0/ 4740]
[2022-08-08 00:47:47,474] loss: 0.000233  [  480/ 4740]
[2022-08-08 00:47:52,815] loss: 0.000606  [  960/ 4740]
[2022-08-08 00:47:58,155] loss: 0.000417  [ 1440/ 4740]
[2022-08-08 00:48:03,498] loss: 0.001013  [ 1920/ 4740]
[2022-08-08 00:48:08,839] loss: 0.000982  [ 2400/ 4740]
[2022-08-08 00:48:14,181] loss: 0.000814  [ 2880/ 4740]
[2022-08-08 00:48:19,523] loss: 0.000268  [ 3360/ 4740]
[2022-08-08 00:48:24,863] loss: 0.000378  [ 3840/ 4740]
[2022-08-08 00:48:30,204] loss: 0.000631  [ 4320/ 4740]
[2022-08-08 00:48:52,994] Train Error: Accuracy: 100.000%, Avg loss: 0.000785
[2022-08-08 00:49:00,951] Test  Error: Accuracy: 99.804%, Avg loss: 0.006174
[2022-08-08 00:49:00,951] Done!
[2022-08-08 00:49:00,955] Number of parameters:3186442
[2022-08-08 00:49:00,956] ## end time: 2022-08-08 00:49:00.951067
[2022-08-08 00:49:00,956] ## used time: 0:19:48.413864
