[2022-08-08 18:47:51,106] ## start time: 2022-08-08 18:47:50.914260
[2022-08-08 18:47:51,107] Using cuda device
[2022-08-08 18:47:51,108] In train:p&d10.npy.
[2022-08-08 18:47:51,108] One Channel
[2022-08-08 18:47:51,109] With Normal data.
[2022-08-08 18:47:51,110] Nunber of classes:10.
[2022-08-08 18:47:51,111] Nunber of ViT channels:1.
[2022-08-08 18:47:51,414] Totol epochs: 15
[2022-08-08 18:47:51,418] Epoch 1---------------
[2022-08-08 18:47:51,419] lr: 2.000000e-03
[2022-08-08 18:47:52,733] loss: 2.328917  [    0/ 4705]
[2022-08-08 18:48:12,407] loss: 2.062680  [  480/ 4705]
[2022-08-08 18:48:32,114] loss: 1.698655  [  960/ 4705]
[2022-08-08 18:48:51,788] loss: 1.693056  [ 1440/ 4705]
[2022-08-08 18:49:11,462] loss: 1.479097  [ 1920/ 4705]
[2022-08-08 18:49:31,133] loss: 1.382821  [ 2400/ 4705]
[2022-08-08 18:49:50,805] loss: 1.480945  [ 2880/ 4705]
[2022-08-08 18:50:10,477] loss: 1.026620  [ 3360/ 4705]
[2022-08-08 18:50:30,151] loss: 0.899177  [ 3840/ 4705]
[2022-08-08 18:50:49,824] loss: 0.601272  [ 4320/ 4705]
[2022-08-08 18:52:16,529] Train Error: Accuracy: 64.803%, Avg loss: 1.263981
[2022-08-08 18:52:48,420] Test  Error: Accuracy: 63.234%, Avg loss: 1.294476
[2022-08-08 18:52:48,420] Epoch 2---------------
[2022-08-08 18:52:48,421] lr: 1.900000e-03
[2022-08-08 18:52:49,735] loss: 1.307094  [    0/ 4705]
[2022-08-08 18:53:09,406] loss: 1.219694  [  480/ 4705]
[2022-08-08 18:53:29,080] loss: 0.319499  [  960/ 4705]
[2022-08-08 18:53:48,751] loss: 0.428692  [ 1440/ 4705]
[2022-08-08 18:54:08,424] loss: 0.351859  [ 1920/ 4705]
[2022-08-08 18:54:28,096] loss: 0.235136  [ 2400/ 4705]
[2022-08-08 18:54:47,767] loss: 0.528461  [ 2880/ 4705]
[2022-08-08 18:55:07,438] loss: 0.378442  [ 3360/ 4705]
[2022-08-08 18:55:27,110] loss: 0.156695  [ 3840/ 4705]
[2022-08-08 18:55:46,783] loss: 0.348023  [ 4320/ 4705]
[2022-08-08 18:57:13,485] Train Error: Accuracy: 87.396%, Avg loss: 0.321957
[2022-08-08 18:57:45,401] Test  Error: Accuracy: 86.141%, Avg loss: 0.371221
[2022-08-08 18:57:45,402] Epoch 3---------------
[2022-08-08 18:57:45,403] lr: 1.805000e-03
[2022-08-08 18:57:46,717] loss: 0.626517  [    0/ 4705]
[2022-08-08 18:58:06,394] loss: 0.092177  [  480/ 4705]
[2022-08-08 18:58:26,072] loss: 0.440998  [  960/ 4705]
[2022-08-08 18:58:45,746] loss: 0.176880  [ 1440/ 4705]
[2022-08-08 18:59:05,418] loss: 0.129322  [ 1920/ 4705]
[2022-08-08 18:59:25,090] loss: 0.089653  [ 2400/ 4705]
[2022-08-08 18:59:44,763] loss: 0.317793  [ 2880/ 4705]
[2022-08-08 19:00:04,435] loss: 0.248018  [ 3360/ 4705]
[2022-08-08 19:00:24,107] loss: 0.301079  [ 3840/ 4705]
[2022-08-08 19:00:43,781] loss: 0.052271  [ 4320/ 4705]
[2022-08-08 19:02:10,472] Train Error: Accuracy: 95.133%, Avg loss: 0.152346
[2022-08-08 19:02:42,364] Test  Error: Accuracy: 94.033%, Avg loss: 0.186307
[2022-08-08 19:02:42,364] Epoch 4---------------
[2022-08-08 19:02:42,365] lr: 1.714750e-03
[2022-08-08 19:02:43,680] loss: 0.158504  [    0/ 4705]
[2022-08-08 19:03:03,353] loss: 0.022235  [  480/ 4705]
[2022-08-08 19:03:23,027] loss: 0.126598  [  960/ 4705]
[2022-08-08 19:03:42,699] loss: 0.051826  [ 1440/ 4705]
[2022-08-08 19:04:02,373] loss: 0.205765  [ 1920/ 4705]
[2022-08-08 19:04:22,049] loss: 0.315660  [ 2400/ 4705]
[2022-08-08 19:04:41,727] loss: 0.127557  [ 2880/ 4705]
[2022-08-08 19:05:01,405] loss: 0.125964  [ 3360/ 4705]
[2022-08-08 19:05:21,081] loss: 0.048999  [ 3840/ 4705]
[2022-08-08 19:05:40,756] loss: 0.096000  [ 4320/ 4705]
[2022-08-08 19:07:07,461] Train Error: Accuracy: 96.302%, Avg loss: 0.108192
[2022-08-08 19:07:39,356] Test  Error: Accuracy: 96.150%, Avg loss: 0.128687
[2022-08-08 19:07:39,357] Epoch 5---------------
[2022-08-08 19:07:39,358] lr: 1.629012e-03
[2022-08-08 19:07:40,670] loss: 0.049219  [    0/ 4705]
[2022-08-08 19:08:00,345] loss: 0.038393  [  480/ 4705]
[2022-08-08 19:08:20,018] loss: 0.033359  [  960/ 4705]
[2022-08-08 19:08:39,692] loss: 0.055811  [ 1440/ 4705]
[2022-08-08 19:08:59,365] loss: 0.189755  [ 1920/ 4705]
[2022-08-08 19:09:19,039] loss: 0.037641  [ 2400/ 4705]
[2022-08-08 19:09:38,713] loss: 0.122524  [ 2880/ 4705]
[2022-08-08 19:09:58,386] loss: 0.125032  [ 3360/ 4705]
[2022-08-08 19:10:18,060] loss: 0.139909  [ 3840/ 4705]
[2022-08-08 19:10:37,732] loss: 0.025483  [ 4320/ 4705]
[2022-08-08 19:12:04,447] Train Error: Accuracy: 95.728%, Avg loss: 0.136950
[2022-08-08 19:12:36,345] Test  Error: Accuracy: 94.466%, Avg loss: 0.169373
[2022-08-08 19:12:36,345] Epoch 6---------------
[2022-08-08 19:12:36,346] lr: 1.137600e-03
[2022-08-08 19:12:37,661] loss: 0.164657  [    0/ 4705]
[2022-08-08 19:12:57,333] loss: 0.207599  [  480/ 4705]
[2022-08-08 19:13:17,004] loss: 0.061869  [  960/ 4705]
[2022-08-08 19:13:36,676] loss: 0.087708  [ 1440/ 4705]
[2022-08-08 19:13:56,349] loss: 0.012892  [ 1920/ 4705]
[2022-08-08 19:14:16,020] loss: 0.014992  [ 2400/ 4705]
[2022-08-08 19:14:35,692] loss: 0.009266  [ 2880/ 4705]
[2022-08-08 19:14:55,365] loss: 0.342477  [ 3360/ 4705]
[2022-08-08 19:15:15,035] loss: 0.052743  [ 3840/ 4705]
[2022-08-08 19:15:34,708] loss: 0.103367  [ 4320/ 4705]
[2022-08-08 19:17:01,429] Train Error: Accuracy: 98.385%, Avg loss: 0.052667
[2022-08-08 19:17:33,325] Test  Error: Accuracy: 97.786%, Avg loss: 0.083331
[2022-08-08 19:17:33,326] Epoch 7---------------
[2022-08-08 19:17:33,327] lr: 1.080720e-03
[2022-08-08 19:17:34,640] loss: 0.025943  [    0/ 4705]
[2022-08-08 19:17:54,314] loss: 0.011795  [  480/ 4705]
[2022-08-08 19:18:13,984] loss: 0.138000  [  960/ 4705]
[2022-08-08 19:18:33,655] loss: 0.050043  [ 1440/ 4705]
[2022-08-08 19:18:53,330] loss: 0.061215  [ 1920/ 4705]
[2022-08-08 19:19:13,002] loss: 0.089319  [ 2400/ 4705]
[2022-08-08 19:19:32,673] loss: 0.053940  [ 2880/ 4705]
[2022-08-08 19:19:52,347] loss: 0.002578  [ 3360/ 4705]
[2022-08-08 19:20:12,019] loss: 0.013911  [ 3840/ 4705]
[2022-08-08 19:20:31,692] loss: 0.085618  [ 4320/ 4705]
[2022-08-08 19:21:58,444] Train Error: Accuracy: 99.065%, Avg loss: 0.033365
[2022-08-08 19:22:30,361] Test  Error: Accuracy: 98.556%, Avg loss: 0.068235
[2022-08-08 19:22:30,361] Epoch 8---------------
[2022-08-08 19:22:30,363] lr: 1.026684e-03
[2022-08-08 19:22:31,677] loss: 0.025234  [    0/ 4705]
[2022-08-08 19:22:51,349] loss: 0.044329  [  480/ 4705]
[2022-08-08 19:23:11,024] loss: 0.024215  [  960/ 4705]
[2022-08-08 19:23:30,699] loss: 0.059723  [ 1440/ 4705]
[2022-08-08 19:23:50,375] loss: 0.009996  [ 1920/ 4705]
[2022-08-08 19:24:10,049] loss: 0.071602  [ 2400/ 4705]
[2022-08-08 19:24:29,724] loss: 0.046874  [ 2880/ 4705]
[2022-08-08 19:24:49,425] loss: 0.068377  [ 3360/ 4705]
[2022-08-08 19:25:09,099] loss: 0.104880  [ 3840/ 4705]
[2022-08-08 19:25:28,774] loss: 0.015529  [ 4320/ 4705]
[2022-08-08 19:26:55,525] Train Error: Accuracy: 99.384%, Avg loss: 0.023062
[2022-08-08 19:27:27,441] Test  Error: Accuracy: 98.508%, Avg loss: 0.061525
[2022-08-08 19:27:27,442] Epoch 9---------------
[2022-08-08 19:27:27,443] lr: 9.753500e-04
[2022-08-08 19:27:28,757] loss: 0.058851  [    0/ 4705]
[2022-08-08 19:27:48,432] loss: 0.006323  [  480/ 4705]
[2022-08-08 19:28:08,109] loss: 0.005012  [  960/ 4705]
[2022-08-08 19:28:27,785] loss: 0.002544  [ 1440/ 4705]
[2022-08-08 19:28:47,462] loss: 0.037332  [ 1920/ 4705]
[2022-08-08 19:29:07,137] loss: 0.015817  [ 2400/ 4705]
[2022-08-08 19:29:26,813] loss: 0.151081  [ 2880/ 4705]
[2022-08-08 19:29:46,489] loss: 0.002095  [ 3360/ 4705]
[2022-08-08 19:30:06,165] loss: 0.012065  [ 3840/ 4705]
[2022-08-08 19:30:25,841] loss: 0.050526  [ 4320/ 4705]
[2022-08-08 19:31:52,580] Train Error: Accuracy: 99.447%, Avg loss: 0.020278
[2022-08-08 19:32:24,495] Test  Error: Accuracy: 99.278%, Avg loss: 0.038947
[2022-08-08 19:32:24,496] Epoch 10---------------
[2022-08-08 19:32:24,497] lr: 9.265825e-04
[2022-08-08 19:32:25,811] loss: 0.005825  [    0/ 4705]
[2022-08-08 19:32:45,486] loss: 0.012073  [  480/ 4705]
[2022-08-08 19:33:05,161] loss: 0.046751  [  960/ 4705]
[2022-08-08 19:33:24,839] loss: 0.026274  [ 1440/ 4705]
[2022-08-08 19:33:44,513] loss: 0.037156  [ 1920/ 4705]
[2022-08-08 19:34:04,187] loss: 0.022034  [ 2400/ 4705]
[2022-08-08 19:34:23,861] loss: 0.001902  [ 2880/ 4705]
[2022-08-08 19:34:43,537] loss: 0.018033  [ 3360/ 4705]
[2022-08-08 19:35:03,210] loss: 0.004802  [ 3840/ 4705]
[2022-08-08 19:35:22,883] loss: 0.033549  [ 4320/ 4705]
[2022-08-08 19:36:49,622] Train Error: Accuracy: 99.044%, Avg loss: 0.037148
[2022-08-08 19:37:21,532] Test  Error: Accuracy: 98.556%, Avg loss: 0.061360
[2022-08-08 19:37:21,533] Epoch 11---------------
[2022-08-08 19:37:21,534] lr: 6.470671e-04
[2022-08-08 19:37:22,849] loss: 0.017549  [    0/ 4705]
[2022-08-08 19:37:42,525] loss: 0.018491  [  480/ 4705]
[2022-08-08 19:38:02,200] loss: 0.014592  [  960/ 4705]
[2022-08-08 19:38:21,876] loss: 0.020568  [ 1440/ 4705]
[2022-08-08 19:38:41,550] loss: 0.006873  [ 1920/ 4705]
[2022-08-08 19:39:01,227] loss: 0.002211  [ 2400/ 4705]
[2022-08-08 19:39:20,901] loss: 0.004607  [ 2880/ 4705]
[2022-08-08 19:39:40,578] loss: 0.001688  [ 3360/ 4705]
[2022-08-08 19:40:00,254] loss: 0.006276  [ 3840/ 4705]
[2022-08-08 19:40:19,929] loss: 0.008951  [ 4320/ 4705]
[2022-08-08 19:41:46,676] Train Error: Accuracy: 99.681%, Avg loss: 0.010494
[2022-08-08 19:42:18,585] Test  Error: Accuracy: 99.278%, Avg loss: 0.037031
[2022-08-08 19:42:18,585] Epoch 12---------------
[2022-08-08 19:42:18,586] lr: 6.147137e-04
[2022-08-08 19:42:19,900] loss: 0.001418  [    0/ 4705]
[2022-08-08 19:42:39,576] loss: 0.007645  [  480/ 4705]
[2022-08-08 19:42:59,252] loss: 0.001589  [  960/ 4705]
[2022-08-08 19:43:18,929] loss: 0.047577  [ 1440/ 4705]
[2022-08-08 19:43:38,604] loss: 0.004224  [ 1920/ 4705]
[2022-08-08 19:43:58,280] loss: 0.003118  [ 2400/ 4705]
[2022-08-08 19:44:17,954] loss: 0.008452  [ 2880/ 4705]
[2022-08-08 19:44:37,629] loss: 0.008691  [ 3360/ 4705]
[2022-08-08 19:44:57,304] loss: 0.001056  [ 3840/ 4705]
[2022-08-08 19:45:16,979] loss: 0.020914  [ 4320/ 4705]
[2022-08-08 19:46:43,723] Train Error: Accuracy: 99.469%, Avg loss: 0.016385
[2022-08-08 19:47:15,630] Test  Error: Accuracy: 98.701%, Avg loss: 0.054627
[2022-08-08 19:47:15,631] Epoch 13---------------
[2022-08-08 19:47:15,632] lr: 4.292775e-04
[2022-08-08 19:47:16,946] loss: 0.003826  [    0/ 4705]
[2022-08-08 19:47:36,624] loss: 0.001793  [  480/ 4705]
[2022-08-08 19:47:56,301] loss: 0.017621  [  960/ 4705]
[2022-08-08 19:48:15,977] loss: 0.003573  [ 1440/ 4705]
[2022-08-08 19:48:35,652] loss: 0.032080  [ 1920/ 4705]
[2022-08-08 19:48:55,328] loss: 0.008278  [ 2400/ 4705]
[2022-08-08 19:49:15,004] loss: 0.014158  [ 2880/ 4705]
[2022-08-08 19:49:34,682] loss: 0.002203  [ 3360/ 4705]
[2022-08-08 19:49:54,357] loss: 0.001841  [ 3840/ 4705]
[2022-08-08 19:50:14,033] loss: 0.001581  [ 4320/ 4705]
[2022-08-08 19:51:40,775] Train Error: Accuracy: 99.936%, Avg loss: 0.005486
[2022-08-08 19:52:12,683] Test  Error: Accuracy: 99.086%, Avg loss: 0.040735
[2022-08-08 19:52:12,683] Epoch 14---------------
[2022-08-08 19:52:12,685] lr: 4.078137e-04
[2022-08-08 19:52:14,000] loss: 0.001013  [    0/ 4705]
[2022-08-08 19:52:33,675] loss: 0.002404  [  480/ 4705]
[2022-08-08 19:52:53,349] loss: 0.001117  [  960/ 4705]
[2022-08-08 19:53:13,026] loss: 0.003714  [ 1440/ 4705]
[2022-08-08 19:53:32,700] loss: 0.009640  [ 1920/ 4705]
[2022-08-08 19:53:52,376] loss: 0.001265  [ 2400/ 4705]
[2022-08-08 19:54:12,052] loss: 0.013256  [ 2880/ 4705]
[2022-08-08 19:54:31,728] loss: 0.006524  [ 3360/ 4705]
[2022-08-08 19:54:51,405] loss: 0.000495  [ 3840/ 4705]
[2022-08-08 19:55:11,078] loss: 0.002972  [ 4320/ 4705]
[2022-08-08 19:56:37,825] Train Error: Accuracy: 93.879%, Avg loss: 0.239395
[2022-08-08 19:57:09,738] Test  Error: Accuracy: 92.733%, Avg loss: 0.313052
[2022-08-08 19:57:09,738] Epoch 15---------------
[2022-08-08 19:57:09,739] lr: 2.847915e-04
[2022-08-08 19:57:11,054] loss: 0.001311  [    0/ 4705]
[2022-08-08 19:57:30,731] loss: 0.007929  [  480/ 4705]
[2022-08-08 19:57:50,407] loss: 0.001860  [  960/ 4705]
[2022-08-08 19:58:10,082] loss: 0.009140  [ 1440/ 4705]
[2022-08-08 19:58:29,756] loss: 0.005503  [ 1920/ 4705]
[2022-08-08 19:58:49,430] loss: 0.000797  [ 2400/ 4705]
[2022-08-08 19:59:09,105] loss: 0.001643  [ 2880/ 4705]
[2022-08-08 19:59:28,780] loss: 0.001494  [ 3360/ 4705]
[2022-08-08 19:59:48,454] loss: 0.002254  [ 3840/ 4705]
[2022-08-08 20:00:08,128] loss: 0.001427  [ 4320/ 4705]
[2022-08-08 20:01:34,878] Train Error: Accuracy: 99.872%, Avg loss: 0.004569
[2022-08-08 20:02:06,788] Test  Error: Accuracy: 99.326%, Avg loss: 0.038771
[2022-08-08 20:02:06,789] Done!
[2022-08-08 20:02:06,796] Number of parameters:2443018
[2022-08-08 20:02:06,797] ## end time: 2022-08-08 20:02:06.789853
[2022-08-08 20:02:06,797] ## used time: 1:14:15.875593
