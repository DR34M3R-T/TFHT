[2022-08-08 02:18:50,161] ## start time: 2022-08-08 02:18:50.017831
[2022-08-08 02:18:50,162] Using cuda device
[2022-08-08 02:18:50,164] In train:p&d10.npy.
[2022-08-08 02:18:50,165] One Channel
[2022-08-08 02:18:50,166] With Normal data.
[2022-08-08 02:18:50,166] Nunber of classes:10.
[2022-08-08 02:18:50,167] Nunber of ViT channels:1.
[2022-08-08 02:18:50,407] Totol epochs: 15
[2022-08-08 02:18:50,410] Epoch 1---------------
[2022-08-08 02:18:50,410] lr: 2.000000e-03
[2022-08-08 02:18:50,472] loss: 2.587569  [    0/ 4737]
[2022-08-08 02:18:51,379] loss: 1.689489  [  480/ 4737]
[2022-08-08 02:18:52,283] loss: 1.425056  [  960/ 4737]
[2022-08-08 02:18:53,188] loss: 1.281097  [ 1440/ 4737]
[2022-08-08 02:18:54,092] loss: 0.437319  [ 1920/ 4737]
[2022-08-08 02:18:54,995] loss: 0.196569  [ 2400/ 4737]
[2022-08-08 02:18:55,898] loss: 0.117690  [ 2880/ 4737]
[2022-08-08 02:18:56,802] loss: 0.108714  [ 3360/ 4737]
[2022-08-08 02:18:57,706] loss: 0.399427  [ 3840/ 4737]
[2022-08-08 02:18:58,609] loss: 0.013759  [ 4320/ 4737]
[2022-08-08 02:19:02,048] Train Error: Accuracy: 99.873%, Avg loss: 0.016144
[2022-08-08 02:19:03,204] Test  Error: Accuracy: 99.658%, Avg loss: 0.019512
[2022-08-08 02:19:03,204] Epoch 2---------------
[2022-08-08 02:19:03,205] lr: 1.900000e-03
[2022-08-08 02:19:03,268] loss: 0.008202  [    0/ 4737]
[2022-08-08 02:19:04,170] loss: 0.014193  [  480/ 4737]
[2022-08-08 02:19:05,074] loss: 0.004564  [  960/ 4737]
[2022-08-08 02:19:05,976] loss: 0.018501  [ 1440/ 4737]
[2022-08-08 02:19:06,879] loss: 0.003984  [ 1920/ 4737]
[2022-08-08 02:19:07,780] loss: 0.003853  [ 2400/ 4737]
[2022-08-08 02:19:08,683] loss: 0.004191  [ 2880/ 4737]
[2022-08-08 02:19:09,585] loss: 0.045861  [ 3360/ 4737]
[2022-08-08 02:19:10,488] loss: 0.722448  [ 3840/ 4737]
[2022-08-08 02:19:11,394] loss: 0.079901  [ 4320/ 4737]
[2022-08-08 02:19:14,832] Train Error: Accuracy: 99.578%, Avg loss: 0.028268
[2022-08-08 02:19:15,988] Test  Error: Accuracy: 98.974%, Avg loss: 0.036055
[2022-08-08 02:19:15,989] Epoch 3---------------
[2022-08-08 02:19:15,990] lr: 1.326841e-03
[2022-08-08 02:19:16,052] loss: 0.018455  [    0/ 4737]
[2022-08-08 02:19:16,956] loss: 0.009370  [  480/ 4737]
[2022-08-08 02:19:17,860] loss: 0.011477  [  960/ 4737]
[2022-08-08 02:19:18,764] loss: 0.009355  [ 1440/ 4737]
[2022-08-08 02:19:19,667] loss: 0.006481  [ 1920/ 4737]
[2022-08-08 02:19:20,570] loss: 0.004035  [ 2400/ 4737]
[2022-08-08 02:19:21,474] loss: 0.005690  [ 2880/ 4737]
[2022-08-08 02:19:22,378] loss: 0.010178  [ 3360/ 4737]
[2022-08-08 02:19:23,282] loss: 0.004909  [ 3840/ 4737]
[2022-08-08 02:19:24,185] loss: 0.003362  [ 4320/ 4737]
[2022-08-08 02:19:27,625] Train Error: Accuracy: 100.000%, Avg loss: 0.003353
[2022-08-08 02:19:28,780] Test  Error: Accuracy: 100.000%, Avg loss: 0.004020
[2022-08-08 02:19:28,780] Epoch 4---------------
[2022-08-08 02:19:28,782] lr: 1.260499e-03
[2022-08-08 02:19:28,844] loss: 0.003260  [    0/ 4737]
[2022-08-08 02:19:29,747] loss: 0.002044  [  480/ 4737]
[2022-08-08 02:19:30,649] loss: 0.001438  [  960/ 4737]
[2022-08-08 02:19:31,551] loss: 0.001904  [ 1440/ 4737]
[2022-08-08 02:19:32,454] loss: 0.002559  [ 1920/ 4737]
[2022-08-08 02:19:33,357] loss: 0.001867  [ 2400/ 4737]
[2022-08-08 02:19:34,258] loss: 0.001789  [ 2880/ 4737]
[2022-08-08 02:19:35,162] loss: 0.001342  [ 3360/ 4737]
[2022-08-08 02:19:36,064] loss: 0.000880  [ 3840/ 4737]
[2022-08-08 02:19:36,967] loss: 0.001483  [ 4320/ 4737]
[2022-08-08 02:19:40,404] Train Error: Accuracy: 99.979%, Avg loss: 0.002079
[2022-08-08 02:19:41,560] Test  Error: Accuracy: 99.951%, Avg loss: 0.002677
[2022-08-08 02:19:41,560] Epoch 5---------------
[2022-08-08 02:19:41,561] lr: 1.197474e-03
[2022-08-08 02:19:41,624] loss: 0.001315  [    0/ 4737]
[2022-08-08 02:19:42,527] loss: 0.001406  [  480/ 4737]
[2022-08-08 02:19:43,431] loss: 0.001026  [  960/ 4737]
[2022-08-08 02:19:44,335] loss: 0.000950  [ 1440/ 4737]
[2022-08-08 02:19:45,238] loss: 0.001194  [ 1920/ 4737]
[2022-08-08 02:19:46,142] loss: 0.000667  [ 2400/ 4737]
[2022-08-08 02:19:47,045] loss: 0.000906  [ 2880/ 4737]
[2022-08-08 02:19:47,949] loss: 0.001340  [ 3360/ 4737]
[2022-08-08 02:19:48,854] loss: 0.000859  [ 3840/ 4737]
[2022-08-08 02:19:49,757] loss: 0.001090  [ 4320/ 4737]
[2022-08-08 02:19:53,195] Train Error: Accuracy: 100.000%, Avg loss: 0.000922
[2022-08-08 02:19:54,350] Test  Error: Accuracy: 100.000%, Avg loss: 0.001265
[2022-08-08 02:19:54,350] Epoch 6---------------
[2022-08-08 02:19:54,351] lr: 1.137600e-03
[2022-08-08 02:19:54,414] loss: 0.000905  [    0/ 4737]
[2022-08-08 02:19:55,316] loss: 0.000770  [  480/ 4737]
[2022-08-08 02:19:56,218] loss: 0.000678  [  960/ 4737]
[2022-08-08 02:19:57,121] loss: 0.000683  [ 1440/ 4737]
[2022-08-08 02:19:58,023] loss: 0.000598  [ 1920/ 4737]
[2022-08-08 02:19:58,926] loss: 0.000718  [ 2400/ 4737]
[2022-08-08 02:19:59,830] loss: 0.000664  [ 2880/ 4737]
[2022-08-08 02:20:00,733] loss: 0.000899  [ 3360/ 4737]
[2022-08-08 02:20:01,637] loss: 0.001002  [ 3840/ 4737]
[2022-08-08 02:20:02,538] loss: 0.000908  [ 4320/ 4737]
[2022-08-08 02:20:05,982] Train Error: Accuracy: 100.000%, Avg loss: 0.000710
[2022-08-08 02:20:07,148] Test  Error: Accuracy: 100.000%, Avg loss: 0.000899
[2022-08-08 02:20:07,149] Epoch 7---------------
[2022-08-08 02:20:07,150] lr: 1.080720e-03
[2022-08-08 02:20:07,212] loss: 0.000640  [    0/ 4737]
[2022-08-08 02:20:08,116] loss: 0.000506  [  480/ 4737]
[2022-08-08 02:20:09,018] loss: 0.000613  [  960/ 4737]
[2022-08-08 02:20:09,923] loss: 0.000619  [ 1440/ 4737]
[2022-08-08 02:20:10,826] loss: 0.000529  [ 1920/ 4737]
[2022-08-08 02:20:11,730] loss: 0.000844  [ 2400/ 4737]
[2022-08-08 02:20:12,633] loss: 0.000523  [ 2880/ 4737]
[2022-08-08 02:20:13,538] loss: 0.000761  [ 3360/ 4737]
[2022-08-08 02:20:14,441] loss: 0.000554  [ 3840/ 4737]
[2022-08-08 02:20:15,346] loss: 0.001284  [ 4320/ 4737]
[2022-08-08 02:20:18,787] Train Error: Accuracy: 99.979%, Avg loss: 0.001062
[2022-08-08 02:20:19,944] Test  Error: Accuracy: 99.902%, Avg loss: 0.003540
[2022-08-08 02:20:19,944] Epoch 8---------------
[2022-08-08 02:20:19,945] lr: 7.547072e-04
[2022-08-08 02:20:20,008] loss: 0.002261  [    0/ 4737]
[2022-08-08 02:20:20,911] loss: 0.000587  [  480/ 4737]
[2022-08-08 02:20:21,814] loss: 0.000600  [  960/ 4737]
[2022-08-08 02:20:22,717] loss: 0.000505  [ 1440/ 4737]
[2022-08-08 02:20:23,619] loss: 0.000780  [ 1920/ 4737]
[2022-08-08 02:20:24,521] loss: 0.000602  [ 2400/ 4737]
[2022-08-08 02:20:25,425] loss: 0.000456  [ 2880/ 4737]
[2022-08-08 02:20:26,326] loss: 0.000390  [ 3360/ 4737]
[2022-08-08 02:20:27,228] loss: 0.000476  [ 3840/ 4737]
[2022-08-08 02:20:28,130] loss: 0.000670  [ 4320/ 4737]
[2022-08-08 02:20:31,568] Train Error: Accuracy: 100.000%, Avg loss: 0.000514
[2022-08-08 02:20:32,726] Test  Error: Accuracy: 100.000%, Avg loss: 0.000801
[2022-08-08 02:20:32,726] Epoch 9---------------
[2022-08-08 02:20:32,727] lr: 7.169718e-04
[2022-08-08 02:20:32,790] loss: 0.000463  [    0/ 4737]
[2022-08-08 02:20:33,694] loss: 0.000596  [  480/ 4737]
[2022-08-08 02:20:34,597] loss: 0.000446  [  960/ 4737]
[2022-08-08 02:20:35,500] loss: 0.000663  [ 1440/ 4737]
[2022-08-08 02:20:36,404] loss: 0.000561  [ 1920/ 4737]
[2022-08-08 02:20:37,307] loss: 0.000738  [ 2400/ 4737]
[2022-08-08 02:20:38,211] loss: 0.000398  [ 2880/ 4737]
[2022-08-08 02:20:39,114] loss: 0.000947  [ 3360/ 4737]
[2022-08-08 02:20:40,017] loss: 0.000517  [ 3840/ 4737]
[2022-08-08 02:20:40,921] loss: 0.000398  [ 4320/ 4737]
[2022-08-08 02:20:44,359] Train Error: Accuracy: 100.000%, Avg loss: 0.000500
[2022-08-08 02:20:45,514] Test  Error: Accuracy: 100.000%, Avg loss: 0.000633
[2022-08-08 02:20:45,514] Epoch 10---------------
[2022-08-08 02:20:45,515] lr: 6.811233e-04
[2022-08-08 02:20:45,578] loss: 0.000443  [    0/ 4737]
[2022-08-08 02:20:46,481] loss: 0.000419  [  480/ 4737]
[2022-08-08 02:20:47,383] loss: 0.000288  [  960/ 4737]
[2022-08-08 02:20:48,286] loss: 0.000319  [ 1440/ 4737]
[2022-08-08 02:20:49,187] loss: 0.002584  [ 1920/ 4737]
[2022-08-08 02:20:50,089] loss: 0.000452  [ 2400/ 4737]
[2022-08-08 02:20:50,993] loss: 0.000555  [ 2880/ 4737]
[2022-08-08 02:20:51,896] loss: 0.000324  [ 3360/ 4737]
[2022-08-08 02:20:52,799] loss: 0.000374  [ 3840/ 4737]
[2022-08-08 02:20:53,701] loss: 0.000353  [ 4320/ 4737]
[2022-08-08 02:20:57,148] Train Error: Accuracy: 100.000%, Avg loss: 0.000471
[2022-08-08 02:20:58,306] Test  Error: Accuracy: 100.000%, Avg loss: 0.000780
[2022-08-08 02:20:58,307] Epoch 11---------------
[2022-08-08 02:20:58,308] lr: 5.270402e-04
[2022-08-08 02:20:58,370] loss: 0.000493  [    0/ 4737]
[2022-08-08 02:20:59,273] loss: 0.000479  [  480/ 4737]
[2022-08-08 02:21:00,178] loss: 0.000543  [  960/ 4737]
[2022-08-08 02:21:01,081] loss: 0.000395  [ 1440/ 4737]
[2022-08-08 02:21:01,985] loss: 0.000532  [ 1920/ 4737]
[2022-08-08 02:21:02,888] loss: 0.000355  [ 2400/ 4737]
[2022-08-08 02:21:03,792] loss: 0.000311  [ 2880/ 4737]
[2022-08-08 02:21:04,696] loss: 0.000410  [ 3360/ 4737]
[2022-08-08 02:21:05,600] loss: 0.000568  [ 3840/ 4737]
[2022-08-08 02:21:06,503] loss: 0.000602  [ 4320/ 4737]
[2022-08-08 02:21:09,943] Train Error: Accuracy: 100.000%, Avg loss: 0.000465
[2022-08-08 02:21:11,102] Test  Error: Accuracy: 100.000%, Avg loss: 0.000700
[2022-08-08 02:21:11,102] Epoch 12---------------
[2022-08-08 02:21:11,103] lr: 5.006882e-04
[2022-08-08 02:21:11,166] loss: 0.000444  [    0/ 4737]
[2022-08-08 02:21:12,069] loss: 0.000456  [  480/ 4737]
[2022-08-08 02:21:12,971] loss: 0.000300  [  960/ 4737]
[2022-08-08 02:21:13,875] loss: 0.000386  [ 1440/ 4737]
[2022-08-08 02:21:14,777] loss: 0.000516  [ 1920/ 4737]
[2022-08-08 02:21:15,680] loss: 0.000358  [ 2400/ 4737]
[2022-08-08 02:21:16,583] loss: 0.000339  [ 2880/ 4737]
[2022-08-08 02:21:17,485] loss: 0.000603  [ 3360/ 4737]
[2022-08-08 02:21:18,389] loss: 0.000379  [ 3840/ 4737]
[2022-08-08 02:21:19,291] loss: 0.000385  [ 4320/ 4737]
[2022-08-08 02:21:22,758] Train Error: Accuracy: 100.000%, Avg loss: 0.000493
[2022-08-08 02:21:23,930] Test  Error: Accuracy: 100.000%, Avg loss: 0.000564
[2022-08-08 02:21:23,931] Epoch 13---------------
[2022-08-08 02:21:23,932] lr: 4.756538e-04
[2022-08-08 02:21:23,995] loss: 0.000506  [    0/ 4737]
[2022-08-08 02:21:24,898] loss: 0.000429  [  480/ 4737]
[2022-08-08 02:21:25,802] loss: 0.000424  [  960/ 4737]
[2022-08-08 02:21:26,705] loss: 0.000362  [ 1440/ 4737]
[2022-08-08 02:21:27,608] loss: 0.000435  [ 1920/ 4737]
[2022-08-08 02:21:28,512] loss: 0.000409  [ 2400/ 4737]
[2022-08-08 02:21:29,419] loss: 0.000325  [ 2880/ 4737]
[2022-08-08 02:21:30,323] loss: 0.000369  [ 3360/ 4737]
[2022-08-08 02:21:31,226] loss: 0.000295  [ 3840/ 4737]
[2022-08-08 02:21:32,130] loss: 0.000465  [ 4320/ 4737]
[2022-08-08 02:21:35,581] Train Error: Accuracy: 100.000%, Avg loss: 0.000456
[2022-08-08 02:21:36,737] Test  Error: Accuracy: 100.000%, Avg loss: 0.000737
[2022-08-08 02:21:36,738] Epoch 14---------------
[2022-08-08 02:21:36,739] lr: 3.321668e-04
[2022-08-08 02:21:36,801] loss: 0.000911  [    0/ 4737]
[2022-08-08 02:21:37,704] loss: 0.000388  [  480/ 4737]
[2022-08-08 02:21:38,607] loss: 0.000576  [  960/ 4737]
[2022-08-08 02:21:39,509] loss: 0.000347  [ 1440/ 4737]
[2022-08-08 02:21:40,412] loss: 0.000355  [ 1920/ 4737]
[2022-08-08 02:21:41,314] loss: 0.000473  [ 2400/ 4737]
[2022-08-08 02:21:42,216] loss: 0.000358  [ 2880/ 4737]
[2022-08-08 02:21:43,118] loss: 0.000250  [ 3360/ 4737]
[2022-08-08 02:21:44,021] loss: 0.000459  [ 3840/ 4737]
[2022-08-08 02:21:44,923] loss: 0.000454  [ 4320/ 4737]
[2022-08-08 02:21:48,367] Train Error: Accuracy: 100.000%, Avg loss: 0.000435
[2022-08-08 02:21:49,531] Test  Error: Accuracy: 100.000%, Avg loss: 0.000952
[2022-08-08 02:21:49,531] Epoch 15---------------
[2022-08-08 02:21:49,532] lr: 2.319644e-04
[2022-08-08 02:21:49,594] loss: 0.000416  [    0/ 4737]
[2022-08-08 02:21:50,498] loss: 0.000307  [  480/ 4737]
[2022-08-08 02:21:51,403] loss: 0.000401  [  960/ 4737]
[2022-08-08 02:21:52,306] loss: 0.000448  [ 1440/ 4737]
[2022-08-08 02:21:53,209] loss: 0.000496  [ 1920/ 4737]
[2022-08-08 02:21:54,113] loss: 0.000561  [ 2400/ 4737]
[2022-08-08 02:21:55,016] loss: 0.000485  [ 2880/ 4737]
[2022-08-08 02:21:55,920] loss: 0.000657  [ 3360/ 4737]
[2022-08-08 02:21:56,823] loss: 0.000427  [ 3840/ 4737]
[2022-08-08 02:21:57,727] loss: 0.000322  [ 4320/ 4737]
[2022-08-08 02:22:01,177] Train Error: Accuracy: 100.000%, Avg loss: 0.000437
[2022-08-08 02:22:02,332] Test  Error: Accuracy: 100.000%, Avg loss: 0.000659
[2022-08-08 02:22:02,333] Done!
[2022-08-08 02:22:02,337] Number of parameters:3229450
[2022-08-08 02:22:02,337] ## end time: 2022-08-08 02:22:02.333141
[2022-08-08 02:22:02,337] ## used time: 0:03:12.315310
