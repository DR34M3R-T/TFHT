[2022-08-08 02:29:24,665] ## start time: 2022-08-08 02:29:24.516196
[2022-08-08 02:29:24,666] Using cuda device
[2022-08-08 02:29:24,667] In train:p&d10.npy.
[2022-08-08 02:29:24,669] One Channel
[2022-08-08 02:29:24,669] With Normal data.
[2022-08-08 02:29:24,670] Nunber of classes:10.
[2022-08-08 02:29:24,670] Nunber of ViT channels:1.
[2022-08-08 02:29:24,910] Totol epochs: 15
[2022-08-08 02:29:24,914] Epoch 1---------------
[2022-08-08 02:29:24,914] lr: 2.000000e-03
[2022-08-08 02:29:24,958] loss: 2.423751  [    0/ 4791]
[2022-08-08 02:29:25,573] loss: 1.465225  [  480/ 4791]
[2022-08-08 02:29:26,189] loss: 1.297972  [  960/ 4791]
[2022-08-08 02:29:26,794] loss: 0.587127  [ 1440/ 4791]
[2022-08-08 02:29:27,399] loss: 0.136700  [ 1920/ 4791]
[2022-08-08 02:29:28,007] loss: 0.065642  [ 2400/ 4791]
[2022-08-08 02:29:28,620] loss: 0.436298  [ 2880/ 4791]
[2022-08-08 02:29:29,226] loss: 0.153962  [ 3360/ 4791]
[2022-08-08 02:29:29,832] loss: 0.020730  [ 3840/ 4791]
[2022-08-08 02:29:30,438] loss: 0.074149  [ 4320/ 4791]
[2022-08-08 02:29:32,596] Train Error: Accuracy: 98.497%, Avg loss: 0.065767
[2022-08-08 02:29:33,267] Test  Error: Accuracy: 97.641%, Avg loss: 0.077363
[2022-08-08 02:29:33,268] Epoch 2---------------
[2022-08-08 02:29:33,269] lr: 1.900000e-03
[2022-08-08 02:29:33,313] loss: 0.039735  [    0/ 4791]
[2022-08-08 02:29:33,918] loss: 0.014658  [  480/ 4791]
[2022-08-08 02:29:34,519] loss: 0.008811  [  960/ 4791]
[2022-08-08 02:29:35,124] loss: 0.033552  [ 1440/ 4791]
[2022-08-08 02:29:35,727] loss: 0.006604  [ 1920/ 4791]
[2022-08-08 02:29:36,327] loss: 0.006612  [ 2400/ 4791]
[2022-08-08 02:29:36,930] loss: 0.011284  [ 2880/ 4791]
[2022-08-08 02:29:37,529] loss: 0.012082  [ 3360/ 4791]
[2022-08-08 02:29:38,131] loss: 0.006845  [ 3840/ 4791]
[2022-08-08 02:29:38,736] loss: 0.005131  [ 4320/ 4791]
[2022-08-08 02:29:40,899] Train Error: Accuracy: 100.000%, Avg loss: 0.003081
[2022-08-08 02:29:41,571] Test  Error: Accuracy: 99.950%, Avg loss: 0.004458
[2022-08-08 02:29:41,572] Epoch 3---------------
[2022-08-08 02:29:41,573] lr: 1.805000e-03
[2022-08-08 02:29:41,616] loss: 0.002554  [    0/ 4791]
[2022-08-08 02:29:42,221] loss: 0.002048  [  480/ 4791]
[2022-08-08 02:29:42,825] loss: 0.002672  [  960/ 4791]
[2022-08-08 02:29:43,428] loss: 0.001680  [ 1440/ 4791]
[2022-08-08 02:29:44,027] loss: 0.001851  [ 1920/ 4791]
[2022-08-08 02:29:44,631] loss: 0.001474  [ 2400/ 4791]
[2022-08-08 02:29:45,236] loss: 0.001062  [ 2880/ 4791]
[2022-08-08 02:29:45,839] loss: 0.001361  [ 3360/ 4791]
[2022-08-08 02:29:46,444] loss: 0.001325  [ 3840/ 4791]
[2022-08-08 02:29:47,045] loss: 0.001441  [ 4320/ 4791]
[2022-08-08 02:29:49,212] Train Error: Accuracy: 99.624%, Avg loss: 0.013822
[2022-08-08 02:29:49,902] Test  Error: Accuracy: 99.598%, Avg loss: 0.016317
[2022-08-08 02:29:49,902] Epoch 4---------------
[2022-08-08 02:29:49,903] lr: 1.260499e-03
[2022-08-08 02:29:49,946] loss: 0.193361  [    0/ 4791]
[2022-08-08 02:29:50,550] loss: 0.001026  [  480/ 4791]
[2022-08-08 02:29:51,152] loss: 0.001294  [  960/ 4791]
[2022-08-08 02:29:51,753] loss: 0.001443  [ 1440/ 4791]
[2022-08-08 02:29:52,353] loss: 0.001222  [ 1920/ 4791]
[2022-08-08 02:29:52,957] loss: 0.000977  [ 2400/ 4791]
[2022-08-08 02:29:53,557] loss: 0.000833  [ 2880/ 4791]
[2022-08-08 02:29:54,159] loss: 0.001210  [ 3360/ 4791]
[2022-08-08 02:29:54,764] loss: 0.000991  [ 3840/ 4791]
[2022-08-08 02:29:55,368] loss: 0.002401  [ 4320/ 4791]
[2022-08-08 02:29:57,556] Train Error: Accuracy: 100.000%, Avg loss: 0.001077
[2022-08-08 02:29:58,243] Test  Error: Accuracy: 100.000%, Avg loss: 0.001362
[2022-08-08 02:29:58,244] Epoch 5---------------
[2022-08-08 02:29:58,245] lr: 1.197474e-03
[2022-08-08 02:29:58,288] loss: 0.000928  [    0/ 4791]
[2022-08-08 02:29:58,900] loss: 0.001549  [  480/ 4791]
[2022-08-08 02:29:59,506] loss: 0.000735  [  960/ 4791]
[2022-08-08 02:30:00,109] loss: 1.096204  [ 1440/ 4791]
[2022-08-08 02:30:00,714] loss: 0.117047  [ 1920/ 4791]
[2022-08-08 02:30:01,319] loss: 0.018692  [ 2400/ 4791]
[2022-08-08 02:30:01,920] loss: 0.006271  [ 2880/ 4791]
[2022-08-08 02:30:02,521] loss: 0.009480  [ 3360/ 4791]
[2022-08-08 02:30:03,125] loss: 0.007498  [ 3840/ 4791]
[2022-08-08 02:30:03,731] loss: 0.005324  [ 4320/ 4791]
[2022-08-08 02:30:05,920] Train Error: Accuracy: 99.770%, Avg loss: 0.016986
[2022-08-08 02:30:06,619] Test  Error: Accuracy: 99.398%, Avg loss: 0.026681
[2022-08-08 02:30:06,619] Epoch 6---------------
[2022-08-08 02:30:06,620] lr: 8.362407e-04
[2022-08-08 02:30:06,662] loss: 0.015440  [    0/ 4791]
[2022-08-08 02:30:07,266] loss: 0.004319  [  480/ 4791]
[2022-08-08 02:30:07,871] loss: 0.002316  [  960/ 4791]
[2022-08-08 02:30:08,472] loss: 0.002209  [ 1440/ 4791]
[2022-08-08 02:30:09,076] loss: 0.013321  [ 1920/ 4791]
[2022-08-08 02:30:09,678] loss: 0.002275  [ 2400/ 4791]
[2022-08-08 02:30:10,279] loss: 0.001613  [ 2880/ 4791]
[2022-08-08 02:30:10,880] loss: 0.002218  [ 3360/ 4791]
[2022-08-08 02:30:11,486] loss: 0.001669  [ 3840/ 4791]
[2022-08-08 02:30:12,088] loss: 0.001447  [ 4320/ 4791]
[2022-08-08 02:30:14,276] Train Error: Accuracy: 100.000%, Avg loss: 0.002203
[2022-08-08 02:30:14,953] Test  Error: Accuracy: 99.900%, Avg loss: 0.003933
[2022-08-08 02:30:14,953] Epoch 7---------------
[2022-08-08 02:30:14,954] lr: 7.944286e-04
[2022-08-08 02:30:14,996] loss: 0.001667  [    0/ 4791]
[2022-08-08 02:30:15,603] loss: 0.001417  [  480/ 4791]
[2022-08-08 02:30:16,210] loss: 0.001858  [  960/ 4791]
[2022-08-08 02:30:16,813] loss: 0.001085  [ 1440/ 4791]
[2022-08-08 02:30:17,417] loss: 0.001549  [ 1920/ 4791]
[2022-08-08 02:30:18,019] loss: 0.001380  [ 2400/ 4791]
[2022-08-08 02:30:18,622] loss: 0.001127  [ 2880/ 4791]
[2022-08-08 02:30:19,223] loss: 0.001033  [ 3360/ 4791]
[2022-08-08 02:30:19,825] loss: 0.000960  [ 3840/ 4791]
[2022-08-08 02:30:20,429] loss: 0.001639  [ 4320/ 4791]
[2022-08-08 02:30:22,607] Train Error: Accuracy: 100.000%, Avg loss: 0.001280
[2022-08-08 02:30:23,288] Test  Error: Accuracy: 99.950%, Avg loss: 0.003383
[2022-08-08 02:30:23,288] Epoch 8---------------
[2022-08-08 02:30:23,289] lr: 7.547072e-04
[2022-08-08 02:30:23,332] loss: 0.001760  [    0/ 4791]
[2022-08-08 02:30:23,937] loss: 0.001112  [  480/ 4791]
[2022-08-08 02:30:24,540] loss: 0.000766  [  960/ 4791]
[2022-08-08 02:30:25,142] loss: 0.000785  [ 1440/ 4791]
[2022-08-08 02:30:25,744] loss: 0.000991  [ 1920/ 4791]
[2022-08-08 02:30:26,347] loss: 0.103581  [ 2400/ 4791]
[2022-08-08 02:30:26,951] loss: 0.006033  [ 2880/ 4791]
[2022-08-08 02:30:27,555] loss: 0.003078  [ 3360/ 4791]
[2022-08-08 02:30:28,155] loss: 0.004709  [ 3840/ 4791]
[2022-08-08 02:30:28,756] loss: 0.002101  [ 4320/ 4791]
[2022-08-08 02:30:30,927] Train Error: Accuracy: 99.541%, Avg loss: 0.013956
[2022-08-08 02:30:31,605] Test  Error: Accuracy: 99.548%, Avg loss: 0.014028
[2022-08-08 02:30:31,605] Epoch 9---------------
[2022-08-08 02:30:31,606] lr: 5.270402e-04
[2022-08-08 02:30:31,649] loss: 0.002654  [    0/ 4791]
[2022-08-08 02:30:32,253] loss: 0.001307  [  480/ 4791]
[2022-08-08 02:30:32,858] loss: 0.001446  [  960/ 4791]
[2022-08-08 02:30:33,461] loss: 0.013269  [ 1440/ 4791]
[2022-08-08 02:30:34,063] loss: 0.002707  [ 1920/ 4791]
[2022-08-08 02:30:34,666] loss: 0.002838  [ 2400/ 4791]
[2022-08-08 02:30:35,270] loss: 0.001088  [ 2880/ 4791]
[2022-08-08 02:30:35,872] loss: 0.001372  [ 3360/ 4791]
[2022-08-08 02:30:36,475] loss: 0.001387  [ 3840/ 4791]
[2022-08-08 02:30:37,086] loss: 0.002974  [ 4320/ 4791]
[2022-08-08 02:30:39,259] Train Error: Accuracy: 100.000%, Avg loss: 0.001805
[2022-08-08 02:30:39,935] Test  Error: Accuracy: 100.000%, Avg loss: 0.002187
[2022-08-08 02:30:39,935] Epoch 10---------------
[2022-08-08 02:30:39,936] lr: 5.006882e-04
[2022-08-08 02:30:39,978] loss: 0.001614  [    0/ 4791]
[2022-08-08 02:30:40,582] loss: 0.001116  [  480/ 4791]
[2022-08-08 02:30:41,184] loss: 0.000801  [  960/ 4791]
[2022-08-08 02:30:41,788] loss: 0.001159  [ 1440/ 4791]
[2022-08-08 02:30:42,393] loss: 0.000647  [ 1920/ 4791]
[2022-08-08 02:30:42,997] loss: 0.001083  [ 2400/ 4791]
[2022-08-08 02:30:43,601] loss: 0.000934  [ 2880/ 4791]
[2022-08-08 02:30:44,202] loss: 0.000469  [ 3360/ 4791]
[2022-08-08 02:30:44,804] loss: 0.000793  [ 3840/ 4791]
[2022-08-08 02:30:45,407] loss: 0.001051  [ 4320/ 4791]
[2022-08-08 02:30:47,575] Train Error: Accuracy: 100.000%, Avg loss: 0.001153
[2022-08-08 02:30:48,252] Test  Error: Accuracy: 99.950%, Avg loss: 0.001499
[2022-08-08 02:30:48,253] Epoch 11---------------
[2022-08-08 02:30:48,254] lr: 4.756538e-04
[2022-08-08 02:30:48,296] loss: 0.000909  [    0/ 4791]
[2022-08-08 02:30:48,899] loss: 0.002009  [  480/ 4791]
[2022-08-08 02:30:49,505] loss: 0.000502  [  960/ 4791]
[2022-08-08 02:30:50,120] loss: 0.001068  [ 1440/ 4791]
[2022-08-08 02:30:50,732] loss: 0.000769  [ 1920/ 4791]
[2022-08-08 02:30:51,344] loss: 0.000443  [ 2400/ 4791]
[2022-08-08 02:30:51,957] loss: 0.000692  [ 2880/ 4791]
[2022-08-08 02:30:52,569] loss: 0.000480  [ 3360/ 4791]
[2022-08-08 02:30:53,182] loss: 0.000564  [ 3840/ 4791]
[2022-08-08 02:30:53,795] loss: 0.000618  [ 4320/ 4791]
[2022-08-08 02:30:55,993] Train Error: Accuracy: 100.000%, Avg loss: 0.000718
[2022-08-08 02:30:56,674] Test  Error: Accuracy: 100.000%, Avg loss: 0.001103
[2022-08-08 02:30:56,674] Epoch 12---------------
[2022-08-08 02:30:56,675] lr: 4.518711e-04
[2022-08-08 02:30:56,718] loss: 0.000548  [    0/ 4791]
[2022-08-08 02:30:57,326] loss: 0.000495  [  480/ 4791]
[2022-08-08 02:30:57,934] loss: 0.000573  [  960/ 4791]
[2022-08-08 02:30:58,535] loss: 0.000407  [ 1440/ 4791]
[2022-08-08 02:30:59,139] loss: 0.000632  [ 1920/ 4791]
[2022-08-08 02:30:59,743] loss: 0.001288  [ 2400/ 4791]
[2022-08-08 02:31:00,345] loss: 0.000608  [ 2880/ 4791]
[2022-08-08 02:31:00,945] loss: 0.000604  [ 3360/ 4791]
[2022-08-08 02:31:01,548] loss: 0.000616  [ 3840/ 4791]
[2022-08-08 02:31:02,148] loss: 0.000690  [ 4320/ 4791]
[2022-08-08 02:31:04,315] Train Error: Accuracy: 100.000%, Avg loss: 0.000596
[2022-08-08 02:31:04,990] Test  Error: Accuracy: 100.000%, Avg loss: 0.000812
[2022-08-08 02:31:04,991] Epoch 13---------------
[2022-08-08 02:31:04,992] lr: 4.292775e-04
[2022-08-08 02:31:05,034] loss: 0.000428  [    0/ 4791]
[2022-08-08 02:31:05,638] loss: 0.000564  [  480/ 4791]
[2022-08-08 02:31:06,239] loss: 0.000539  [  960/ 4791]
[2022-08-08 02:31:06,842] loss: 0.000476  [ 1440/ 4791]
[2022-08-08 02:31:07,446] loss: 0.000392  [ 1920/ 4791]
[2022-08-08 02:31:08,049] loss: 0.000549  [ 2400/ 4791]
[2022-08-08 02:31:08,652] loss: 0.000401  [ 2880/ 4791]
[2022-08-08 02:31:09,254] loss: 0.000741  [ 3360/ 4791]
[2022-08-08 02:31:09,859] loss: 0.000502  [ 3840/ 4791]
[2022-08-08 02:31:10,460] loss: 0.001014  [ 4320/ 4791]
[2022-08-08 02:31:12,631] Train Error: Accuracy: 100.000%, Avg loss: 0.000603
[2022-08-08 02:31:13,308] Test  Error: Accuracy: 100.000%, Avg loss: 0.000790
[2022-08-08 02:31:13,309] Epoch 14---------------
[2022-08-08 02:31:13,310] lr: 4.078137e-04
[2022-08-08 02:31:13,352] loss: 0.000420  [    0/ 4791]
[2022-08-08 02:31:13,961] loss: 0.000710  [  480/ 4791]
[2022-08-08 02:31:14,566] loss: 0.000381  [  960/ 4791]
[2022-08-08 02:31:15,171] loss: 0.001974  [ 1440/ 4791]
[2022-08-08 02:31:15,775] loss: 0.000524  [ 1920/ 4791]
[2022-08-08 02:31:16,379] loss: 0.000453  [ 2400/ 4791]
[2022-08-08 02:31:16,983] loss: 0.000438  [ 2880/ 4791]
[2022-08-08 02:31:17,586] loss: 0.000708  [ 3360/ 4791]
[2022-08-08 02:31:18,190] loss: 0.000464  [ 3840/ 4791]
[2022-08-08 02:31:18,796] loss: 0.000442  [ 4320/ 4791]
[2022-08-08 02:31:20,967] Train Error: Accuracy: 99.958%, Avg loss: 0.001289
[2022-08-08 02:31:21,653] Test  Error: Accuracy: 99.900%, Avg loss: 0.002404
[2022-08-08 02:31:21,653] Epoch 15---------------
[2022-08-08 02:31:21,654] lr: 2.847915e-04
[2022-08-08 02:31:21,697] loss: 0.000456  [    0/ 4791]
[2022-08-08 02:31:22,299] loss: 0.000676  [  480/ 4791]
[2022-08-08 02:31:22,900] loss: 0.000592  [  960/ 4791]
[2022-08-08 02:31:23,504] loss: 0.000291  [ 1440/ 4791]
[2022-08-08 02:31:24,105] loss: 0.000300  [ 1920/ 4791]
[2022-08-08 02:31:24,708] loss: 0.000979  [ 2400/ 4791]
[2022-08-08 02:31:25,311] loss: 0.000524  [ 2880/ 4791]
[2022-08-08 02:31:25,914] loss: 0.000463  [ 3360/ 4791]
[2022-08-08 02:31:26,515] loss: 0.000357  [ 3840/ 4791]
[2022-08-08 02:31:27,118] loss: 0.000573  [ 4320/ 4791]
[2022-08-08 02:31:29,288] Train Error: Accuracy: 100.000%, Avg loss: 0.000485
[2022-08-08 02:31:29,965] Test  Error: Accuracy: 100.000%, Avg loss: 0.000585
[2022-08-08 02:31:29,965] Done!
[2022-08-08 02:31:29,969] Number of parameters:3293962
[2022-08-08 02:31:29,969] ## end time: 2022-08-08 02:31:29.965645
[2022-08-08 02:31:29,970] ## used time: 0:02:05.449449
