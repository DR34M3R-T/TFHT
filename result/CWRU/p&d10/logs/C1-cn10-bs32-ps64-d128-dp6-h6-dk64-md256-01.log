[2022-08-08 01:28:47,071] ## start time: 2022-08-08 01:28:46.937779
[2022-08-08 01:28:47,072] Using cuda device
[2022-08-08 01:28:47,073] In train:p&d10.npy.
[2022-08-08 01:28:47,075] One Channel
[2022-08-08 01:28:47,075] With Normal data.
[2022-08-08 01:28:47,075] Nunber of classes:10.
[2022-08-08 01:28:47,076] Nunber of ViT channels:1.
[2022-08-08 01:28:47,314] Totol epochs: 15
[2022-08-08 01:28:47,317] Epoch 1---------------
[2022-08-08 01:28:47,317] lr: 2.000000e-03
[2022-08-08 01:28:47,500] loss: 2.476690  [    0/ 4768]
[2022-08-08 01:28:50,226] loss: 2.561060  [  480/ 4768]
[2022-08-08 01:28:52,951] loss: 2.559245  [  960/ 4768]
[2022-08-08 01:28:55,677] loss: 0.832711  [ 1440/ 4768]
[2022-08-08 01:28:58,404] loss: 0.398140  [ 1920/ 4768]
[2022-08-08 01:29:01,130] loss: 0.293442  [ 2400/ 4768]
[2022-08-08 01:29:03,856] loss: 0.078680  [ 2880/ 4768]
[2022-08-08 01:29:06,579] loss: 0.068453  [ 3360/ 4768]
[2022-08-08 01:29:09,306] loss: 0.234456  [ 3840/ 4768]
[2022-08-08 01:29:12,034] loss: 0.095475  [ 4320/ 4768]
[2022-08-08 01:29:23,501] Train Error: Accuracy: 99.560%, Avg loss: 0.034911
[2022-08-08 01:29:27,352] Test  Error: Accuracy: 99.603%, Avg loss: 0.031178
[2022-08-08 01:29:27,352] Epoch 2---------------
[2022-08-08 01:29:27,353] lr: 1.900000e-03
[2022-08-08 01:29:27,537] loss: 0.018150  [    0/ 4768]
[2022-08-08 01:29:30,263] loss: 0.812278  [  480/ 4768]
[2022-08-08 01:29:32,988] loss: 0.023434  [  960/ 4768]
[2022-08-08 01:29:35,715] loss: 0.033990  [ 1440/ 4768]
[2022-08-08 01:29:38,442] loss: 0.938345  [ 1920/ 4768]
[2022-08-08 01:29:41,167] loss: 0.114111  [ 2400/ 4768]
[2022-08-08 01:29:43,893] loss: 0.011922  [ 2880/ 4768]
[2022-08-08 01:29:46,620] loss: 0.013239  [ 3360/ 4768]
[2022-08-08 01:29:49,346] loss: 1.629808  [ 3840/ 4768]
[2022-08-08 01:29:52,073] loss: 0.012748  [ 4320/ 4768]
[2022-08-08 01:30:03,525] Train Error: Accuracy: 99.748%, Avg loss: 0.021121
[2022-08-08 01:30:07,370] Test  Error: Accuracy: 99.851%, Avg loss: 0.017611
[2022-08-08 01:30:07,370] Epoch 3---------------
[2022-08-08 01:30:07,371] lr: 1.805000e-03
[2022-08-08 01:30:07,555] loss: 0.005641  [    0/ 4768]
[2022-08-08 01:30:10,280] loss: 0.028194  [  480/ 4768]
[2022-08-08 01:30:13,006] loss: 0.004952  [  960/ 4768]
[2022-08-08 01:30:15,733] loss: 0.022966  [ 1440/ 4768]
[2022-08-08 01:30:18,459] loss: 0.006793  [ 1920/ 4768]
[2022-08-08 01:30:21,186] loss: 0.001840  [ 2400/ 4768]
[2022-08-08 01:30:23,913] loss: 0.002228  [ 2880/ 4768]
[2022-08-08 01:30:26,635] loss: 0.001656  [ 3360/ 4768]
[2022-08-08 01:30:29,360] loss: 0.003561  [ 3840/ 4768]
[2022-08-08 01:30:32,087] loss: 0.007146  [ 4320/ 4768]
[2022-08-08 01:30:43,541] Train Error: Accuracy: 99.916%, Avg loss: 0.006940
[2022-08-08 01:30:47,385] Test  Error: Accuracy: 99.752%, Avg loss: 0.010392
[2022-08-08 01:30:47,385] Epoch 4---------------
[2022-08-08 01:30:47,386] lr: 1.714750e-03
[2022-08-08 01:30:47,571] loss: 0.003040  [    0/ 4768]
[2022-08-08 01:30:50,299] loss: 0.012395  [  480/ 4768]
[2022-08-08 01:30:53,023] loss: 0.001355  [  960/ 4768]
[2022-08-08 01:30:55,749] loss: 0.018019  [ 1440/ 4768]
[2022-08-08 01:30:58,476] loss: 0.003554  [ 1920/ 4768]
[2022-08-08 01:31:01,201] loss: 0.001399  [ 2400/ 4768]
[2022-08-08 01:31:03,927] loss: 0.001771  [ 2880/ 4768]
[2022-08-08 01:31:06,653] loss: 0.001246  [ 3360/ 4768]
[2022-08-08 01:31:09,380] loss: 0.004380  [ 3840/ 4768]
[2022-08-08 01:31:12,106] loss: 0.001784  [ 4320/ 4768]
[2022-08-08 01:31:23,556] Train Error: Accuracy: 99.937%, Avg loss: 0.002906
[2022-08-08 01:31:27,402] Test  Error: Accuracy: 100.000%, Avg loss: 0.003017
[2022-08-08 01:31:27,403] Epoch 5---------------
[2022-08-08 01:31:27,404] lr: 1.629012e-03
[2022-08-08 01:31:27,587] loss: 0.001959  [    0/ 4768]
[2022-08-08 01:31:30,313] loss: 0.007234  [  480/ 4768]
[2022-08-08 01:31:33,040] loss: 0.001200  [  960/ 4768]
[2022-08-08 01:31:35,767] loss: 0.228948  [ 1440/ 4768]
[2022-08-08 01:31:38,494] loss: 0.001976  [ 1920/ 4768]
[2022-08-08 01:31:41,218] loss: 0.018352  [ 2400/ 4768]
[2022-08-08 01:31:43,945] loss: 1.946822  [ 2880/ 4768]
[2022-08-08 01:31:46,671] loss: 0.171562  [ 3360/ 4768]
[2022-08-08 01:31:49,396] loss: 0.124913  [ 3840/ 4768]
[2022-08-08 01:31:52,121] loss: 0.038081  [ 4320/ 4768]
[2022-08-08 01:32:03,577] Train Error: Accuracy: 83.893%, Avg loss: 0.894906
[2022-08-08 01:32:07,424] Test  Error: Accuracy: 85.310%, Avg loss: 0.810222
[2022-08-08 01:32:07,424] Epoch 6---------------
[2022-08-08 01:32:07,425] lr: 1.137600e-03
[2022-08-08 01:32:07,609] loss: 0.481392  [    0/ 4768]
[2022-08-08 01:32:10,333] loss: 0.005625  [  480/ 4768]
[2022-08-08 01:32:13,060] loss: 0.066077  [  960/ 4768]
[2022-08-08 01:32:15,787] loss: 0.025162  [ 1440/ 4768]
[2022-08-08 01:32:18,512] loss: 0.002455  [ 1920/ 4768]
[2022-08-08 01:32:21,238] loss: 0.002198  [ 2400/ 4768]
[2022-08-08 01:32:23,963] loss: 0.004148  [ 2880/ 4768]
[2022-08-08 01:32:26,691] loss: 0.001274  [ 3360/ 4768]
[2022-08-08 01:32:29,415] loss: 0.002601  [ 3840/ 4768]
[2022-08-08 01:32:32,140] loss: 0.003411  [ 4320/ 4768]
[2022-08-08 01:32:43,593] Train Error: Accuracy: 99.874%, Avg loss: 0.006745
[2022-08-08 01:32:47,439] Test  Error: Accuracy: 99.801%, Avg loss: 0.009541
[2022-08-08 01:32:47,439] Epoch 7---------------
[2022-08-08 01:32:47,441] lr: 1.080720e-03
[2022-08-08 01:32:47,624] loss: 0.056558  [    0/ 4768]
[2022-08-08 01:32:50,351] loss: 0.008260  [  480/ 4768]
[2022-08-08 01:32:53,076] loss: 0.004426  [  960/ 4768]
[2022-08-08 01:32:55,801] loss: 0.001300  [ 1440/ 4768]
[2022-08-08 01:32:58,528] loss: 0.005118  [ 1920/ 4768]
[2022-08-08 01:33:01,254] loss: 0.001568  [ 2400/ 4768]
[2022-08-08 01:33:03,980] loss: 0.000953  [ 2880/ 4768]
[2022-08-08 01:33:06,706] loss: 0.001143  [ 3360/ 4768]
[2022-08-08 01:33:09,432] loss: 0.002193  [ 3840/ 4768]
[2022-08-08 01:33:12,156] loss: 0.000977  [ 4320/ 4768]
[2022-08-08 01:33:23,605] Train Error: Accuracy: 99.979%, Avg loss: 0.002695
[2022-08-08 01:33:27,454] Test  Error: Accuracy: 99.950%, Avg loss: 0.003278
[2022-08-08 01:33:27,455] Epoch 8---------------
[2022-08-08 01:33:27,456] lr: 1.026684e-03
[2022-08-08 01:33:27,639] loss: 0.000770  [    0/ 4768]
[2022-08-08 01:33:30,366] loss: 0.004026  [  480/ 4768]
[2022-08-08 01:33:33,093] loss: 0.000893  [  960/ 4768]
[2022-08-08 01:33:35,819] loss: 0.001081  [ 1440/ 4768]
[2022-08-08 01:33:38,545] loss: 0.002247  [ 1920/ 4768]
[2022-08-08 01:33:41,272] loss: 0.001176  [ 2400/ 4768]
[2022-08-08 01:33:43,996] loss: 0.000762  [ 2880/ 4768]
[2022-08-08 01:33:46,723] loss: 0.002225  [ 3360/ 4768]
[2022-08-08 01:33:49,447] loss: 0.000809  [ 3840/ 4768]
[2022-08-08 01:33:52,173] loss: 0.003757  [ 4320/ 4768]
[2022-08-08 01:34:03,632] Train Error: Accuracy: 99.979%, Avg loss: 0.001519
[2022-08-08 01:34:07,476] Test  Error: Accuracy: 99.801%, Avg loss: 0.004413
[2022-08-08 01:34:07,477] Epoch 9---------------
[2022-08-08 01:34:07,479] lr: 7.169718e-04
[2022-08-08 01:34:07,662] loss: 0.001019  [    0/ 4768]
[2022-08-08 01:34:10,388] loss: 0.000654  [  480/ 4768]
[2022-08-08 01:34:13,114] loss: 0.001119  [  960/ 4768]
[2022-08-08 01:34:15,840] loss: 0.000521  [ 1440/ 4768]
[2022-08-08 01:34:18,565] loss: 0.001120  [ 1920/ 4768]
[2022-08-08 01:34:21,291] loss: 0.000833  [ 2400/ 4768]
[2022-08-08 01:34:24,017] loss: 0.000384  [ 2880/ 4768]
[2022-08-08 01:34:26,741] loss: 0.000650  [ 3360/ 4768]
[2022-08-08 01:34:29,467] loss: 0.000585  [ 3840/ 4768]
[2022-08-08 01:34:32,194] loss: 0.001379  [ 4320/ 4768]
[2022-08-08 01:34:43,652] Train Error: Accuracy: 99.979%, Avg loss: 0.001058
[2022-08-08 01:34:47,498] Test  Error: Accuracy: 99.950%, Avg loss: 0.001980
[2022-08-08 01:34:47,499] Epoch 10---------------
[2022-08-08 01:34:47,500] lr: 6.811233e-04
[2022-08-08 01:34:47,684] loss: 0.001001  [    0/ 4768]
[2022-08-08 01:34:50,409] loss: 0.000442  [  480/ 4768]
[2022-08-08 01:34:53,136] loss: 0.000792  [  960/ 4768]
[2022-08-08 01:34:55,860] loss: 0.000585  [ 1440/ 4768]
[2022-08-08 01:34:58,587] loss: 0.000432  [ 1920/ 4768]
[2022-08-08 01:35:01,313] loss: 0.000366  [ 2400/ 4768]
[2022-08-08 01:35:04,039] loss: 0.001313  [ 2880/ 4768]
[2022-08-08 01:35:06,765] loss: 0.000277  [ 3360/ 4768]
[2022-08-08 01:35:09,490] loss: 0.000358  [ 3840/ 4768]
[2022-08-08 01:35:12,218] loss: 0.000409  [ 4320/ 4768]
[2022-08-08 01:35:23,670] Train Error: Accuracy: 100.000%, Avg loss: 0.000623
[2022-08-08 01:35:27,516] Test  Error: Accuracy: 99.851%, Avg loss: 0.003563
[2022-08-08 01:35:27,517] Epoch 11---------------
[2022-08-08 01:35:27,518] lr: 4.756538e-04
[2022-08-08 01:35:27,702] loss: 0.000346  [    0/ 4768]
[2022-08-08 01:35:30,428] loss: 0.000675  [  480/ 4768]
[2022-08-08 01:35:33,154] loss: 0.000479  [  960/ 4768]
[2022-08-08 01:35:35,881] loss: 0.000449  [ 1440/ 4768]
[2022-08-08 01:35:38,606] loss: 0.000432  [ 1920/ 4768]
[2022-08-08 01:35:41,333] loss: 0.000304  [ 2400/ 4768]
[2022-08-08 01:35:44,057] loss: 0.000450  [ 2880/ 4768]
[2022-08-08 01:35:46,783] loss: 0.000500  [ 3360/ 4768]
[2022-08-08 01:35:49,508] loss: 0.000412  [ 3840/ 4768]
[2022-08-08 01:35:52,233] loss: 0.000289  [ 4320/ 4768]
[2022-08-08 01:36:03,685] Train Error: Accuracy: 100.000%, Avg loss: 0.000513
[2022-08-08 01:36:07,531] Test  Error: Accuracy: 99.950%, Avg loss: 0.001664
[2022-08-08 01:36:07,532] Epoch 12---------------
[2022-08-08 01:36:07,533] lr: 4.518711e-04
[2022-08-08 01:36:07,716] loss: 0.001282  [    0/ 4768]
[2022-08-08 01:36:10,443] loss: 0.000426  [  480/ 4768]
[2022-08-08 01:36:13,167] loss: 0.000622  [  960/ 4768]
[2022-08-08 01:36:15,892] loss: 0.003619  [ 1440/ 4768]
[2022-08-08 01:36:18,618] loss: 0.000588  [ 1920/ 4768]
[2022-08-08 01:36:21,342] loss: 0.001379  [ 2400/ 4768]
[2022-08-08 01:36:24,067] loss: 0.000210  [ 2880/ 4768]
[2022-08-08 01:36:26,792] loss: 0.000298  [ 3360/ 4768]
[2022-08-08 01:36:29,518] loss: 0.000437  [ 3840/ 4768]
[2022-08-08 01:36:32,244] loss: 0.000524  [ 4320/ 4768]
[2022-08-08 01:36:43,697] Train Error: Accuracy: 100.000%, Avg loss: 0.000588
[2022-08-08 01:36:47,542] Test  Error: Accuracy: 99.950%, Avg loss: 0.003291
[2022-08-08 01:36:47,542] Epoch 13---------------
[2022-08-08 01:36:47,543] lr: 3.155584e-04
[2022-08-08 01:36:47,727] loss: 0.000340  [    0/ 4768]
[2022-08-08 01:36:50,452] loss: 0.000391  [  480/ 4768]
[2022-08-08 01:36:53,177] loss: 0.000479  [  960/ 4768]
[2022-08-08 01:36:55,902] loss: 0.000389  [ 1440/ 4768]
[2022-08-08 01:36:58,629] loss: 0.000563  [ 1920/ 4768]
[2022-08-08 01:37:01,355] loss: 0.000922  [ 2400/ 4768]
[2022-08-08 01:37:04,082] loss: 0.000568  [ 2880/ 4768]
[2022-08-08 01:37:06,809] loss: 0.000391  [ 3360/ 4768]
[2022-08-08 01:37:09,534] loss: 0.001351  [ 3840/ 4768]
[2022-08-08 01:37:12,261] loss: 0.000255  [ 4320/ 4768]
[2022-08-08 01:37:23,707] Train Error: Accuracy: 100.000%, Avg loss: 0.000445
[2022-08-08 01:37:27,547] Test  Error: Accuracy: 99.950%, Avg loss: 0.002275
[2022-08-08 01:37:27,547] Epoch 14---------------
[2022-08-08 01:37:27,548] lr: 2.997805e-04
[2022-08-08 01:37:27,731] loss: 0.000437  [    0/ 4768]
[2022-08-08 01:37:30,460] loss: 0.000302  [  480/ 4768]
[2022-08-08 01:37:33,185] loss: 0.000553  [  960/ 4768]
[2022-08-08 01:37:35,909] loss: 0.000354  [ 1440/ 4768]
[2022-08-08 01:37:38,636] loss: 0.000489  [ 1920/ 4768]
[2022-08-08 01:37:41,360] loss: 0.000329  [ 2400/ 4768]
[2022-08-08 01:37:44,086] loss: 0.000434  [ 2880/ 4768]
[2022-08-08 01:37:46,810] loss: 0.000259  [ 3360/ 4768]
[2022-08-08 01:37:49,537] loss: 0.000439  [ 3840/ 4768]
[2022-08-08 01:37:52,261] loss: 0.000537  [ 4320/ 4768]
[2022-08-08 01:38:03,706] Train Error: Accuracy: 99.979%, Avg loss: 0.000624
[2022-08-08 01:38:07,551] Test  Error: Accuracy: 99.950%, Avg loss: 0.001195
[2022-08-08 01:38:07,552] Epoch 15---------------
[2022-08-08 01:38:07,552] lr: 2.847915e-04
[2022-08-08 01:38:07,736] loss: 0.000353  [    0/ 4768]
[2022-08-08 01:38:10,461] loss: 0.000279  [  480/ 4768]
[2022-08-08 01:38:13,186] loss: 0.000538  [  960/ 4768]
[2022-08-08 01:38:15,913] loss: 0.000230  [ 1440/ 4768]
[2022-08-08 01:38:18,637] loss: 0.000419  [ 1920/ 4768]
[2022-08-08 01:38:21,363] loss: 0.000266  [ 2400/ 4768]
[2022-08-08 01:38:24,087] loss: 0.000459  [ 2880/ 4768]
[2022-08-08 01:38:26,810] loss: 0.000216  [ 3360/ 4768]
[2022-08-08 01:38:29,537] loss: 0.000539  [ 3840/ 4768]
[2022-08-08 01:38:32,262] loss: 0.000289  [ 4320/ 4768]
[2022-08-08 01:38:43,700] Train Error: Accuracy: 100.000%, Avg loss: 0.000473
[2022-08-08 01:38:47,535] Test  Error: Accuracy: 99.950%, Avg loss: 0.001892
[2022-08-08 01:38:47,535] Done!
[2022-08-08 01:38:47,539] Number of parameters:3186442
[2022-08-08 01:38:47,540] ## end time: 2022-08-08 01:38:47.535421
[2022-08-08 01:38:47,540] ## used time: 0:10:00.597642
