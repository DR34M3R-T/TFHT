[2022-08-08 02:04:21,451] ## start time: 2022-08-08 02:04:21.308246
[2022-08-08 02:04:21,451] Using cuda device
[2022-08-08 02:04:21,453] In train:p&d10.npy.
[2022-08-08 02:04:21,454] One Channel
[2022-08-08 02:04:21,454] With Normal data.
[2022-08-08 02:04:21,455] Nunber of classes:10.
[2022-08-08 02:04:21,455] Nunber of ViT channels:1.
[2022-08-08 02:04:21,707] Totol epochs: 15
[2022-08-08 02:04:21,710] Epoch 1---------------
[2022-08-08 02:04:21,710] lr: 2.000000e-03
[2022-08-08 02:04:21,813] loss: 2.368317  [    0/ 4718]
[2022-08-08 02:04:23,319] loss: 1.933382  [  480/ 4718]
[2022-08-08 02:04:24,825] loss: 1.557153  [  960/ 4718]
[2022-08-08 02:04:26,333] loss: 0.986377  [ 1440/ 4718]
[2022-08-08 02:04:27,842] loss: 0.463827  [ 1920/ 4718]
[2022-08-08 02:04:29,349] loss: 0.071431  [ 2400/ 4718]
[2022-08-08 02:04:30,853] loss: 0.071725  [ 2880/ 4718]
[2022-08-08 02:04:32,358] loss: 0.199510  [ 3360/ 4718]
[2022-08-08 02:04:33,866] loss: 0.021725  [ 3840/ 4718]
[2022-08-08 02:04:35,371] loss: 0.035242  [ 4320/ 4718]
[2022-08-08 02:04:41,728] Train Error: Accuracy: 66.511%, Avg loss: 1.250196
[2022-08-08 02:04:44,003] Test  Error: Accuracy: 64.891%, Avg loss: 1.302244
[2022-08-08 02:04:44,003] Epoch 2---------------
[2022-08-08 02:04:44,004] lr: 1.900000e-03
[2022-08-08 02:04:44,107] loss: 1.321227  [    0/ 4718]
[2022-08-08 02:04:45,613] loss: 0.033616  [  480/ 4718]
[2022-08-08 02:04:47,119] loss: 0.018503  [  960/ 4718]
[2022-08-08 02:04:48,627] loss: 0.008512  [ 1440/ 4718]
[2022-08-08 02:04:50,134] loss: 0.008381  [ 1920/ 4718]
[2022-08-08 02:04:51,640] loss: 0.010818  [ 2400/ 4718]
[2022-08-08 02:04:53,146] loss: 1.278273  [ 2880/ 4718]
[2022-08-08 02:04:54,651] loss: 0.044112  [ 3360/ 4718]
[2022-08-08 02:04:56,160] loss: 0.010018  [ 3840/ 4718]
[2022-08-08 02:04:57,668] loss: 0.015648  [ 4320/ 4718]
[2022-08-08 02:05:04,028] Train Error: Accuracy: 99.873%, Avg loss: 0.014141
[2022-08-08 02:05:06,309] Test  Error: Accuracy: 99.806%, Avg loss: 0.018253
[2022-08-08 02:05:06,309] Epoch 3---------------
[2022-08-08 02:05:06,311] lr: 1.805000e-03
[2022-08-08 02:05:06,413] loss: 0.016768  [    0/ 4718]
[2022-08-08 02:05:07,923] loss: 0.025505  [  480/ 4718]
[2022-08-08 02:05:09,431] loss: 0.004836  [  960/ 4718]
[2022-08-08 02:05:10,939] loss: 0.008796  [ 1440/ 4718]
[2022-08-08 02:05:12,449] loss: 0.005507  [ 1920/ 4718]
[2022-08-08 02:05:13,957] loss: 0.002865  [ 2400/ 4718]
[2022-08-08 02:05:15,463] loss: 0.005975  [ 2880/ 4718]
[2022-08-08 02:05:16,970] loss: 0.002551  [ 3360/ 4718]
[2022-08-08 02:05:18,473] loss: 0.003470  [ 3840/ 4718]
[2022-08-08 02:05:19,984] loss: 0.002694  [ 4320/ 4718]
[2022-08-08 02:05:26,346] Train Error: Accuracy: 100.000%, Avg loss: 0.002061
[2022-08-08 02:05:28,628] Test  Error: Accuracy: 99.903%, Avg loss: 0.003945
[2022-08-08 02:05:28,629] Epoch 4---------------
[2022-08-08 02:05:28,630] lr: 1.714750e-03
[2022-08-08 02:05:28,731] loss: 0.001348  [    0/ 4718]
[2022-08-08 02:05:30,239] loss: 0.002293  [  480/ 4718]
[2022-08-08 02:05:31,743] loss: 0.001230  [  960/ 4718]
[2022-08-08 02:05:33,248] loss: 0.001105  [ 1440/ 4718]
[2022-08-08 02:05:34,755] loss: 0.001068  [ 1920/ 4718]
[2022-08-08 02:05:36,264] loss: 0.000938  [ 2400/ 4718]
[2022-08-08 02:05:37,770] loss: 0.002422  [ 2880/ 4718]
[2022-08-08 02:05:39,274] loss: 0.002804  [ 3360/ 4718]
[2022-08-08 02:05:40,782] loss: 0.000779  [ 3840/ 4718]
[2022-08-08 02:05:42,286] loss: 0.000611  [ 4320/ 4718]
[2022-08-08 02:05:48,639] Train Error: Accuracy: 100.000%, Avg loss: 0.000980
[2022-08-08 02:05:50,918] Test  Error: Accuracy: 99.952%, Avg loss: 0.002362
[2022-08-08 02:05:50,918] Epoch 5---------------
[2022-08-08 02:05:50,919] lr: 1.629012e-03
[2022-08-08 02:05:51,022] loss: 0.000662  [    0/ 4718]
[2022-08-08 02:05:52,527] loss: 0.000631  [  480/ 4718]
[2022-08-08 02:05:54,032] loss: 0.000702  [  960/ 4718]
[2022-08-08 02:05:55,540] loss: 0.000755  [ 1440/ 4718]
[2022-08-08 02:05:57,049] loss: 0.000684  [ 1920/ 4718]
[2022-08-08 02:05:58,553] loss: 0.000606  [ 2400/ 4718]
[2022-08-08 02:06:00,057] loss: 0.000583  [ 2880/ 4718]
[2022-08-08 02:06:01,563] loss: 0.000506  [ 3360/ 4718]
[2022-08-08 02:06:03,069] loss: 0.003475  [ 3840/ 4718]
[2022-08-08 02:06:04,573] loss: 0.002120  [ 4320/ 4718]
[2022-08-08 02:06:10,945] Train Error: Accuracy: 99.979%, Avg loss: 0.002421
[2022-08-08 02:06:13,225] Test  Error: Accuracy: 100.000%, Avg loss: 0.002893
[2022-08-08 02:06:13,225] Epoch 6---------------
[2022-08-08 02:06:13,226] lr: 1.260499e-03
[2022-08-08 02:06:13,330] loss: 0.001286  [    0/ 4718]
[2022-08-08 02:06:14,839] loss: 0.001005  [  480/ 4718]
[2022-08-08 02:06:16,347] loss: 0.000710  [  960/ 4718]
[2022-08-08 02:06:17,854] loss: 0.000888  [ 1440/ 4718]
[2022-08-08 02:06:19,365] loss: 0.000623  [ 1920/ 4718]
[2022-08-08 02:06:20,866] loss: 0.000817  [ 2400/ 4718]
[2022-08-08 02:06:22,374] loss: 0.002239  [ 2880/ 4718]
[2022-08-08 02:06:23,878] loss: 0.000882  [ 3360/ 4718]
[2022-08-08 02:06:25,390] loss: 0.002297  [ 3840/ 4718]
[2022-08-08 02:06:26,898] loss: 0.606598  [ 4320/ 4718]
[2022-08-08 02:06:33,268] Train Error: Accuracy: 89.212%, Avg loss: 0.368371
[2022-08-08 02:06:35,544] Test  Error: Accuracy: 88.378%, Avg loss: 0.400345
[2022-08-08 02:06:35,545] Epoch 7---------------
[2022-08-08 02:06:35,546] lr: 8.802533e-04
[2022-08-08 02:06:35,649] loss: 0.238523  [    0/ 4718]
[2022-08-08 02:06:37,156] loss: 0.008969  [  480/ 4718]
[2022-08-08 02:06:38,661] loss: 0.004907  [  960/ 4718]
[2022-08-08 02:06:40,169] loss: 0.023889  [ 1440/ 4718]
[2022-08-08 02:06:41,677] loss: 0.002391  [ 1920/ 4718]
[2022-08-08 02:06:43,182] loss: 0.001290  [ 2400/ 4718]
[2022-08-08 02:06:44,685] loss: 0.003059  [ 2880/ 4718]
[2022-08-08 02:06:46,193] loss: 0.003466  [ 3360/ 4718]
[2022-08-08 02:06:47,700] loss: 0.002058  [ 3840/ 4718]
[2022-08-08 02:06:49,206] loss: 0.001774  [ 4320/ 4718]
[2022-08-08 02:06:55,575] Train Error: Accuracy: 99.979%, Avg loss: 0.002391
[2022-08-08 02:06:57,849] Test  Error: Accuracy: 99.952%, Avg loss: 0.002976
[2022-08-08 02:06:57,849] Epoch 8---------------
[2022-08-08 02:06:57,850] lr: 8.362407e-04
[2022-08-08 02:06:57,952] loss: 0.002375  [    0/ 4718]
[2022-08-08 02:06:59,457] loss: 0.001736  [  480/ 4718]
[2022-08-08 02:07:00,965] loss: 0.001399  [  960/ 4718]
[2022-08-08 02:07:02,469] loss: 0.001755  [ 1440/ 4718]
[2022-08-08 02:07:03,974] loss: 0.001545  [ 1920/ 4718]
[2022-08-08 02:07:05,479] loss: 0.000942  [ 2400/ 4718]
[2022-08-08 02:07:06,988] loss: 0.001156  [ 2880/ 4718]
[2022-08-08 02:07:08,492] loss: 0.000803  [ 3360/ 4718]
[2022-08-08 02:07:09,998] loss: 0.000755  [ 3840/ 4718]
[2022-08-08 02:07:11,502] loss: 0.000747  [ 4320/ 4718]
[2022-08-08 02:07:17,851] Train Error: Accuracy: 100.000%, Avg loss: 0.001407
[2022-08-08 02:07:20,117] Test  Error: Accuracy: 99.952%, Avg loss: 0.002044
[2022-08-08 02:07:20,118] Epoch 9---------------
[2022-08-08 02:07:20,119] lr: 7.944286e-04
[2022-08-08 02:07:20,221] loss: 0.000877  [    0/ 4718]
[2022-08-08 02:07:21,726] loss: 0.001973  [  480/ 4718]
[2022-08-08 02:07:23,228] loss: 0.002065  [  960/ 4718]
[2022-08-08 02:07:24,734] loss: 0.001383  [ 1440/ 4718]
[2022-08-08 02:07:26,240] loss: 0.000668  [ 1920/ 4718]
[2022-08-08 02:07:27,746] loss: 0.001217  [ 2400/ 4718]
[2022-08-08 02:07:29,252] loss: 0.000759  [ 2880/ 4718]
[2022-08-08 02:07:30,753] loss: 0.001050  [ 3360/ 4718]
[2022-08-08 02:07:32,258] loss: 0.000707  [ 3840/ 4718]
[2022-08-08 02:07:33,762] loss: 0.000860  [ 4320/ 4718]
[2022-08-08 02:07:40,093] Train Error: Accuracy: 99.979%, Avg loss: 0.001289
[2022-08-08 02:07:42,359] Test  Error: Accuracy: 100.000%, Avg loss: 0.001401
[2022-08-08 02:07:42,361] Epoch 10---------------
[2022-08-08 02:07:42,362] lr: 7.547072e-04
[2022-08-08 02:07:42,464] loss: 0.000574  [    0/ 4718]
[2022-08-08 02:07:43,970] loss: 0.000596  [  480/ 4718]
[2022-08-08 02:07:45,482] loss: 0.001293  [  960/ 4718]
[2022-08-08 02:07:46,987] loss: 0.000576  [ 1440/ 4718]
[2022-08-08 02:07:48,496] loss: 0.002452  [ 1920/ 4718]
[2022-08-08 02:07:50,003] loss: 0.000511  [ 2400/ 4718]
[2022-08-08 02:07:51,509] loss: 0.002531  [ 2880/ 4718]
[2022-08-08 02:07:53,012] loss: 0.000385  [ 3360/ 4718]
[2022-08-08 02:07:54,519] loss: 0.000739  [ 3840/ 4718]
[2022-08-08 02:07:56,023] loss: 0.000340  [ 4320/ 4718]
[2022-08-08 02:08:02,349] Train Error: Accuracy: 100.000%, Avg loss: 0.000697
[2022-08-08 02:08:04,616] Test  Error: Accuracy: 100.000%, Avg loss: 0.000843
[2022-08-08 02:08:04,616] Epoch 11---------------
[2022-08-08 02:08:04,617] lr: 7.169718e-04
[2022-08-08 02:08:04,719] loss: 0.000472  [    0/ 4718]
[2022-08-08 02:08:06,228] loss: 0.000379  [  480/ 4718]
[2022-08-08 02:08:07,737] loss: 0.000553  [  960/ 4718]
[2022-08-08 02:08:09,245] loss: 0.000773  [ 1440/ 4718]
[2022-08-08 02:08:10,752] loss: 0.000644  [ 1920/ 4718]
[2022-08-08 02:08:12,257] loss: 0.000447  [ 2400/ 4718]
[2022-08-08 02:08:13,765] loss: 0.001175  [ 2880/ 4718]
[2022-08-08 02:08:15,271] loss: 0.000340  [ 3360/ 4718]
[2022-08-08 02:08:16,779] loss: 0.000386  [ 3840/ 4718]
[2022-08-08 02:08:18,285] loss: 0.000310  [ 4320/ 4718]
[2022-08-08 02:08:24,621] Train Error: Accuracy: 100.000%, Avg loss: 0.000555
[2022-08-08 02:08:26,887] Test  Error: Accuracy: 100.000%, Avg loss: 0.000824
[2022-08-08 02:08:26,887] Epoch 12---------------
[2022-08-08 02:08:26,888] lr: 6.811233e-04
[2022-08-08 02:08:26,991] loss: 0.000746  [    0/ 4718]
[2022-08-08 02:08:28,499] loss: 0.001467  [  480/ 4718]
[2022-08-08 02:08:30,007] loss: 0.000859  [  960/ 4718]
[2022-08-08 02:08:31,515] loss: 0.000414  [ 1440/ 4718]
[2022-08-08 02:08:33,018] loss: 0.000359  [ 1920/ 4718]
[2022-08-08 02:08:34,525] loss: 0.000969  [ 2400/ 4718]
[2022-08-08 02:08:36,032] loss: 0.000364  [ 2880/ 4718]
[2022-08-08 02:08:37,540] loss: 0.000324  [ 3360/ 4718]
[2022-08-08 02:08:39,046] loss: 0.000374  [ 3840/ 4718]
[2022-08-08 02:08:40,552] loss: 0.000424  [ 4320/ 4718]
[2022-08-08 02:08:46,890] Train Error: Accuracy: 100.000%, Avg loss: 0.000458
[2022-08-08 02:08:49,158] Test  Error: Accuracy: 99.952%, Avg loss: 0.002306
[2022-08-08 02:08:49,158] Epoch 13---------------
[2022-08-08 02:08:49,160] lr: 4.756538e-04
[2022-08-08 02:08:49,262] loss: 0.000213  [    0/ 4718]
[2022-08-08 02:08:50,769] loss: 0.000365  [  480/ 4718]
[2022-08-08 02:08:52,272] loss: 0.000482  [  960/ 4718]
[2022-08-08 02:08:53,779] loss: 0.000484  [ 1440/ 4718]
[2022-08-08 02:08:55,285] loss: 0.000319  [ 1920/ 4718]
[2022-08-08 02:08:56,790] loss: 0.000351  [ 2400/ 4718]
[2022-08-08 02:08:58,296] loss: 0.000317  [ 2880/ 4718]
[2022-08-08 02:08:59,799] loss: 0.000485  [ 3360/ 4718]
[2022-08-08 02:09:01,305] loss: 0.000311  [ 3840/ 4718]
[2022-08-08 02:09:02,814] loss: 0.000383  [ 4320/ 4718]
[2022-08-08 02:09:09,138] Train Error: Accuracy: 100.000%, Avg loss: 0.000408
[2022-08-08 02:09:11,407] Test  Error: Accuracy: 99.952%, Avg loss: 0.001162
[2022-08-08 02:09:11,408] Epoch 14---------------
[2022-08-08 02:09:11,409] lr: 4.518711e-04
[2022-08-08 02:09:11,511] loss: 0.000278  [    0/ 4718]
[2022-08-08 02:09:13,020] loss: 0.000253  [  480/ 4718]
[2022-08-08 02:09:14,529] loss: 0.000325  [  960/ 4718]
[2022-08-08 02:09:16,036] loss: 0.000358  [ 1440/ 4718]
[2022-08-08 02:09:17,541] loss: 0.000345  [ 1920/ 4718]
[2022-08-08 02:09:19,049] loss: 0.000403  [ 2400/ 4718]
[2022-08-08 02:09:20,556] loss: 0.000312  [ 2880/ 4718]
[2022-08-08 02:09:22,063] loss: 0.000311  [ 3360/ 4718]
[2022-08-08 02:09:23,569] loss: 0.000446  [ 3840/ 4718]
[2022-08-08 02:09:25,077] loss: 0.000312  [ 4320/ 4718]
[2022-08-08 02:09:31,414] Train Error: Accuracy: 100.000%, Avg loss: 0.000397
[2022-08-08 02:09:33,681] Test  Error: Accuracy: 100.000%, Avg loss: 0.000755
[2022-08-08 02:09:33,681] Epoch 15---------------
[2022-08-08 02:09:33,682] lr: 4.292775e-04
[2022-08-08 02:09:33,784] loss: 0.000313  [    0/ 4718]
[2022-08-08 02:09:35,291] loss: 0.000371  [  480/ 4718]
[2022-08-08 02:09:36,802] loss: 0.000324  [  960/ 4718]
[2022-08-08 02:09:38,310] loss: 0.000206  [ 1440/ 4718]
[2022-08-08 02:09:39,820] loss: 0.000291  [ 1920/ 4718]
[2022-08-08 02:09:41,327] loss: 0.000328  [ 2400/ 4718]
[2022-08-08 02:09:42,831] loss: 0.000290  [ 2880/ 4718]
[2022-08-08 02:09:44,336] loss: 0.000324  [ 3360/ 4718]
[2022-08-08 02:09:45,841] loss: 0.000214  [ 3840/ 4718]
[2022-08-08 02:09:47,348] loss: 0.000293  [ 4320/ 4718]
[2022-08-08 02:09:53,678] Train Error: Accuracy: 100.000%, Avg loss: 0.000369
[2022-08-08 02:09:55,946] Test  Error: Accuracy: 100.000%, Avg loss: 0.000680
[2022-08-08 02:09:55,946] Done!
[2022-08-08 02:09:55,950] Number of parameters:3198730
[2022-08-08 02:09:55,950] ## end time: 2022-08-08 02:09:55.946209
[2022-08-08 02:09:55,951] ## used time: 0:05:34.637963
