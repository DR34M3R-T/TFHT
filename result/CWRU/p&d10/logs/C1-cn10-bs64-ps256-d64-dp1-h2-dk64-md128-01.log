[2022-08-06 15:30:00,862] ## start time: 2022-08-06 15:30:00.733564
[2022-08-06 15:30:00,862] Using cuda device
[2022-08-06 15:30:00,863] In train:p&d10.npy.
[2022-08-06 15:30:00,864] One Channel
[2022-08-06 15:30:00,865] With Normal data.
[2022-08-06 15:30:00,865] Nunber of classes:10.
[2022-08-06 15:30:00,866] Nunber of ViT channels:1.
[2022-08-06 15:30:01,052] Totol epochs: 10
[2022-08-06 15:30:01,054] Epoch 1---------------
[2022-08-06 15:30:01,054] lr: 2.000000e-03
[2022-08-06 15:30:01,069] loss: 2.731324  [    0/ 4796]
[2022-08-06 15:30:01,248] loss: 1.781978  [  960/ 4796]
[2022-08-06 15:30:01,403] loss: 1.341751  [ 1920/ 4796]
[2022-08-06 15:30:01,555] loss: 0.640177  [ 2880/ 4796]
[2022-08-06 15:30:01,717] loss: 0.253252  [ 3840/ 4796]
[2022-08-06 15:30:02,187] Train Error: Accuracy: 97.415%, Avg loss: 0.163587
[2022-08-06 15:30:02,315] Test  Error: Accuracy: 96.930%, Avg loss: 0.167189
[2022-08-06 15:30:02,316] Epoch 2---------------
[2022-08-06 15:30:02,317] lr: 1.900000e-03
[2022-08-06 15:30:02,329] loss: 0.192973  [    0/ 4796]
[2022-08-06 15:30:02,486] loss: 0.089339  [  960/ 4796]
[2022-08-06 15:30:02,640] loss: 0.064610  [ 1920/ 4796]
[2022-08-06 15:30:02,793] loss: 1.174387  [ 2880/ 4796]
[2022-08-06 15:30:02,947] loss: 0.137458  [ 3840/ 4796]
[2022-08-06 15:30:03,379] Train Error: Accuracy: 99.708%, Avg loss: 0.040419
[2022-08-06 15:30:03,502] Test  Error: Accuracy: 99.446%, Avg loss: 0.047738
[2022-08-06 15:30:03,503] Epoch 3---------------
[2022-08-06 15:30:03,504] lr: 1.805000e-03
[2022-08-06 15:30:03,515] loss: 0.040005  [    0/ 4796]
[2022-08-06 15:30:03,669] loss: 0.034601  [  960/ 4796]
[2022-08-06 15:30:03,827] loss: 0.032176  [ 1920/ 4796]
[2022-08-06 15:30:03,978] loss: 0.021266  [ 2880/ 4796]
[2022-08-06 15:30:04,129] loss: 0.011800  [ 3840/ 4796]
[2022-08-06 15:30:04,560] Train Error: Accuracy: 99.875%, Avg loss: 0.013646
[2022-08-06 15:30:04,684] Test  Error: Accuracy: 99.497%, Avg loss: 0.022469
[2022-08-06 15:30:04,684] Epoch 4---------------
[2022-08-06 15:30:04,685] lr: 1.714750e-03
[2022-08-06 15:30:04,697] loss: 0.016257  [    0/ 4796]
[2022-08-06 15:30:04,849] loss: 0.009161  [  960/ 4796]
[2022-08-06 15:30:04,999] loss: 0.010260  [ 1920/ 4796]
[2022-08-06 15:30:05,153] loss: 0.005629  [ 2880/ 4796]
[2022-08-06 15:30:05,303] loss: 0.005371  [ 3840/ 4796]
[2022-08-06 15:30:05,744] Train Error: Accuracy: 99.979%, Avg loss: 0.007168
[2022-08-06 15:30:05,867] Test  Error: Accuracy: 99.748%, Avg loss: 0.014479
[2022-08-06 15:30:05,867] Epoch 5---------------
[2022-08-06 15:30:05,868] lr: 1.629012e-03
[2022-08-06 15:30:05,879] loss: 0.006614  [    0/ 4796]
[2022-08-06 15:30:06,031] loss: 0.014152  [  960/ 4796]
[2022-08-06 15:30:06,181] loss: 0.535729  [ 1920/ 4796]
[2022-08-06 15:30:06,331] loss: 0.007774  [ 2880/ 4796]
[2022-08-06 15:30:06,481] loss: 0.009703  [ 3840/ 4796]
[2022-08-06 15:30:06,913] Train Error: Accuracy: 99.041%, Avg loss: 0.029867
[2022-08-06 15:30:07,038] Test  Error: Accuracy: 99.044%, Avg loss: 0.028551
[2022-08-06 15:30:07,038] Epoch 6---------------
[2022-08-06 15:30:07,039] lr: 1.137600e-03
[2022-08-06 15:30:07,050] loss: 0.079217  [    0/ 4796]
[2022-08-06 15:30:07,202] loss: 0.004389  [  960/ 4796]
[2022-08-06 15:30:07,353] loss: 0.004367  [ 1920/ 4796]
[2022-08-06 15:30:07,506] loss: 0.010753  [ 2880/ 4796]
[2022-08-06 15:30:07,660] loss: 0.004078  [ 3840/ 4796]
[2022-08-06 15:30:08,089] Train Error: Accuracy: 100.000%, Avg loss: 0.004403
[2022-08-06 15:30:08,211] Test  Error: Accuracy: 99.849%, Avg loss: 0.010558
[2022-08-06 15:30:08,211] Epoch 7---------------
[2022-08-06 15:30:08,212] lr: 1.080720e-03
[2022-08-06 15:30:08,224] loss: 0.003439  [    0/ 4796]
[2022-08-06 15:30:08,376] loss: 0.004196  [  960/ 4796]
[2022-08-06 15:30:08,527] loss: 0.007020  [ 1920/ 4796]
[2022-08-06 15:30:08,682] loss: 0.007360  [ 2880/ 4796]
[2022-08-06 15:30:08,834] loss: 0.018128  [ 3840/ 4796]
[2022-08-06 15:30:09,267] Train Error: Accuracy: 100.000%, Avg loss: 0.003860
[2022-08-06 15:30:09,389] Test  Error: Accuracy: 99.950%, Avg loss: 0.007972
[2022-08-06 15:30:09,390] Epoch 8---------------
[2022-08-06 15:30:09,392] lr: 1.026684e-03
[2022-08-06 15:30:09,403] loss: 0.010591  [    0/ 4796]
[2022-08-06 15:30:09,557] loss: 0.001579  [  960/ 4796]
[2022-08-06 15:30:09,710] loss: 0.002978  [ 1920/ 4796]
[2022-08-06 15:30:09,865] loss: 0.002490  [ 2880/ 4796]
[2022-08-06 15:30:10,016] loss: 0.001387  [ 3840/ 4796]
[2022-08-06 15:30:10,448] Train Error: Accuracy: 99.979%, Avg loss: 0.002777
[2022-08-06 15:30:10,570] Test  Error: Accuracy: 99.799%, Avg loss: 0.009353
[2022-08-06 15:30:10,570] Epoch 9---------------
[2022-08-06 15:30:10,571] lr: 7.944286e-04
[2022-08-06 15:30:10,583] loss: 0.001763  [    0/ 4796]
[2022-08-06 15:30:10,736] loss: 0.002225  [  960/ 4796]
[2022-08-06 15:30:10,886] loss: 0.001814  [ 1920/ 4796]
[2022-08-06 15:30:11,036] loss: 0.005200  [ 2880/ 4796]
[2022-08-06 15:30:11,187] loss: 0.002707  [ 3840/ 4796]
[2022-08-06 15:30:11,616] Train Error: Accuracy: 100.000%, Avg loss: 0.002402
[2022-08-06 15:30:11,737] Test  Error: Accuracy: 99.899%, Avg loss: 0.008503
[2022-08-06 15:30:11,737] Epoch 10---------------
[2022-08-06 15:30:11,738] lr: 7.547072e-04
[2022-08-06 15:30:11,750] loss: 0.003456  [    0/ 4796]
[2022-08-06 15:30:11,904] loss: 0.002150  [  960/ 4796]
[2022-08-06 15:30:12,053] loss: 0.000964  [ 1920/ 4796]
[2022-08-06 15:30:12,205] loss: 0.001216  [ 2880/ 4796]
[2022-08-06 15:30:12,356] loss: 0.001556  [ 3840/ 4796]
[2022-08-06 15:30:12,787] Train Error: Accuracy: 100.000%, Avg loss: 0.002298
[2022-08-06 15:30:12,909] Test  Error: Accuracy: 99.748%, Avg loss: 0.011104
[2022-08-06 15:30:12,910] Done!
[2022-08-06 15:30:12,911] Number of parameters:135050
[2022-08-06 15:30:12,912] ## end time: 2022-08-06 15:30:12.910864
[2022-08-06 15:30:12,912] ## used time: 0:00:12.177300
