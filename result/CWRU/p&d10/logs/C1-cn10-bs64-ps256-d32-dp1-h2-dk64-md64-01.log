[2022-08-06 15:10:05,330] ## start time: 2022-08-06 15:10:05.207479
[2022-08-06 15:10:05,330] Using cuda device
[2022-08-06 15:10:05,331] In train:p&d10.npy.
[2022-08-06 15:10:05,331] One Channel
[2022-08-06 15:10:05,331] With Normal data.
[2022-08-06 15:10:05,331] Nunber of classes:10.
[2022-08-06 15:10:05,331] Nunber of ViT channels:1.
[2022-08-06 15:10:06,337] Totol epochs: 10
[2022-08-06 15:10:06,339] Epoch 1---------------
[2022-08-06 15:10:06,339] lr: 2.000000e-03
[2022-08-06 15:10:07,903] loss: 2.571267  [    0/ 4771]
[2022-08-06 15:10:08,065] loss: 2.030857  [  960/ 4771]
[2022-08-06 15:10:08,222] loss: 1.608156  [ 1920/ 4771]
[2022-08-06 15:10:08,379] loss: 0.920410  [ 2880/ 4771]
[2022-08-06 15:10:08,535] loss: 0.844307  [ 3840/ 4771]
[2022-08-06 15:10:08,966] Train Error: Accuracy: 82.016%, Avg loss: 0.584405
[2022-08-06 15:10:09,084] Test  Error: Accuracy: 80.517%, Avg loss: 0.606320
[2022-08-06 15:10:09,084] Epoch 2---------------
[2022-08-06 15:10:09,085] lr: 1.900000e-03
[2022-08-06 15:10:09,096] loss: 0.555016  [    0/ 4771]
[2022-08-06 15:10:09,254] loss: 0.402394  [  960/ 4771]
[2022-08-06 15:10:09,415] loss: 0.372547  [ 1920/ 4771]
[2022-08-06 15:10:09,571] loss: 0.300532  [ 2880/ 4771]
[2022-08-06 15:10:09,725] loss: 0.272485  [ 3840/ 4771]
[2022-08-06 15:10:10,142] Train Error: Accuracy: 97.191%, Avg loss: 0.154799
[2022-08-06 15:10:10,258] Test  Error: Accuracy: 95.775%, Avg loss: 0.183695
[2022-08-06 15:10:10,258] Epoch 3---------------
[2022-08-06 15:10:10,259] lr: 1.805000e-03
[2022-08-06 15:10:10,271] loss: 0.155068  [    0/ 4771]
[2022-08-06 15:10:10,425] loss: 0.140915  [  960/ 4771]
[2022-08-06 15:10:10,577] loss: 0.119619  [ 1920/ 4771]
[2022-08-06 15:10:10,732] loss: 0.099123  [ 2880/ 4771]
[2022-08-06 15:10:10,884] loss: 0.160291  [ 3840/ 4771]
[2022-08-06 15:10:11,298] Train Error: Accuracy: 96.479%, Avg loss: 0.147884
[2022-08-06 15:10:11,414] Test  Error: Accuracy: 94.632%, Avg loss: 0.190140
[2022-08-06 15:10:11,414] Epoch 4---------------
[2022-08-06 15:10:11,415] lr: 1.547562e-03
[2022-08-06 15:10:11,427] loss: 0.122040  [    0/ 4771]
[2022-08-06 15:10:11,579] loss: 0.106894  [  960/ 4771]
[2022-08-06 15:10:11,732] loss: 0.054916  [ 1920/ 4771]
[2022-08-06 15:10:11,889] loss: 0.050008  [ 2880/ 4771]
[2022-08-06 15:10:12,046] loss: 0.032728  [ 3840/ 4771]
[2022-08-06 15:10:12,458] Train Error: Accuracy: 98.973%, Avg loss: 0.053043
[2022-08-06 15:10:12,572] Test  Error: Accuracy: 98.062%, Avg loss: 0.079590
[2022-08-06 15:10:12,573] Epoch 5---------------
[2022-08-06 15:10:12,574] lr: 1.470184e-03
[2022-08-06 15:10:12,587] loss: 0.050461  [    0/ 4771]
[2022-08-06 15:10:12,739] loss: 0.047832  [  960/ 4771]
[2022-08-06 15:10:12,893] loss: 0.025054  [ 1920/ 4771]
[2022-08-06 15:10:13,045] loss: 0.023799  [ 2880/ 4771]
[2022-08-06 15:10:13,199] loss: 0.055515  [ 3840/ 4771]
[2022-08-06 15:10:13,611] Train Error: Accuracy: 99.539%, Avg loss: 0.027115
[2022-08-06 15:10:13,727] Test  Error: Accuracy: 98.608%, Avg loss: 0.051054
[2022-08-06 15:10:13,728] Epoch 6---------------
[2022-08-06 15:10:13,729] lr: 1.396675e-03
[2022-08-06 15:10:13,741] loss: 0.016942  [    0/ 4771]
[2022-08-06 15:10:13,895] loss: 0.011789  [  960/ 4771]
[2022-08-06 15:10:14,046] loss: 0.019054  [ 1920/ 4771]
[2022-08-06 15:10:14,201] loss: 0.028780  [ 2880/ 4771]
[2022-08-06 15:10:14,352] loss: 0.036673  [ 3840/ 4771]
[2022-08-06 15:10:14,765] Train Error: Accuracy: 92.685%, Avg loss: 0.230699
[2022-08-06 15:10:14,881] Test  Error: Accuracy: 90.507%, Avg loss: 0.290524
[2022-08-06 15:10:14,881] Epoch 7---------------
[2022-08-06 15:10:14,883] lr: 9.753500e-04
[2022-08-06 15:10:14,895] loss: 0.135743  [    0/ 4771]
[2022-08-06 15:10:15,051] loss: 0.024282  [  960/ 4771]
[2022-08-06 15:10:15,215] loss: 0.019191  [ 1920/ 4771]
[2022-08-06 15:10:15,377] loss: 0.007630  [ 2880/ 4771]
[2022-08-06 15:10:15,537] loss: 0.019178  [ 3840/ 4771]
[2022-08-06 15:10:16,025] Train Error: Accuracy: 99.832%, Avg loss: 0.014657
[2022-08-06 15:10:16,158] Test  Error: Accuracy: 99.205%, Avg loss: 0.029705
[2022-08-06 15:10:16,158] Epoch 8---------------
[2022-08-06 15:10:16,159] lr: 9.265825e-04
[2022-08-06 15:10:16,171] loss: 0.035293  [    0/ 4771]
[2022-08-06 15:10:16,337] loss: 0.035554  [  960/ 4771]
[2022-08-06 15:10:16,498] loss: 0.006847  [ 1920/ 4771]
[2022-08-06 15:10:16,654] loss: 0.009539  [ 2880/ 4771]
[2022-08-06 15:10:16,809] loss: 0.009992  [ 3840/ 4771]
[2022-08-06 15:10:17,240] Train Error: Accuracy: 99.853%, Avg loss: 0.012189
[2022-08-06 15:10:17,364] Test  Error: Accuracy: 99.354%, Avg loss: 0.028086
[2022-08-06 15:10:17,364] Epoch 9---------------
[2022-08-06 15:10:17,365] lr: 8.802533e-04
[2022-08-06 15:10:17,377] loss: 0.016894  [    0/ 4771]
[2022-08-06 15:10:17,535] loss: 0.015430  [  960/ 4771]
[2022-08-06 15:10:17,695] loss: 0.008312  [ 1920/ 4771]
[2022-08-06 15:10:17,852] loss: 0.007257  [ 2880/ 4771]
[2022-08-06 15:10:18,006] loss: 0.013583  [ 3840/ 4771]
[2022-08-06 15:10:18,452] Train Error: Accuracy: 99.895%, Avg loss: 0.011785
[2022-08-06 15:10:18,574] Test  Error: Accuracy: 99.304%, Avg loss: 0.027207
[2022-08-06 15:10:18,575] Epoch 10---------------
[2022-08-06 15:10:18,576] lr: 8.362407e-04
[2022-08-06 15:10:18,587] loss: 0.009511  [    0/ 4771]
[2022-08-06 15:10:18,744] loss: 0.003619  [  960/ 4771]
[2022-08-06 15:10:18,903] loss: 0.026440  [ 1920/ 4771]
[2022-08-06 15:10:19,062] loss: 0.004730  [ 2880/ 4771]
[2022-08-06 15:10:19,222] loss: 0.011138  [ 3840/ 4771]
[2022-08-06 15:10:19,651] Train Error: Accuracy: 99.769%, Avg loss: 0.012573
[2022-08-06 15:10:19,766] Test  Error: Accuracy: 99.056%, Avg loss: 0.034415
[2022-08-06 15:10:19,766] Done!
[2022-08-06 15:10:19,768] Number of parameters:59338
[2022-08-06 15:10:19,768] ## end time: 2022-08-06 15:10:19.766177
[2022-08-06 15:10:19,769] ## used time: 0:00:14.558698
