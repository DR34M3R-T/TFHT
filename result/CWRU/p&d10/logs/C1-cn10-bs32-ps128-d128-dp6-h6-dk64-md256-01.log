[2022-08-08 01:58:45,073] ## start time: 2022-08-08 01:58:44.944795
[2022-08-08 01:58:45,073] Using cuda device
[2022-08-08 01:58:45,074] In train:p&d10.npy.
[2022-08-08 01:58:45,075] One Channel
[2022-08-08 01:58:45,076] With Normal data.
[2022-08-08 01:58:45,076] Nunber of classes:10.
[2022-08-08 01:58:45,077] Nunber of ViT channels:1.
[2022-08-08 01:58:45,330] Totol epochs: 15
[2022-08-08 01:58:45,333] Epoch 1---------------
[2022-08-08 01:58:45,334] lr: 2.000000e-03
[2022-08-08 01:58:45,436] loss: 2.291956  [    0/ 4756]
[2022-08-08 01:58:46,943] loss: 1.595763  [  480/ 4756]
[2022-08-08 01:58:48,451] loss: 1.018014  [  960/ 4756]
[2022-08-08 01:58:49,957] loss: 0.347884  [ 1440/ 4756]
[2022-08-08 01:58:51,461] loss: 0.144816  [ 1920/ 4756]
[2022-08-08 01:58:52,967] loss: 0.014663  [ 2400/ 4756]
[2022-08-08 01:58:54,474] loss: 0.015158  [ 2880/ 4756]
[2022-08-08 01:58:55,981] loss: 0.008123  [ 3360/ 4756]
[2022-08-08 01:58:57,488] loss: 0.113213  [ 3840/ 4756]
[2022-08-08 01:58:58,994] loss: 0.017876  [ 4320/ 4756]
[2022-08-08 01:59:05,499] Train Error: Accuracy: 99.706%, Avg loss: 0.036218
[2022-08-08 01:59:07,734] Test  Error: Accuracy: 99.655%, Avg loss: 0.039306
[2022-08-08 01:59:07,735] Epoch 2---------------
[2022-08-08 01:59:07,736] lr: 1.900000e-03
[2022-08-08 01:59:07,839] loss: 0.033731  [    0/ 4756]
[2022-08-08 01:59:09,341] loss: 0.013847  [  480/ 4756]
[2022-08-08 01:59:10,849] loss: 0.005811  [  960/ 4756]
[2022-08-08 01:59:12,355] loss: 0.006484  [ 1440/ 4756]
[2022-08-08 01:59:13,862] loss: 0.003559  [ 1920/ 4756]
[2022-08-08 01:59:15,369] loss: 0.002504  [ 2400/ 4756]
[2022-08-08 01:59:16,878] loss: 0.004279  [ 2880/ 4756]
[2022-08-08 01:59:18,387] loss: 0.034713  [ 3360/ 4756]
[2022-08-08 01:59:19,894] loss: 0.104992  [ 3840/ 4756]
[2022-08-08 01:59:21,401] loss: 0.168696  [ 4320/ 4756]
[2022-08-08 01:59:27,906] Train Error: Accuracy: 99.601%, Avg loss: 0.025812
[2022-08-08 01:59:30,134] Test  Error: Accuracy: 99.408%, Avg loss: 0.031087
[2022-08-08 01:59:30,135] Epoch 3---------------
[2022-08-08 01:59:30,136] lr: 1.805000e-03
[2022-08-08 01:59:30,238] loss: 0.010200  [    0/ 4756]
[2022-08-08 01:59:31,742] loss: 0.022463  [  480/ 4756]
[2022-08-08 01:59:33,247] loss: 0.010657  [  960/ 4756]
[2022-08-08 01:59:34,752] loss: 0.005842  [ 1440/ 4756]
[2022-08-08 01:59:36,259] loss: 0.004387  [ 1920/ 4756]
[2022-08-08 01:59:37,764] loss: 0.005227  [ 2400/ 4756]
[2022-08-08 01:59:39,271] loss: 0.003866  [ 2880/ 4756]
[2022-08-08 01:59:40,775] loss: 0.004083  [ 3360/ 4756]
[2022-08-08 01:59:42,282] loss: 0.180788  [ 3840/ 4756]
[2022-08-08 01:59:43,786] loss: 0.066985  [ 4320/ 4756]
[2022-08-08 01:59:50,283] Train Error: Accuracy: 99.811%, Avg loss: 0.014615
[2022-08-08 01:59:52,513] Test  Error: Accuracy: 99.605%, Avg loss: 0.022453
[2022-08-08 01:59:52,514] Epoch 4---------------
[2022-08-08 01:59:52,515] lr: 1.714750e-03
[2022-08-08 01:59:52,617] loss: 0.008919  [    0/ 4756]
[2022-08-08 01:59:54,123] loss: 0.027359  [  480/ 4756]
[2022-08-08 01:59:55,630] loss: 0.005775  [  960/ 4756]
[2022-08-08 01:59:57,135] loss: 0.006386  [ 1440/ 4756]
[2022-08-08 01:59:58,645] loss: 0.004912  [ 1920/ 4756]
[2022-08-08 02:00:00,153] loss: 0.067703  [ 2400/ 4756]
[2022-08-08 02:00:01,658] loss: 0.002956  [ 2880/ 4756]
[2022-08-08 02:00:03,164] loss: 0.131214  [ 3360/ 4756]
[2022-08-08 02:00:04,673] loss: 0.002486  [ 3840/ 4756]
[2022-08-08 02:00:06,179] loss: 0.001412  [ 4320/ 4756]
[2022-08-08 02:00:12,677] Train Error: Accuracy: 99.895%, Avg loss: 0.004951
[2022-08-08 02:00:14,906] Test  Error: Accuracy: 99.753%, Avg loss: 0.009723
[2022-08-08 02:00:14,907] Epoch 5---------------
[2022-08-08 02:00:14,908] lr: 1.629012e-03
[2022-08-08 02:00:15,010] loss: 0.014960  [    0/ 4756]
[2022-08-08 02:00:16,518] loss: 0.004424  [  480/ 4756]
[2022-08-08 02:00:18,023] loss: 0.003154  [  960/ 4756]
[2022-08-08 02:00:19,530] loss: 0.002773  [ 1440/ 4756]
[2022-08-08 02:00:21,038] loss: 0.001786  [ 1920/ 4756]
[2022-08-08 02:00:22,546] loss: 0.001339  [ 2400/ 4756]
[2022-08-08 02:00:24,048] loss: 0.001062  [ 2880/ 4756]
[2022-08-08 02:00:25,553] loss: 0.001236  [ 3360/ 4756]
[2022-08-08 02:00:27,061] loss: 0.674117  [ 3840/ 4756]
[2022-08-08 02:00:28,568] loss: 0.027680  [ 4320/ 4756]
[2022-08-08 02:00:35,064] Train Error: Accuracy: 99.664%, Avg loss: 0.023806
[2022-08-08 02:00:37,292] Test  Error: Accuracy: 99.359%, Avg loss: 0.032002
[2022-08-08 02:00:37,293] Epoch 6---------------
[2022-08-08 02:00:37,294] lr: 1.137600e-03
[2022-08-08 02:00:37,396] loss: 0.010890  [    0/ 4756]
[2022-08-08 02:00:38,901] loss: 0.035555  [  480/ 4756]
[2022-08-08 02:00:40,409] loss: 0.016118  [  960/ 4756]
[2022-08-08 02:00:41,916] loss: 0.005011  [ 1440/ 4756]
[2022-08-08 02:00:43,421] loss: 0.003801  [ 1920/ 4756]
[2022-08-08 02:00:44,923] loss: 0.007294  [ 2400/ 4756]
[2022-08-08 02:00:46,432] loss: 0.005609  [ 2880/ 4756]
[2022-08-08 02:00:47,937] loss: 0.004527  [ 3360/ 4756]
[2022-08-08 02:00:49,444] loss: 0.007392  [ 3840/ 4756]
[2022-08-08 02:00:50,950] loss: 0.001597  [ 4320/ 4756]
[2022-08-08 02:00:57,441] Train Error: Accuracy: 100.000%, Avg loss: 0.002505
[2022-08-08 02:00:59,671] Test  Error: Accuracy: 99.901%, Avg loss: 0.007456
[2022-08-08 02:00:59,671] Epoch 7---------------
[2022-08-08 02:00:59,672] lr: 1.080720e-03
[2022-08-08 02:00:59,774] loss: 0.003003  [    0/ 4756]
[2022-08-08 02:01:01,278] loss: 0.002523  [  480/ 4756]
[2022-08-08 02:01:02,785] loss: 0.001576  [  960/ 4756]
[2022-08-08 02:01:04,290] loss: 0.001013  [ 1440/ 4756]
[2022-08-08 02:01:05,798] loss: 0.002576  [ 1920/ 4756]
[2022-08-08 02:01:07,307] loss: 0.000960  [ 2400/ 4756]
[2022-08-08 02:01:08,815] loss: 0.001170  [ 2880/ 4756]
[2022-08-08 02:01:10,327] loss: 0.001524  [ 3360/ 4756]
[2022-08-08 02:01:11,833] loss: 0.009014  [ 3840/ 4756]
[2022-08-08 02:01:13,342] loss: 0.001618  [ 4320/ 4756]
[2022-08-08 02:01:19,836] Train Error: Accuracy: 100.000%, Avg loss: 0.001784
[2022-08-08 02:01:22,065] Test  Error: Accuracy: 99.753%, Avg loss: 0.006668
[2022-08-08 02:01:22,066] Epoch 8---------------
[2022-08-08 02:01:22,067] lr: 1.026684e-03
[2022-08-08 02:01:22,169] loss: 0.000843  [    0/ 4756]
[2022-08-08 02:01:23,678] loss: 0.001088  [  480/ 4756]
[2022-08-08 02:01:25,182] loss: 0.000820  [  960/ 4756]
[2022-08-08 02:01:26,691] loss: 0.000633  [ 1440/ 4756]
[2022-08-08 02:01:28,196] loss: 0.000550  [ 1920/ 4756]
[2022-08-08 02:01:29,701] loss: 0.001830  [ 2400/ 4756]
[2022-08-08 02:01:31,207] loss: 0.000740  [ 2880/ 4756]
[2022-08-08 02:01:32,715] loss: 0.017659  [ 3360/ 4756]
[2022-08-08 02:01:34,216] loss: 0.001205  [ 3840/ 4756]
[2022-08-08 02:01:35,722] loss: 0.006898  [ 4320/ 4756]
[2022-08-08 02:01:42,218] Train Error: Accuracy: 100.000%, Avg loss: 0.001666
[2022-08-08 02:01:44,447] Test  Error: Accuracy: 99.852%, Avg loss: 0.005615
[2022-08-08 02:01:44,447] Epoch 9---------------
[2022-08-08 02:01:44,448] lr: 9.753500e-04
[2022-08-08 02:01:44,551] loss: 0.001120  [    0/ 4756]
[2022-08-08 02:01:46,057] loss: 0.000775  [  480/ 4756]
[2022-08-08 02:01:47,564] loss: 0.001376  [  960/ 4756]
[2022-08-08 02:01:49,072] loss: 0.000806  [ 1440/ 4756]
[2022-08-08 02:01:50,581] loss: 0.000816  [ 1920/ 4756]
[2022-08-08 02:01:52,092] loss: 0.000994  [ 2400/ 4756]
[2022-08-08 02:01:53,597] loss: 0.000676  [ 2880/ 4756]
[2022-08-08 02:01:55,103] loss: 0.001785  [ 3360/ 4756]
[2022-08-08 02:01:56,610] loss: 0.000946  [ 3840/ 4756]
[2022-08-08 02:01:58,121] loss: 0.000543  [ 4320/ 4756]
[2022-08-08 02:02:04,616] Train Error: Accuracy: 100.000%, Avg loss: 0.001042
[2022-08-08 02:02:06,844] Test  Error: Accuracy: 99.753%, Avg loss: 0.006611
[2022-08-08 02:02:06,844] Epoch 10---------------
[2022-08-08 02:02:06,845] lr: 7.547072e-04
[2022-08-08 02:02:06,948] loss: 0.001142  [    0/ 4756]
[2022-08-08 02:02:08,456] loss: 0.000272  [  480/ 4756]
[2022-08-08 02:02:09,962] loss: 0.000589  [  960/ 4756]
[2022-08-08 02:02:11,468] loss: 0.000671  [ 1440/ 4756]
[2022-08-08 02:02:12,975] loss: 0.000369  [ 1920/ 4756]
[2022-08-08 02:02:14,482] loss: 0.000867  [ 2400/ 4756]
[2022-08-08 02:02:15,987] loss: 0.000571  [ 2880/ 4756]
[2022-08-08 02:02:17,496] loss: 0.001444  [ 3360/ 4756]
[2022-08-08 02:02:19,002] loss: 0.000333  [ 3840/ 4756]
[2022-08-08 02:02:20,507] loss: 0.000308  [ 4320/ 4756]
[2022-08-08 02:02:27,004] Train Error: Accuracy: 100.000%, Avg loss: 0.000687
[2022-08-08 02:02:29,232] Test  Error: Accuracy: 99.901%, Avg loss: 0.003662
[2022-08-08 02:02:29,233] Epoch 11---------------
[2022-08-08 02:02:29,235] lr: 7.169718e-04
[2022-08-08 02:02:29,336] loss: 0.000416  [    0/ 4756]
[2022-08-08 02:02:30,844] loss: 0.000345  [  480/ 4756]
[2022-08-08 02:02:32,353] loss: 0.000436  [  960/ 4756]
[2022-08-08 02:02:33,859] loss: 0.000521  [ 1440/ 4756]
[2022-08-08 02:02:35,367] loss: 0.000400  [ 1920/ 4756]
[2022-08-08 02:02:36,873] loss: 0.000536  [ 2400/ 4756]
[2022-08-08 02:02:38,381] loss: 0.000315  [ 2880/ 4756]
[2022-08-08 02:02:39,887] loss: 0.000634  [ 3360/ 4756]
[2022-08-08 02:02:41,396] loss: 0.000394  [ 3840/ 4756]
[2022-08-08 02:02:42,901] loss: 0.000979  [ 4320/ 4756]
[2022-08-08 02:02:49,400] Train Error: Accuracy: 100.000%, Avg loss: 0.000570
[2022-08-08 02:02:51,632] Test  Error: Accuracy: 99.951%, Avg loss: 0.003685
[2022-08-08 02:02:51,633] Epoch 12---------------
[2022-08-08 02:02:51,635] lr: 6.147137e-04
[2022-08-08 02:02:51,736] loss: 0.000478  [    0/ 4756]
[2022-08-08 02:02:53,243] loss: 0.000592  [  480/ 4756]
[2022-08-08 02:02:54,751] loss: 0.000420  [  960/ 4756]
[2022-08-08 02:02:56,256] loss: 0.000673  [ 1440/ 4756]
[2022-08-08 02:02:57,764] loss: 0.000485  [ 1920/ 4756]
[2022-08-08 02:02:59,271] loss: 0.000624  [ 2400/ 4756]
[2022-08-08 02:03:00,779] loss: 0.000315  [ 2880/ 4756]
[2022-08-08 02:03:02,286] loss: 0.000447  [ 3360/ 4756]
[2022-08-08 02:03:03,791] loss: 0.000666  [ 3840/ 4756]
[2022-08-08 02:03:05,300] loss: 0.000247  [ 4320/ 4756]
[2022-08-08 02:03:11,810] Train Error: Accuracy: 100.000%, Avg loss: 0.000485
[2022-08-08 02:03:14,045] Test  Error: Accuracy: 99.852%, Avg loss: 0.003476
[2022-08-08 02:03:14,046] Epoch 13---------------
[2022-08-08 02:03:14,047] lr: 5.839780e-04
[2022-08-08 02:03:14,148] loss: 0.000327  [    0/ 4756]
[2022-08-08 02:03:15,653] loss: 0.000505  [  480/ 4756]
[2022-08-08 02:03:17,160] loss: 0.000322  [  960/ 4756]
[2022-08-08 02:03:18,667] loss: 0.000218  [ 1440/ 4756]
[2022-08-08 02:03:20,175] loss: 0.000429  [ 1920/ 4756]
[2022-08-08 02:03:21,683] loss: 0.000701  [ 2400/ 4756]
[2022-08-08 02:03:23,189] loss: 0.000454  [ 2880/ 4756]
[2022-08-08 02:03:24,693] loss: 0.000399  [ 3360/ 4756]
[2022-08-08 02:03:26,201] loss: 0.000910  [ 3840/ 4756]
[2022-08-08 02:03:27,705] loss: 0.000272  [ 4320/ 4756]
[2022-08-08 02:03:34,210] Train Error: Accuracy: 100.000%, Avg loss: 0.000438
[2022-08-08 02:03:36,442] Test  Error: Accuracy: 99.951%, Avg loss: 0.002955
[2022-08-08 02:03:36,443] Epoch 14---------------
[2022-08-08 02:03:36,444] lr: 5.547791e-04
[2022-08-08 02:03:36,546] loss: 0.000537  [    0/ 4756]
[2022-08-08 02:03:38,052] loss: 0.000523  [  480/ 4756]
[2022-08-08 02:03:39,560] loss: 0.000327  [  960/ 4756]
[2022-08-08 02:03:41,068] loss: 0.000633  [ 1440/ 4756]
[2022-08-08 02:03:42,575] loss: 0.000496  [ 1920/ 4756]
[2022-08-08 02:03:44,080] loss: 0.000490  [ 2400/ 4756]
[2022-08-08 02:03:45,583] loss: 0.000243  [ 2880/ 4756]
[2022-08-08 02:03:47,090] loss: 0.000448  [ 3360/ 4756]
[2022-08-08 02:03:48,596] loss: 0.000531  [ 3840/ 4756]
[2022-08-08 02:03:50,105] loss: 0.000590  [ 4320/ 4756]
[2022-08-08 02:03:56,614] Train Error: Accuracy: 100.000%, Avg loss: 0.000520
[2022-08-08 02:03:58,847] Test  Error: Accuracy: 99.951%, Avg loss: 0.002363
[2022-08-08 02:03:58,848] Epoch 15---------------
[2022-08-08 02:03:58,849] lr: 5.270402e-04
[2022-08-08 02:03:58,952] loss: 0.000494  [    0/ 4756]
[2022-08-08 02:04:00,457] loss: 0.000508  [  480/ 4756]
[2022-08-08 02:04:01,962] loss: 0.000391  [  960/ 4756]
[2022-08-08 02:04:03,470] loss: 0.000241  [ 1440/ 4756]
[2022-08-08 02:04:04,980] loss: 0.000508  [ 1920/ 4756]
[2022-08-08 02:04:06,486] loss: 0.001064  [ 2400/ 4756]
[2022-08-08 02:04:07,991] loss: 0.000350  [ 2880/ 4756]
[2022-08-08 02:04:09,501] loss: 0.000661  [ 3360/ 4756]
[2022-08-08 02:04:11,011] loss: 0.000285  [ 3840/ 4756]
[2022-08-08 02:04:12,519] loss: 0.008010  [ 4320/ 4756]
[2022-08-08 02:04:19,020] Train Error: Accuracy: 99.979%, Avg loss: 0.001193
[2022-08-08 02:04:21,254] Test  Error: Accuracy: 99.951%, Avg loss: 0.003716
[2022-08-08 02:04:21,254] Done!
[2022-08-08 02:04:21,258] Number of parameters:3198730
[2022-08-08 02:04:21,258] ## end time: 2022-08-08 02:04:21.254233
[2022-08-08 02:04:21,259] ## used time: 0:05:36.309438
