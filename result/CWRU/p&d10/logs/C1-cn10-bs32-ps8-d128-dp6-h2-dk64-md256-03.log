[2022-08-08 18:00:38,398] ## start time: 2022-08-08 18:00:38.224745
[2022-08-08 18:00:38,399] Using cuda device
[2022-08-08 18:00:38,400] In train:p&d10.npy.
[2022-08-08 18:00:38,401] One Channel
[2022-08-08 18:00:38,402] With Normal data.
[2022-08-08 18:00:38,403] Nunber of classes:10.
[2022-08-08 18:00:38,403] Nunber of ViT channels:1.
[2022-08-08 18:00:38,736] Totol epochs: 15
[2022-08-08 18:00:38,740] Epoch 1---------------
[2022-08-08 18:00:38,740] lr: 2.000000e-03
[2022-08-08 18:00:39,573] loss: 2.513726  [    0/ 4747]
[2022-08-08 18:00:52,019] loss: 2.049934  [  480/ 4747]
[2022-08-08 18:01:04,464] loss: 1.602812  [  960/ 4747]
[2022-08-08 18:01:16,910] loss: 1.449584  [ 1440/ 4747]
[2022-08-08 18:01:29,357] loss: 1.557530  [ 1920/ 4747]
[2022-08-08 18:01:41,801] loss: 1.503870  [ 2400/ 4747]
[2022-08-08 18:01:54,245] loss: 1.091989  [ 2880/ 4747]
[2022-08-08 18:02:06,690] loss: 1.348011  [ 3360/ 4747]
[2022-08-08 18:02:19,137] loss: 1.085873  [ 3840/ 4747]
[2022-08-08 18:02:31,582] loss: 0.789713  [ 4320/ 4747]
[2022-08-08 18:03:27,914] Train Error: Accuracy: 61.218%, Avg loss: 1.040196
[2022-08-08 18:03:47,681] Test  Error: Accuracy: 60.855%, Avg loss: 1.042921
[2022-08-08 18:03:47,681] Epoch 2---------------
[2022-08-08 18:03:47,683] lr: 1.900000e-03
[2022-08-08 18:03:48,515] loss: 1.545700  [    0/ 4747]
[2022-08-08 18:04:00,962] loss: 0.497033  [  480/ 4747]
[2022-08-08 18:04:13,407] loss: 0.904794  [  960/ 4747]
[2022-08-08 18:04:25,854] loss: 0.665282  [ 1440/ 4747]
[2022-08-08 18:04:38,298] loss: 0.584812  [ 1920/ 4747]
[2022-08-08 18:04:50,742] loss: 0.676625  [ 2400/ 4747]
[2022-08-08 18:05:03,189] loss: 1.364887  [ 2880/ 4747]
[2022-08-08 18:05:15,636] loss: 0.429552  [ 3360/ 4747]
[2022-08-08 18:05:28,082] loss: 0.527298  [ 3840/ 4747]
[2022-08-08 18:05:40,526] loss: 0.543811  [ 4320/ 4747]
[2022-08-08 18:06:36,879] Train Error: Accuracy: 85.717%, Avg loss: 0.419413
[2022-08-08 18:06:56,657] Test  Error: Accuracy: 85.216%, Avg loss: 0.428710
[2022-08-08 18:06:56,657] Epoch 3---------------
[2022-08-08 18:06:56,658] lr: 1.805000e-03
[2022-08-08 18:06:57,489] loss: 0.407251  [    0/ 4747]
[2022-08-08 18:07:09,936] loss: 0.233874  [  480/ 4747]
[2022-08-08 18:07:22,384] loss: 0.294008  [  960/ 4747]
[2022-08-08 18:07:34,830] loss: 0.420421  [ 1440/ 4747]
[2022-08-08 18:07:47,280] loss: 0.398260  [ 1920/ 4747]
[2022-08-08 18:07:59,726] loss: 0.798805  [ 2400/ 4747]
[2022-08-08 18:08:12,175] loss: 0.315514  [ 2880/ 4747]
[2022-08-08 18:08:24,624] loss: 0.409774  [ 3360/ 4747]
[2022-08-08 18:08:37,102] loss: 0.292463  [ 3840/ 4747]
[2022-08-08 18:08:49,552] loss: 0.306024  [ 4320/ 4747]
[2022-08-08 18:09:45,891] Train Error: Accuracy: 87.171%, Avg loss: 0.381488
[2022-08-08 18:10:05,666] Test  Error: Accuracy: 86.984%, Avg loss: 0.379182
[2022-08-08 18:10:05,666] Epoch 4---------------
[2022-08-08 18:10:05,668] lr: 1.714750e-03
[2022-08-08 18:10:06,501] loss: 0.125560  [    0/ 4747]
[2022-08-08 18:10:18,946] loss: 0.134963  [  480/ 4747]
[2022-08-08 18:10:31,394] loss: 0.363409  [  960/ 4747]
[2022-08-08 18:10:43,841] loss: 0.396341  [ 1440/ 4747]
[2022-08-08 18:10:56,287] loss: 0.118731  [ 1920/ 4747]
[2022-08-08 18:11:08,731] loss: 0.254027  [ 2400/ 4747]
[2022-08-08 18:11:21,176] loss: 0.151888  [ 2880/ 4747]
[2022-08-08 18:11:33,623] loss: 0.214310  [ 3360/ 4747]
[2022-08-08 18:11:46,071] loss: 0.582295  [ 3840/ 4747]
[2022-08-08 18:11:58,517] loss: 0.194047  [ 4320/ 4747]
[2022-08-08 18:12:54,813] Train Error: Accuracy: 93.406%, Avg loss: 0.188922
[2022-08-08 18:13:14,546] Test  Error: Accuracy: 92.534%, Avg loss: 0.213111
[2022-08-08 18:13:14,546] Epoch 5---------------
[2022-08-08 18:13:14,547] lr: 1.629012e-03
[2022-08-08 18:13:15,376] loss: 0.208179  [    0/ 4747]
[2022-08-08 18:13:27,804] loss: 0.084636  [  480/ 4747]
[2022-08-08 18:13:40,231] loss: 0.156646  [  960/ 4747]
[2022-08-08 18:13:52,660] loss: 0.067463  [ 1440/ 4747]
[2022-08-08 18:14:05,089] loss: 0.285025  [ 1920/ 4747]
[2022-08-08 18:14:17,517] loss: 0.173825  [ 2400/ 4747]
[2022-08-08 18:14:29,946] loss: 0.075937  [ 2880/ 4747]
[2022-08-08 18:14:42,373] loss: 0.248468  [ 3360/ 4747]
[2022-08-08 18:14:54,799] loss: 0.225993  [ 3840/ 4747]
[2022-08-08 18:15:07,227] loss: 0.176558  [ 4320/ 4747]
[2022-08-08 18:16:03,441] Train Error: Accuracy: 95.155%, Avg loss: 0.158410
[2022-08-08 18:16:23,170] Test  Error: Accuracy: 93.664%, Avg loss: 0.189079
[2022-08-08 18:16:23,171] Epoch 6---------------
[2022-08-08 18:16:23,171] lr: 1.547562e-03
[2022-08-08 18:16:24,001] loss: 0.087960  [    0/ 4747]
[2022-08-08 18:16:36,430] loss: 0.118820  [  480/ 4747]
[2022-08-08 18:16:48,857] loss: 0.126221  [  960/ 4747]
[2022-08-08 18:17:01,286] loss: 0.249745  [ 1440/ 4747]
[2022-08-08 18:17:13,717] loss: 0.049064  [ 1920/ 4747]
[2022-08-08 18:17:26,147] loss: 0.196165  [ 2400/ 4747]
[2022-08-08 18:17:38,576] loss: 0.176409  [ 2880/ 4747]
[2022-08-08 18:17:51,003] loss: 0.048207  [ 3360/ 4747]
[2022-08-08 18:18:03,431] loss: 0.124538  [ 3840/ 4747]
[2022-08-08 18:18:15,861] loss: 0.044189  [ 4320/ 4747]
[2022-08-08 18:19:12,089] Train Error: Accuracy: 96.840%, Avg loss: 0.097504
[2022-08-08 18:19:31,822] Test  Error: Accuracy: 96.267%, Avg loss: 0.121186
[2022-08-08 18:19:31,822] Epoch 7---------------
[2022-08-08 18:19:31,823] lr: 1.470184e-03
[2022-08-08 18:19:32,653] loss: 0.056748  [    0/ 4747]
[2022-08-08 18:19:45,081] loss: 0.124391  [  480/ 4747]
[2022-08-08 18:19:57,510] loss: 0.076635  [  960/ 4747]
[2022-08-08 18:20:09,939] loss: 0.053250  [ 1440/ 4747]
[2022-08-08 18:20:22,367] loss: 0.029880  [ 1920/ 4747]
[2022-08-08 18:20:34,797] loss: 0.135228  [ 2400/ 4747]
[2022-08-08 18:20:47,226] loss: 0.093712  [ 2880/ 4747]
[2022-08-08 18:20:59,654] loss: 0.159958  [ 3360/ 4747]
[2022-08-08 18:21:12,082] loss: 0.078811  [ 3840/ 4747]
[2022-08-08 18:21:24,512] loss: 0.015898  [ 4320/ 4747]
[2022-08-08 18:22:20,741] Train Error: Accuracy: 95.050%, Avg loss: 0.148315
[2022-08-08 18:22:40,474] Test  Error: Accuracy: 94.499%, Avg loss: 0.194391
[2022-08-08 18:22:40,474] Epoch 8---------------
[2022-08-08 18:22:40,478] lr: 1.026684e-03
[2022-08-08 18:22:41,308] loss: 0.033177  [    0/ 4747]
[2022-08-08 18:22:53,737] loss: 0.041843  [  480/ 4747]
[2022-08-08 18:23:06,166] loss: 0.085867  [  960/ 4747]
[2022-08-08 18:23:18,595] loss: 0.091198  [ 1440/ 4747]
[2022-08-08 18:23:31,027] loss: 0.084393  [ 1920/ 4747]
[2022-08-08 18:23:43,456] loss: 0.026156  [ 2400/ 4747]
[2022-08-08 18:23:55,886] loss: 0.117431  [ 2880/ 4747]
[2022-08-08 18:24:08,314] loss: 0.035778  [ 3360/ 4747]
[2022-08-08 18:24:20,743] loss: 0.011232  [ 3840/ 4747]
[2022-08-08 18:24:33,173] loss: 0.014545  [ 4320/ 4747]
[2022-08-08 18:25:29,402] Train Error: Accuracy: 97.683%, Avg loss: 0.072889
[2022-08-08 18:25:49,131] Test  Error: Accuracy: 96.415%, Avg loss: 0.115032
[2022-08-08 18:25:49,132] Epoch 9---------------
[2022-08-08 18:25:49,132] lr: 9.753500e-04
[2022-08-08 18:25:49,963] loss: 0.164530  [    0/ 4747]
[2022-08-08 18:26:02,392] loss: 0.036333  [  480/ 4747]
[2022-08-08 18:26:14,823] loss: 0.012861  [  960/ 4747]
[2022-08-08 18:26:27,252] loss: 0.024042  [ 1440/ 4747]
[2022-08-08 18:26:39,683] loss: 0.019543  [ 1920/ 4747]
[2022-08-08 18:26:52,112] loss: 0.026756  [ 2400/ 4747]
[2022-08-08 18:27:04,540] loss: 0.010895  [ 2880/ 4747]
[2022-08-08 18:27:16,970] loss: 0.014406  [ 3360/ 4747]
[2022-08-08 18:27:29,403] loss: 0.006669  [ 3840/ 4747]
[2022-08-08 18:27:41,836] loss: 0.080279  [ 4320/ 4747]
[2022-08-08 18:28:38,068] Train Error: Accuracy: 98.631%, Avg loss: 0.043407
[2022-08-08 18:28:57,804] Test  Error: Accuracy: 97.593%, Avg loss: 0.085000
[2022-08-08 18:28:57,804] Epoch 10---------------
[2022-08-08 18:28:57,806] lr: 9.265825e-04
[2022-08-08 18:28:58,635] loss: 0.060974  [    0/ 4747]
[2022-08-08 18:29:11,065] loss: 0.020476  [  480/ 4747]
[2022-08-08 18:29:23,494] loss: 0.032508  [  960/ 4747]
[2022-08-08 18:29:35,921] loss: 0.015750  [ 1440/ 4747]
[2022-08-08 18:29:48,348] loss: 0.034872  [ 1920/ 4747]
[2022-08-08 18:30:00,772] loss: 0.006946  [ 2400/ 4747]
[2022-08-08 18:30:13,199] loss: 0.110446  [ 2880/ 4747]
[2022-08-08 18:30:25,626] loss: 0.040053  [ 3360/ 4747]
[2022-08-08 18:30:38,053] loss: 0.014546  [ 3840/ 4747]
[2022-08-08 18:30:50,479] loss: 0.024670  [ 4320/ 4747]
[2022-08-08 18:31:46,685] Train Error: Accuracy: 99.242%, Avg loss: 0.028930
[2022-08-08 18:32:06,406] Test  Error: Accuracy: 98.576%, Avg loss: 0.055558
[2022-08-08 18:32:06,406] Epoch 11---------------
[2022-08-08 18:32:06,407] lr: 8.802533e-04
[2022-08-08 18:32:07,237] loss: 0.151723  [    0/ 4747]
[2022-08-08 18:32:19,670] loss: 0.010828  [  480/ 4747]
[2022-08-08 18:32:32,111] loss: 0.010614  [  960/ 4747]
[2022-08-08 18:32:44,555] loss: 0.034130  [ 1440/ 4747]
[2022-08-08 18:32:57,000] loss: 0.005017  [ 1920/ 4747]
[2022-08-08 18:33:09,448] loss: 0.023142  [ 2400/ 4747]
[2022-08-08 18:33:21,893] loss: 0.020217  [ 2880/ 4747]
[2022-08-08 18:33:34,340] loss: 0.093092  [ 3360/ 4747]
[2022-08-08 18:33:46,784] loss: 0.015603  [ 3840/ 4747]
[2022-08-08 18:33:59,230] loss: 0.012124  [ 4320/ 4747]
[2022-08-08 18:34:55,540] Train Error: Accuracy: 99.094%, Avg loss: 0.034050
[2022-08-08 18:35:15,297] Test  Error: Accuracy: 98.035%, Avg loss: 0.071193
[2022-08-08 18:35:15,297] Epoch 12---------------
[2022-08-08 18:35:15,298] lr: 6.147137e-04
[2022-08-08 18:35:16,129] loss: 0.071225  [    0/ 4747]
[2022-08-08 18:35:28,572] loss: 0.009626  [  480/ 4747]
[2022-08-08 18:35:41,015] loss: 0.016057  [  960/ 4747]
[2022-08-08 18:35:53,459] loss: 0.014052  [ 1440/ 4747]
[2022-08-08 18:36:05,902] loss: 0.044054  [ 1920/ 4747]
[2022-08-08 18:36:18,345] loss: 0.003989  [ 2400/ 4747]
[2022-08-08 18:36:30,787] loss: 0.013430  [ 2880/ 4747]
[2022-08-08 18:36:43,229] loss: 0.011062  [ 3360/ 4747]
[2022-08-08 18:36:55,670] loss: 0.034774  [ 3840/ 4747]
[2022-08-08 18:37:08,112] loss: 0.003051  [ 4320/ 4747]
[2022-08-08 18:38:04,415] Train Error: Accuracy: 98.399%, Avg loss: 0.039250
[2022-08-08 18:38:24,182] Test  Error: Accuracy: 97.888%, Avg loss: 0.072390
[2022-08-08 18:38:24,182] Epoch 13---------------
[2022-08-08 18:38:24,184] lr: 5.270402e-04
[2022-08-08 18:38:25,015] loss: 0.019581  [    0/ 4747]
[2022-08-08 18:38:37,457] loss: 0.004230  [  480/ 4747]
[2022-08-08 18:38:49,898] loss: 0.028313  [  960/ 4747]
[2022-08-08 18:39:02,338] loss: 0.013535  [ 1440/ 4747]
[2022-08-08 18:39:14,779] loss: 0.030250  [ 1920/ 4747]
[2022-08-08 18:39:27,220] loss: 0.004858  [ 2400/ 4747]
[2022-08-08 18:39:39,661] loss: 0.011997  [ 2880/ 4747]
[2022-08-08 18:39:52,101] loss: 0.010714  [ 3360/ 4747]
[2022-08-08 18:40:04,544] loss: 0.013216  [ 3840/ 4747]
[2022-08-08 18:40:16,985] loss: 0.085514  [ 4320/ 4747]
[2022-08-08 18:41:13,300] Train Error: Accuracy: 99.515%, Avg loss: 0.016580
[2022-08-08 18:41:33,060] Test  Error: Accuracy: 98.281%, Avg loss: 0.052488
[2022-08-08 18:41:33,060] Epoch 14---------------
[2022-08-08 18:41:33,063] lr: 5.006882e-04
[2022-08-08 18:41:33,895] loss: 0.011034  [    0/ 4747]
[2022-08-08 18:41:46,335] loss: 0.083018  [  480/ 4747]
[2022-08-08 18:41:58,777] loss: 0.007678  [  960/ 4747]
[2022-08-08 18:42:11,220] loss: 0.055241  [ 1440/ 4747]
[2022-08-08 18:42:23,661] loss: 0.038970  [ 1920/ 4747]
[2022-08-08 18:42:36,104] loss: 0.002053  [ 2400/ 4747]
[2022-08-08 18:42:48,546] loss: 0.010418  [ 2880/ 4747]
[2022-08-08 18:43:00,988] loss: 0.089980  [ 3360/ 4747]
[2022-08-08 18:43:13,431] loss: 0.048459  [ 3840/ 4747]
[2022-08-08 18:43:25,875] loss: 0.004123  [ 4320/ 4747]
[2022-08-08 18:44:22,180] Train Error: Accuracy: 99.494%, Avg loss: 0.016851
[2022-08-08 18:44:41,926] Test  Error: Accuracy: 98.134%, Avg loss: 0.062217
[2022-08-08 18:44:41,926] Epoch 15---------------
[2022-08-08 18:44:41,927] lr: 3.874230e-04
[2022-08-08 18:44:42,758] loss: 0.024475  [    0/ 4747]
[2022-08-08 18:44:55,195] loss: 0.006123  [  480/ 4747]
[2022-08-08 18:45:07,631] loss: 0.003087  [  960/ 4747]
[2022-08-08 18:45:20,068] loss: 0.004091  [ 1440/ 4747]
[2022-08-08 18:45:32,505] loss: 0.076874  [ 1920/ 4747]
[2022-08-08 18:45:44,943] loss: 0.003021  [ 2400/ 4747]
[2022-08-08 18:45:57,385] loss: 0.002436  [ 2880/ 4747]
[2022-08-08 18:46:09,828] loss: 0.005409  [ 3360/ 4747]
[2022-08-08 18:46:22,272] loss: 0.004583  [ 3840/ 4747]
[2022-08-08 18:46:34,715] loss: 0.008883  [ 4320/ 4747]
[2022-08-08 18:47:31,031] Train Error: Accuracy: 99.705%, Avg loss: 0.010331
[2022-08-08 18:47:50,793] Test  Error: Accuracy: 98.723%, Avg loss: 0.045464
[2022-08-08 18:47:50,793] Done!
[2022-08-08 18:47:50,798] Number of parameters:1656586
[2022-08-08 18:47:50,798] ## end time: 2022-08-08 18:47:50.793233
[2022-08-08 18:47:50,799] ## used time: 0:47:12.568488
