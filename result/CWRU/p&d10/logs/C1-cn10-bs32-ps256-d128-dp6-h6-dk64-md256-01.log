[2022-08-08 02:15:36,142] ## start time: 2022-08-08 02:15:36.006830
[2022-08-08 02:15:36,143] Using cuda device
[2022-08-08 02:15:36,144] In train:p&d10.npy.
[2022-08-08 02:15:36,145] One Channel
[2022-08-08 02:15:36,146] With Normal data.
[2022-08-08 02:15:36,146] Nunber of classes:10.
[2022-08-08 02:15:36,146] Nunber of ViT channels:1.
[2022-08-08 02:15:36,398] Totol epochs: 15
[2022-08-08 02:15:36,401] Epoch 1---------------
[2022-08-08 02:15:36,401] lr: 2.000000e-03
[2022-08-08 02:15:36,463] loss: 2.410810  [    0/ 4803]
[2022-08-08 02:15:37,369] loss: 1.846548  [  480/ 4803]
[2022-08-08 02:15:38,271] loss: 1.420088  [  960/ 4803]
[2022-08-08 02:15:39,173] loss: 0.569870  [ 1440/ 4803]
[2022-08-08 02:15:40,076] loss: 0.382814  [ 1920/ 4803]
[2022-08-08 02:15:40,978] loss: 0.129465  [ 2400/ 4803]
[2022-08-08 02:15:41,880] loss: 0.024080  [ 2880/ 4803]
[2022-08-08 02:15:42,782] loss: 0.018544  [ 3360/ 4803]
[2022-08-08 02:15:43,684] loss: 0.047606  [ 3840/ 4803]
[2022-08-08 02:15:44,586] loss: 0.340584  [ 4320/ 4803]
[2022-08-08 02:15:45,464] loss: 0.055954  [ 4800/ 4803]
[2022-08-08 02:15:48,179] Train Error: Accuracy: 94.524%, Avg loss: 0.220776
[2022-08-08 02:15:49,298] Test  Error: Accuracy: 93.990%, Avg loss: 0.219536
[2022-08-08 02:15:49,298] Epoch 2---------------
[2022-08-08 02:15:49,299] lr: 1.900000e-03
[2022-08-08 02:15:49,361] loss: 0.138024  [    0/ 4803]
[2022-08-08 02:15:50,265] loss: 0.025941  [  480/ 4803]
[2022-08-08 02:15:51,175] loss: 0.023911  [  960/ 4803]
[2022-08-08 02:15:52,079] loss: 0.007145  [ 1440/ 4803]
[2022-08-08 02:15:52,982] loss: 0.022607  [ 1920/ 4803]
[2022-08-08 02:15:53,886] loss: 0.006673  [ 2400/ 4803]
[2022-08-08 02:15:54,789] loss: 0.084599  [ 2880/ 4803]
[2022-08-08 02:15:55,693] loss: 0.018599  [ 3360/ 4803]
[2022-08-08 02:15:56,596] loss: 0.009596  [ 3840/ 4803]
[2022-08-08 02:15:57,500] loss: 0.094851  [ 4320/ 4803]
[2022-08-08 02:15:58,377] loss: 0.002170  [ 4800/ 4803]
[2022-08-08 02:16:01,094] Train Error: Accuracy: 99.896%, Avg loss: 0.007976
[2022-08-08 02:16:02,212] Test  Error: Accuracy: 100.000%, Avg loss: 0.006914
[2022-08-08 02:16:02,212] Epoch 3---------------
[2022-08-08 02:16:02,213] lr: 1.805000e-03
[2022-08-08 02:16:02,275] loss: 0.022043  [    0/ 4803]
[2022-08-08 02:16:03,178] loss: 0.002986  [  480/ 4803]
[2022-08-08 02:16:04,080] loss: 0.041772  [  960/ 4803]
[2022-08-08 02:16:04,982] loss: 0.084984  [ 1440/ 4803]
[2022-08-08 02:16:05,884] loss: 0.013852  [ 1920/ 4803]
[2022-08-08 02:16:06,788] loss: 0.020523  [ 2400/ 4803]
[2022-08-08 02:16:07,690] loss: 0.004837  [ 2880/ 4803]
[2022-08-08 02:16:08,592] loss: 0.005082  [ 3360/ 4803]
[2022-08-08 02:16:09,494] loss: 0.006046  [ 3840/ 4803]
[2022-08-08 02:16:10,397] loss: 0.006415  [ 4320/ 4803]
[2022-08-08 02:16:11,275] loss: 0.003635  [ 4800/ 4803]
[2022-08-08 02:16:13,993] Train Error: Accuracy: 99.563%, Avg loss: 0.024073
[2022-08-08 02:16:15,110] Test  Error: Accuracy: 99.495%, Avg loss: 0.025384
[2022-08-08 02:16:15,110] Epoch 4---------------
[2022-08-08 02:16:15,111] lr: 1.260499e-03
[2022-08-08 02:16:15,174] loss: 0.047039  [    0/ 4803]
[2022-08-08 02:16:16,080] loss: 0.004355  [  480/ 4803]
[2022-08-08 02:16:16,983] loss: 0.002986  [  960/ 4803]
[2022-08-08 02:16:17,887] loss: 0.001498  [ 1440/ 4803]
[2022-08-08 02:16:18,790] loss: 0.001284  [ 1920/ 4803]
[2022-08-08 02:16:19,695] loss: 0.003218  [ 2400/ 4803]
[2022-08-08 02:16:20,599] loss: 0.001599  [ 2880/ 4803]
[2022-08-08 02:16:21,503] loss: 0.000948  [ 3360/ 4803]
[2022-08-08 02:16:22,406] loss: 0.000881  [ 3840/ 4803]
[2022-08-08 02:16:23,309] loss: 0.003624  [ 4320/ 4803]
[2022-08-08 02:16:24,188] loss: 0.000625  [ 4800/ 4803]
[2022-08-08 02:16:26,904] Train Error: Accuracy: 100.000%, Avg loss: 0.001326
[2022-08-08 02:16:28,022] Test  Error: Accuracy: 100.000%, Avg loss: 0.001422
[2022-08-08 02:16:28,023] Epoch 5---------------
[2022-08-08 02:16:28,024] lr: 1.197474e-03
[2022-08-08 02:16:28,086] loss: 0.001324  [    0/ 4803]
[2022-08-08 02:16:28,989] loss: 0.000965  [  480/ 4803]
[2022-08-08 02:16:29,891] loss: 0.000936  [  960/ 4803]
[2022-08-08 02:16:30,793] loss: 0.000671  [ 1440/ 4803]
[2022-08-08 02:16:31,696] loss: 0.003228  [ 1920/ 4803]
[2022-08-08 02:16:32,598] loss: 0.000912  [ 2400/ 4803]
[2022-08-08 02:16:33,500] loss: 0.007019  [ 2880/ 4803]
[2022-08-08 02:16:34,402] loss: 0.002854  [ 3360/ 4803]
[2022-08-08 02:16:35,304] loss: 0.001046  [ 3840/ 4803]
[2022-08-08 02:16:36,206] loss: 0.001098  [ 4320/ 4803]
[2022-08-08 02:16:37,084] loss: 0.001489  [ 4800/ 4803]
[2022-08-08 02:16:39,797] Train Error: Accuracy: 100.000%, Avg loss: 0.001018
[2022-08-08 02:16:40,913] Test  Error: Accuracy: 100.000%, Avg loss: 0.001913
[2022-08-08 02:16:40,914] Epoch 6---------------
[2022-08-08 02:16:40,915] lr: 8.362407e-04
[2022-08-08 02:16:40,977] loss: 0.001007  [    0/ 4803]
[2022-08-08 02:16:41,880] loss: 0.000812  [  480/ 4803]
[2022-08-08 02:16:42,785] loss: 0.000562  [  960/ 4803]
[2022-08-08 02:16:43,689] loss: 0.000875  [ 1440/ 4803]
[2022-08-08 02:16:44,593] loss: 0.000693  [ 1920/ 4803]
[2022-08-08 02:16:45,497] loss: 0.000577  [ 2400/ 4803]
[2022-08-08 02:16:46,401] loss: 0.000678  [ 2880/ 4803]
[2022-08-08 02:16:47,304] loss: 0.000688  [ 3360/ 4803]
[2022-08-08 02:16:48,208] loss: 0.000709  [ 3840/ 4803]
[2022-08-08 02:16:49,111] loss: 0.000441  [ 4320/ 4803]
[2022-08-08 02:16:49,990] loss: 0.000217  [ 4800/ 4803]
[2022-08-08 02:16:52,707] Train Error: Accuracy: 100.000%, Avg loss: 0.000650
[2022-08-08 02:16:53,823] Test  Error: Accuracy: 100.000%, Avg loss: 0.000829
[2022-08-08 02:16:53,823] Epoch 7---------------
[2022-08-08 02:16:53,824] lr: 7.944286e-04
[2022-08-08 02:16:53,886] loss: 0.000674  [    0/ 4803]
[2022-08-08 02:16:54,789] loss: 0.000638  [  480/ 4803]
[2022-08-08 02:16:55,691] loss: 0.000456  [  960/ 4803]
[2022-08-08 02:16:56,594] loss: 0.001191  [ 1440/ 4803]
[2022-08-08 02:16:57,497] loss: 0.000761  [ 1920/ 4803]
[2022-08-08 02:16:58,399] loss: 0.000402  [ 2400/ 4803]
[2022-08-08 02:16:59,302] loss: 0.000727  [ 2880/ 4803]
[2022-08-08 02:17:00,205] loss: 0.000791  [ 3360/ 4803]
[2022-08-08 02:17:01,106] loss: 0.000552  [ 3840/ 4803]
[2022-08-08 02:17:02,009] loss: 0.000809  [ 4320/ 4803]
[2022-08-08 02:17:02,886] loss: 0.000658  [ 4800/ 4803]
[2022-08-08 02:17:05,601] Train Error: Accuracy: 100.000%, Avg loss: 0.000554
[2022-08-08 02:17:06,720] Test  Error: Accuracy: 100.000%, Avg loss: 0.000876
[2022-08-08 02:17:06,720] Epoch 8---------------
[2022-08-08 02:17:06,721] lr: 6.811233e-04
[2022-08-08 02:17:06,785] loss: 0.000366  [    0/ 4803]
[2022-08-08 02:17:07,688] loss: 0.000421  [  480/ 4803]
[2022-08-08 02:17:08,592] loss: 0.000773  [  960/ 4803]
[2022-08-08 02:17:09,495] loss: 0.000526  [ 1440/ 4803]
[2022-08-08 02:17:10,398] loss: 0.000409  [ 1920/ 4803]
[2022-08-08 02:17:11,301] loss: 0.000436  [ 2400/ 4803]
[2022-08-08 02:17:12,205] loss: 0.000323  [ 2880/ 4803]
[2022-08-08 02:17:13,108] loss: 0.000542  [ 3360/ 4803]
[2022-08-08 02:17:14,012] loss: 0.000359  [ 3840/ 4803]
[2022-08-08 02:17:14,915] loss: 0.000411  [ 4320/ 4803]
[2022-08-08 02:17:15,794] loss: 0.000572  [ 4800/ 4803]
[2022-08-08 02:17:18,509] Train Error: Accuracy: 100.000%, Avg loss: 0.000498
[2022-08-08 02:17:19,638] Test  Error: Accuracy: 100.000%, Avg loss: 0.000742
[2022-08-08 02:17:19,639] Epoch 9---------------
[2022-08-08 02:17:19,640] lr: 6.470671e-04
[2022-08-08 02:17:19,702] loss: 0.000624  [    0/ 4803]
[2022-08-08 02:17:20,604] loss: 0.000532  [  480/ 4803]
[2022-08-08 02:17:21,507] loss: 0.000369  [  960/ 4803]
[2022-08-08 02:17:22,409] loss: 0.000394  [ 1440/ 4803]
[2022-08-08 02:17:23,311] loss: 0.000596  [ 1920/ 4803]
[2022-08-08 02:17:24,214] loss: 0.000463  [ 2400/ 4803]
[2022-08-08 02:17:25,116] loss: 0.000349  [ 2880/ 4803]
[2022-08-08 02:17:26,018] loss: 0.000482  [ 3360/ 4803]
[2022-08-08 02:17:26,920] loss: 0.000496  [ 3840/ 4803]
[2022-08-08 02:17:27,823] loss: 0.000708  [ 4320/ 4803]
[2022-08-08 02:17:28,700] loss: 0.000543  [ 4800/ 4803]
[2022-08-08 02:17:31,417] Train Error: Accuracy: 100.000%, Avg loss: 0.000447
[2022-08-08 02:17:32,534] Test  Error: Accuracy: 100.000%, Avg loss: 0.001094
[2022-08-08 02:17:32,534] Epoch 10---------------
[2022-08-08 02:17:32,536] lr: 4.518711e-04
[2022-08-08 02:17:32,597] loss: 0.000552  [    0/ 4803]
[2022-08-08 02:17:33,500] loss: 0.000484  [  480/ 4803]
[2022-08-08 02:17:34,404] loss: 0.000436  [  960/ 4803]
[2022-08-08 02:17:35,308] loss: 0.000447  [ 1440/ 4803]
[2022-08-08 02:17:36,212] loss: 0.000536  [ 1920/ 4803]
[2022-08-08 02:17:37,115] loss: 0.000436  [ 2400/ 4803]
[2022-08-08 02:17:38,019] loss: 0.000752  [ 2880/ 4803]
[2022-08-08 02:17:38,923] loss: 0.000551  [ 3360/ 4803]
[2022-08-08 02:17:39,827] loss: 0.000475  [ 3840/ 4803]
[2022-08-08 02:17:40,731] loss: 0.000565  [ 4320/ 4803]
[2022-08-08 02:17:41,608] loss: 0.000962  [ 4800/ 4803]
[2022-08-08 02:17:44,337] Train Error: Accuracy: 100.000%, Avg loss: 0.000613
[2022-08-08 02:17:45,454] Test  Error: Accuracy: 100.000%, Avg loss: 0.000814
[2022-08-08 02:17:45,455] Epoch 11---------------
[2022-08-08 02:17:45,456] lr: 4.292775e-04
[2022-08-08 02:17:45,519] loss: 0.000669  [    0/ 4803]
[2022-08-08 02:17:46,421] loss: 0.000444  [  480/ 4803]
[2022-08-08 02:17:47,324] loss: 0.000374  [  960/ 4803]
[2022-08-08 02:17:48,227] loss: 0.003588  [ 1440/ 4803]
[2022-08-08 02:17:49,130] loss: 0.000331  [ 1920/ 4803]
[2022-08-08 02:17:50,031] loss: 0.000742  [ 2400/ 4803]
[2022-08-08 02:17:50,933] loss: 0.000431  [ 2880/ 4803]
[2022-08-08 02:17:51,835] loss: 0.000480  [ 3360/ 4803]
[2022-08-08 02:17:52,737] loss: 0.000383  [ 3840/ 4803]
[2022-08-08 02:17:53,639] loss: 0.000366  [ 4320/ 4803]
[2022-08-08 02:17:54,517] loss: 0.000538  [ 4800/ 4803]
[2022-08-08 02:17:57,231] Train Error: Accuracy: 100.000%, Avg loss: 0.000436
[2022-08-08 02:17:58,347] Test  Error: Accuracy: 100.000%, Avg loss: 0.000511
[2022-08-08 02:17:58,347] Epoch 12---------------
[2022-08-08 02:17:58,348] lr: 4.078137e-04
[2022-08-08 02:17:58,411] loss: 0.000388  [    0/ 4803]
[2022-08-08 02:17:59,315] loss: 0.000520  [  480/ 4803]
[2022-08-08 02:18:00,218] loss: 0.000407  [  960/ 4803]
[2022-08-08 02:18:01,122] loss: 0.000348  [ 1440/ 4803]
[2022-08-08 02:18:02,026] loss: 0.000352  [ 1920/ 4803]
[2022-08-08 02:18:02,929] loss: 0.000354  [ 2400/ 4803]
[2022-08-08 02:18:03,833] loss: 0.000613  [ 2880/ 4803]
[2022-08-08 02:18:04,736] loss: 0.000372  [ 3360/ 4803]
[2022-08-08 02:18:05,639] loss: 0.000390  [ 3840/ 4803]
[2022-08-08 02:18:06,543] loss: 0.000280  [ 4320/ 4803]
[2022-08-08 02:18:07,421] loss: 0.002919  [ 4800/ 4803]
[2022-08-08 02:18:10,135] Train Error: Accuracy: 100.000%, Avg loss: 0.000438
[2022-08-08 02:18:11,258] Test  Error: Accuracy: 100.000%, Avg loss: 0.000588
[2022-08-08 02:18:11,258] Epoch 13---------------
[2022-08-08 02:18:11,259] lr: 3.155584e-04
[2022-08-08 02:18:11,321] loss: 0.000489  [    0/ 4803]
[2022-08-08 02:18:12,224] loss: 0.000405  [  480/ 4803]
[2022-08-08 02:18:13,127] loss: 0.000314  [  960/ 4803]
[2022-08-08 02:18:14,029] loss: 0.000489  [ 1440/ 4803]
[2022-08-08 02:18:14,932] loss: 0.000305  [ 1920/ 4803]
[2022-08-08 02:18:15,835] loss: 0.000374  [ 2400/ 4803]
[2022-08-08 02:18:16,737] loss: 0.000604  [ 2880/ 4803]
[2022-08-08 02:18:17,639] loss: 0.000328  [ 3360/ 4803]
[2022-08-08 02:18:18,541] loss: 0.000281  [ 3840/ 4803]
[2022-08-08 02:18:19,444] loss: 0.000401  [ 4320/ 4803]
[2022-08-08 02:18:20,322] loss: 0.000217  [ 4800/ 4803]
[2022-08-08 02:18:23,037] Train Error: Accuracy: 100.000%, Avg loss: 0.000384
[2022-08-08 02:18:24,154] Test  Error: Accuracy: 100.000%, Avg loss: 0.000573
[2022-08-08 02:18:24,154] Epoch 14---------------
[2022-08-08 02:18:24,155] lr: 2.997805e-04
[2022-08-08 02:18:24,218] loss: 0.000254  [    0/ 4803]
[2022-08-08 02:18:25,122] loss: 0.000332  [  480/ 4803]
[2022-08-08 02:18:26,025] loss: 0.000326  [  960/ 4803]
[2022-08-08 02:18:26,928] loss: 0.000374  [ 1440/ 4803]
[2022-08-08 02:18:27,832] loss: 0.000566  [ 1920/ 4803]
[2022-08-08 02:18:28,736] loss: 0.000434  [ 2400/ 4803]
[2022-08-08 02:18:29,639] loss: 0.000393  [ 2880/ 4803]
[2022-08-08 02:18:30,542] loss: 0.000308  [ 3360/ 4803]
[2022-08-08 02:18:31,447] loss: 0.000401  [ 3840/ 4803]
[2022-08-08 02:18:32,350] loss: 0.000266  [ 4320/ 4803]
[2022-08-08 02:18:33,230] loss: 0.000335  [ 4800/ 4803]
[2022-08-08 02:18:35,943] Train Error: Accuracy: 100.000%, Avg loss: 0.000386
[2022-08-08 02:18:37,062] Test  Error: Accuracy: 100.000%, Avg loss: 0.000567
[2022-08-08 02:18:37,063] Epoch 15---------------
[2022-08-08 02:18:37,064] lr: 2.847915e-04
[2022-08-08 02:18:37,127] loss: 0.000404  [    0/ 4803]
[2022-08-08 02:18:38,030] loss: 0.000309  [  480/ 4803]
[2022-08-08 02:18:38,932] loss: 0.000372  [  960/ 4803]
[2022-08-08 02:18:39,834] loss: 0.000374  [ 1440/ 4803]
[2022-08-08 02:18:40,737] loss: 0.000294  [ 1920/ 4803]
[2022-08-08 02:18:41,639] loss: 0.000427  [ 2400/ 4803]
[2022-08-08 02:18:42,541] loss: 0.000383  [ 2880/ 4803]
[2022-08-08 02:18:43,444] loss: 0.000375  [ 3360/ 4803]
[2022-08-08 02:18:44,346] loss: 0.000210  [ 3840/ 4803]
[2022-08-08 02:18:45,249] loss: 0.000583  [ 4320/ 4803]
[2022-08-08 02:18:46,127] loss: 0.000515  [ 4800/ 4803]
[2022-08-08 02:18:48,843] Train Error: Accuracy: 100.000%, Avg loss: 0.000380
[2022-08-08 02:18:49,961] Test  Error: Accuracy: 100.000%, Avg loss: 0.000476
[2022-08-08 02:18:49,961] Done!
[2022-08-08 02:18:49,965] Number of parameters:3229450
[2022-08-08 02:18:49,965] ## end time: 2022-08-08 02:18:49.961816
[2022-08-08 02:18:49,966] ## used time: 0:03:13.954986
