[2022-08-07 22:16:54,139] ## start time: 2022-08-07 22:16:54.016743
[2022-08-07 22:16:54,139] Using cuda device
[2022-08-07 22:16:54,140] In train:p&d10.npy.
[2022-08-07 22:16:54,142] One Channel
[2022-08-07 22:16:54,142] With Normal data.
[2022-08-07 22:16:54,142] Nunber of classes:10.
[2022-08-07 22:16:54,143] Nunber of ViT channels:1.
[2022-08-07 22:16:54,407] Totol epochs: 15
[2022-08-07 22:16:54,409] Epoch 1---------------
[2022-08-07 22:16:54,410] lr: 2.000000e-03
[2022-08-07 22:16:55,202] loss: 2.468107  [    0/ 4760]
[2022-08-07 22:17:07,058] loss: 2.063362  [  480/ 4760]
[2022-08-07 22:17:18,915] loss: 1.633943  [  960/ 4760]
[2022-08-07 22:17:30,772] loss: 1.765929  [ 1440/ 4760]
[2022-08-07 22:17:42,629] loss: 1.251226  [ 1920/ 4760]
[2022-08-07 22:17:54,485] loss: 1.385272  [ 2400/ 4760]
[2022-08-07 22:18:06,341] loss: 0.545182  [ 2880/ 4760]
[2022-08-07 22:18:18,196] loss: 0.561449  [ 3360/ 4760]
[2022-08-07 22:18:30,052] loss: 0.233768  [ 3840/ 4760]
[2022-08-07 22:18:41,906] loss: 0.452658  [ 4320/ 4760]
[2022-08-07 22:19:33,280] Train Error: Accuracy: 94.622%, Avg loss: 0.180081
[2022-08-07 22:19:50,842] Test  Error: Accuracy: 93.969%, Avg loss: 0.196949
[2022-08-07 22:19:50,843] Epoch 2---------------
[2022-08-07 22:19:50,844] lr: 1.900000e-03
[2022-08-07 22:19:51,637] loss: 0.165188  [    0/ 4760]
[2022-08-07 22:20:03,492] loss: 0.164615  [  480/ 4760]
[2022-08-07 22:20:15,349] loss: 0.095394  [  960/ 4760]
[2022-08-07 22:20:27,208] loss: 0.152054  [ 1440/ 4760]
[2022-08-07 22:20:39,063] loss: 0.418569  [ 1920/ 4760]
[2022-08-07 22:20:50,917] loss: 0.330706  [ 2400/ 4760]
[2022-08-07 22:21:02,774] loss: 0.397848  [ 2880/ 4760]
[2022-08-07 22:21:14,630] loss: 0.070833  [ 3360/ 4760]
[2022-08-07 22:21:26,487] loss: 0.132021  [ 3840/ 4760]
[2022-08-07 22:21:38,344] loss: 0.078782  [ 4320/ 4760]
[2022-08-07 22:22:29,713] Train Error: Accuracy: 98.361%, Avg loss: 0.055806
[2022-08-07 22:22:47,273] Test  Error: Accuracy: 98.220%, Avg loss: 0.057186
[2022-08-07 22:22:47,274] Epoch 3---------------
[2022-08-07 22:22:47,275] lr: 1.805000e-03
[2022-08-07 22:22:48,066] loss: 0.027335  [    0/ 4760]
[2022-08-07 22:22:59,922] loss: 0.036427  [  480/ 4760]
[2022-08-07 22:23:11,778] loss: 0.014917  [  960/ 4760]
[2022-08-07 22:23:23,633] loss: 0.338829  [ 1440/ 4760]
[2022-08-07 22:23:35,489] loss: 0.022608  [ 1920/ 4760]
[2022-08-07 22:23:47,344] loss: 0.076647  [ 2400/ 4760]
[2022-08-07 22:23:59,198] loss: 0.270774  [ 2880/ 4760]
[2022-08-07 22:24:11,056] loss: 0.070901  [ 3360/ 4760]
[2022-08-07 22:24:22,913] loss: 0.140165  [ 3840/ 4760]
[2022-08-07 22:24:34,771] loss: 0.022182  [ 4320/ 4760]
[2022-08-07 22:25:26,136] Train Error: Accuracy: 93.130%, Avg loss: 0.199397
[2022-08-07 22:25:43,694] Test  Error: Accuracy: 92.486%, Avg loss: 0.212344
[2022-08-07 22:25:43,695] Epoch 4---------------
[2022-08-07 22:25:43,696] lr: 1.260499e-03
[2022-08-07 22:25:44,487] loss: 0.136770  [    0/ 4760]
[2022-08-07 22:25:56,346] loss: 0.015786  [  480/ 4760]
[2022-08-07 22:26:08,201] loss: 0.016908  [  960/ 4760]
[2022-08-07 22:26:20,057] loss: 0.064172  [ 1440/ 4760]
[2022-08-07 22:26:31,913] loss: 0.032146  [ 1920/ 4760]
[2022-08-07 22:26:43,770] loss: 0.060946  [ 2400/ 4760]
[2022-08-07 22:26:55,626] loss: 0.110456  [ 2880/ 4760]
[2022-08-07 22:27:07,480] loss: 0.026983  [ 3360/ 4760]
[2022-08-07 22:27:19,336] loss: 0.004834  [ 3840/ 4760]
[2022-08-07 22:27:31,192] loss: 0.106485  [ 4320/ 4760]
[2022-08-07 22:28:22,560] Train Error: Accuracy: 99.601%, Avg loss: 0.020662
[2022-08-07 22:28:40,117] Test  Error: Accuracy: 99.308%, Avg loss: 0.033623
[2022-08-07 22:28:40,118] Epoch 5---------------
[2022-08-07 22:28:40,119] lr: 1.197474e-03
[2022-08-07 22:28:40,913] loss: 0.007799  [    0/ 4760]
[2022-08-07 22:28:52,768] loss: 0.012820  [  480/ 4760]
[2022-08-07 22:29:04,627] loss: 0.015412  [  960/ 4760]
[2022-08-07 22:29:16,482] loss: 0.009596  [ 1440/ 4760]
[2022-08-07 22:29:28,335] loss: 0.002322  [ 1920/ 4760]
[2022-08-07 22:29:40,192] loss: 0.023081  [ 2400/ 4760]
[2022-08-07 22:29:52,047] loss: 0.022541  [ 2880/ 4760]
[2022-08-07 22:30:03,902] loss: 0.003702  [ 3360/ 4760]
[2022-08-07 22:30:15,759] loss: 0.017841  [ 3840/ 4760]
[2022-08-07 22:30:27,614] loss: 0.001945  [ 4320/ 4760]
[2022-08-07 22:31:18,981] Train Error: Accuracy: 99.979%, Avg loss: 0.005111
[2022-08-07 22:31:36,539] Test  Error: Accuracy: 99.654%, Avg loss: 0.012892
[2022-08-07 22:31:36,539] Epoch 6---------------
[2022-08-07 22:31:36,540] lr: 1.137600e-03
[2022-08-07 22:31:37,333] loss: 0.003493  [    0/ 4760]
[2022-08-07 22:31:49,189] loss: 0.004734  [  480/ 4760]
[2022-08-07 22:32:01,048] loss: 0.191484  [  960/ 4760]
[2022-08-07 22:32:12,903] loss: 0.001748  [ 1440/ 4760]
[2022-08-07 22:32:24,759] loss: 0.001865  [ 1920/ 4760]
[2022-08-07 22:32:36,614] loss: 0.002394  [ 2400/ 4760]
[2022-08-07 22:32:48,471] loss: 0.001755  [ 2880/ 4760]
[2022-08-07 22:33:00,328] loss: 0.001859  [ 3360/ 4760]
[2022-08-07 22:33:12,186] loss: 0.005464  [ 3840/ 4760]
[2022-08-07 22:33:24,041] loss: 0.001624  [ 4320/ 4760]
[2022-08-07 22:34:15,406] Train Error: Accuracy: 96.513%, Avg loss: 0.113875
[2022-08-07 22:34:32,962] Test  Error: Accuracy: 95.996%, Avg loss: 0.145653
[2022-08-07 22:34:32,962] Epoch 7---------------
[2022-08-07 22:34:32,963] lr: 7.944286e-04
[2022-08-07 22:34:33,756] loss: 0.002233  [    0/ 4760]
[2022-08-07 22:34:45,613] loss: 0.002672  [  480/ 4760]
[2022-08-07 22:34:57,470] loss: 0.011011  [  960/ 4760]
[2022-08-07 22:35:09,325] loss: 0.001872  [ 1440/ 4760]
[2022-08-07 22:35:21,178] loss: 0.050791  [ 1920/ 4760]
[2022-08-07 22:35:33,033] loss: 0.002879  [ 2400/ 4760]
[2022-08-07 22:35:44,890] loss: 0.001283  [ 2880/ 4760]
[2022-08-07 22:35:56,746] loss: 0.004479  [ 3360/ 4760]
[2022-08-07 22:36:08,603] loss: 0.001104  [ 3840/ 4760]
[2022-08-07 22:36:20,461] loss: 0.001130  [ 4320/ 4760]
[2022-08-07 22:37:11,831] Train Error: Accuracy: 99.958%, Avg loss: 0.003236
[2022-08-07 22:37:29,387] Test  Error: Accuracy: 99.802%, Avg loss: 0.012075
[2022-08-07 22:37:29,387] Epoch 8---------------
[2022-08-07 22:37:29,389] lr: 7.547072e-04
[2022-08-07 22:37:30,180] loss: 0.000689  [    0/ 4760]
[2022-08-07 22:37:42,036] loss: 0.001526  [  480/ 4760]
[2022-08-07 22:37:53,894] loss: 0.001987  [  960/ 4760]
[2022-08-07 22:38:05,751] loss: 0.001539  [ 1440/ 4760]
[2022-08-07 22:38:17,607] loss: 0.002300  [ 1920/ 4760]
[2022-08-07 22:38:29,464] loss: 0.000989  [ 2400/ 4760]
[2022-08-07 22:38:41,320] loss: 0.002683  [ 2880/ 4760]
[2022-08-07 22:38:53,178] loss: 0.000465  [ 3360/ 4760]
[2022-08-07 22:39:05,033] loss: 0.001185  [ 3840/ 4760]
[2022-08-07 22:39:16,888] loss: 0.001976  [ 4320/ 4760]
[2022-08-07 22:40:08,252] Train Error: Accuracy: 99.853%, Avg loss: 0.007678
[2022-08-07 22:40:25,808] Test  Error: Accuracy: 99.605%, Avg loss: 0.013431
[2022-08-07 22:40:25,809] Epoch 9---------------
[2022-08-07 22:40:25,810] lr: 5.839780e-04
[2022-08-07 22:40:26,602] loss: 0.006929  [    0/ 4760]
[2022-08-07 22:40:38,459] loss: 0.004260  [  480/ 4760]
[2022-08-07 22:40:50,316] loss: 0.000627  [  960/ 4760]
[2022-08-07 22:41:02,174] loss: 0.001305  [ 1440/ 4760]
[2022-08-07 22:41:14,030] loss: 0.001537  [ 1920/ 4760]
[2022-08-07 22:41:25,886] loss: 0.000601  [ 2400/ 4760]
[2022-08-07 22:41:37,742] loss: 0.000907  [ 2880/ 4760]
[2022-08-07 22:41:49,598] loss: 0.001901  [ 3360/ 4760]
[2022-08-07 22:42:01,456] loss: 0.003684  [ 3840/ 4760]
[2022-08-07 22:42:13,308] loss: 0.001372  [ 4320/ 4760]
[2022-08-07 22:43:04,667] Train Error: Accuracy: 99.979%, Avg loss: 0.002330
[2022-08-07 22:43:22,223] Test  Error: Accuracy: 99.852%, Avg loss: 0.006702
[2022-08-07 22:43:22,223] Epoch 10---------------
[2022-08-07 22:43:22,224] lr: 5.547791e-04
[2022-08-07 22:43:23,016] loss: 0.001626  [    0/ 4760]
[2022-08-07 22:43:34,872] loss: 0.001344  [  480/ 4760]
[2022-08-07 22:43:46,731] loss: 0.006863  [  960/ 4760]
[2022-08-07 22:43:58,589] loss: 0.001101  [ 1440/ 4760]
[2022-08-07 22:44:10,447] loss: 0.001549  [ 1920/ 4760]
[2022-08-07 22:44:22,304] loss: 0.001165  [ 2400/ 4760]
[2022-08-07 22:44:34,162] loss: 0.001059  [ 2880/ 4760]
[2022-08-07 22:44:46,020] loss: 0.002411  [ 3360/ 4760]
[2022-08-07 22:44:57,877] loss: 0.002240  [ 3840/ 4760]
[2022-08-07 22:45:09,732] loss: 0.006509  [ 4320/ 4760]
[2022-08-07 22:46:01,097] Train Error: Accuracy: 99.937%, Avg loss: 0.004927
[2022-08-07 22:46:18,654] Test  Error: Accuracy: 99.753%, Avg loss: 0.008506
[2022-08-07 22:46:18,655] Epoch 11---------------
[2022-08-07 22:46:18,656] lr: 3.874230e-04
[2022-08-07 22:46:19,448] loss: 0.000945  [    0/ 4760]
[2022-08-07 22:46:31,304] loss: 0.000929  [  480/ 4760]
[2022-08-07 22:46:43,163] loss: 0.003843  [  960/ 4760]
[2022-08-07 22:46:55,021] loss: 0.012600  [ 1440/ 4760]
[2022-08-07 22:47:06,878] loss: 0.001502  [ 1920/ 4760]
[2022-08-07 22:47:18,737] loss: 0.000749  [ 2400/ 4760]
[2022-08-07 22:47:30,594] loss: 0.000534  [ 2880/ 4760]
[2022-08-07 22:47:42,450] loss: 0.001328  [ 3360/ 4760]
[2022-08-07 22:47:54,307] loss: 0.000528  [ 3840/ 4760]
[2022-08-07 22:48:06,163] loss: 0.001547  [ 4320/ 4760]
[2022-08-07 22:48:57,540] Train Error: Accuracy: 99.958%, Avg loss: 0.002249
[2022-08-07 22:49:15,100] Test  Error: Accuracy: 99.802%, Avg loss: 0.006974
[2022-08-07 22:49:15,100] Epoch 12---------------
[2022-08-07 22:49:15,101] lr: 3.680518e-04
[2022-08-07 22:49:15,893] loss: 0.000732  [    0/ 4760]
[2022-08-07 22:49:27,749] loss: 0.000685  [  480/ 4760]
[2022-08-07 22:49:39,604] loss: 0.000633  [  960/ 4760]
[2022-08-07 22:49:51,460] loss: 0.001388  [ 1440/ 4760]
[2022-08-07 22:50:03,340] loss: 0.000916  [ 1920/ 4760]
[2022-08-07 22:50:15,198] loss: 0.002399  [ 2400/ 4760]
[2022-08-07 22:50:27,054] loss: 0.002056  [ 2880/ 4760]
[2022-08-07 22:50:38,911] loss: 0.000506  [ 3360/ 4760]
[2022-08-07 22:50:50,768] loss: 0.000371  [ 3840/ 4760]
[2022-08-07 22:51:02,624] loss: 0.001709  [ 4320/ 4760]
[2022-08-07 22:51:53,994] Train Error: Accuracy: 100.000%, Avg loss: 0.001439
[2022-08-07 22:52:11,553] Test  Error: Accuracy: 99.852%, Avg loss: 0.007415
[2022-08-07 22:52:11,553] Epoch 13---------------
[2022-08-07 22:52:11,554] lr: 3.155584e-04
[2022-08-07 22:52:12,347] loss: 0.001166  [    0/ 4760]
[2022-08-07 22:52:24,205] loss: 0.000455  [  480/ 4760]
[2022-08-07 22:52:36,063] loss: 0.000370  [  960/ 4760]
[2022-08-07 22:52:47,920] loss: 0.000960  [ 1440/ 4760]
[2022-08-07 22:52:59,776] loss: 0.001264  [ 1920/ 4760]
[2022-08-07 22:53:11,634] loss: 0.000582  [ 2400/ 4760]
[2022-08-07 22:53:23,493] loss: 0.000631  [ 2880/ 4760]
[2022-08-07 22:53:35,349] loss: 0.000429  [ 3360/ 4760]
[2022-08-07 22:53:47,205] loss: 0.000629  [ 3840/ 4760]
[2022-08-07 22:53:59,063] loss: 0.000907  [ 4320/ 4760]
[2022-08-07 22:54:50,433] Train Error: Accuracy: 100.000%, Avg loss: 0.001563
[2022-08-07 22:55:07,995] Test  Error: Accuracy: 99.753%, Avg loss: 0.008341
[2022-08-07 22:55:07,995] Epoch 14---------------
[2022-08-07 22:55:07,996] lr: 2.441731e-04
[2022-08-07 22:55:08,789] loss: 0.000946  [    0/ 4760]
[2022-08-07 22:55:20,646] loss: 0.001100  [  480/ 4760]
[2022-08-07 22:55:32,504] loss: 0.001510  [  960/ 4760]
[2022-08-07 22:55:44,361] loss: 0.000611  [ 1440/ 4760]
[2022-08-07 22:55:56,219] loss: 0.002597  [ 1920/ 4760]
[2022-08-07 22:56:08,075] loss: 0.000459  [ 2400/ 4760]
[2022-08-07 22:56:19,931] loss: 0.000533  [ 2880/ 4760]
[2022-08-07 22:56:31,786] loss: 0.000545  [ 3360/ 4760]
[2022-08-07 22:56:43,644] loss: 0.000368  [ 3840/ 4760]
[2022-08-07 22:56:55,501] loss: 0.001162  [ 4320/ 4760]
[2022-08-07 22:57:46,868] Train Error: Accuracy: 99.958%, Avg loss: 0.002944
[2022-08-07 22:58:04,425] Test  Error: Accuracy: 99.852%, Avg loss: 0.007190
[2022-08-07 22:58:04,426] Epoch 15---------------
[2022-08-07 22:58:04,427] lr: 2.319644e-04
[2022-08-07 22:58:05,219] loss: 0.000538  [    0/ 4760]
[2022-08-07 22:58:17,074] loss: 0.004793  [  480/ 4760]
[2022-08-07 22:58:28,932] loss: 0.001046  [  960/ 4760]
[2022-08-07 22:58:40,787] loss: 0.001030  [ 1440/ 4760]
[2022-08-07 22:58:52,644] loss: 0.000588  [ 1920/ 4760]
[2022-08-07 22:59:04,501] loss: 0.000583  [ 2400/ 4760]
[2022-08-07 22:59:16,358] loss: 0.000454  [ 2880/ 4760]
[2022-08-07 22:59:28,213] loss: 0.000799  [ 3360/ 4760]
[2022-08-07 22:59:40,069] loss: 0.000386  [ 3840/ 4760]
[2022-08-07 22:59:51,926] loss: 0.000353  [ 4320/ 4760]
[2022-08-07 23:00:43,299] Train Error: Accuracy: 100.000%, Avg loss: 0.000894
[2022-08-07 23:01:00,857] Test  Error: Accuracy: 99.951%, Avg loss: 0.004059
[2022-08-07 23:01:00,858] Done!
[2022-08-07 23:01:00,862] Number of parameters:3198730
[2022-08-07 23:01:00,862] ## end time: 2022-08-07 23:01:00.858264
[2022-08-07 23:01:00,863] ## used time: 0:44:06.841521
