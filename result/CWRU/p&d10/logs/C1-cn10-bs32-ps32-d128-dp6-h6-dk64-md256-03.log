[2022-08-08 01:09:02,520] ## start time: 2022-08-08 01:09:02.391671
[2022-08-08 01:09:02,520] Using cuda device
[2022-08-08 01:09:02,522] In train:p&d10.npy.
[2022-08-08 01:09:02,523] One Channel
[2022-08-08 01:09:02,524] With Normal data.
[2022-08-08 01:09:02,524] Nunber of classes:10.
[2022-08-08 01:09:02,524] Nunber of ViT channels:1.
[2022-08-08 01:09:02,776] Totol epochs: 15
[2022-08-08 01:09:02,780] Epoch 1---------------
[2022-08-08 01:09:02,780] lr: 2.000000e-03
[2022-08-08 01:09:03,137] loss: 2.349975  [    0/ 4716]
[2022-08-08 01:09:08,480] loss: 1.597039  [  480/ 4716]
[2022-08-08 01:09:13,822] loss: 2.418085  [  960/ 4716]
[2022-08-08 01:09:19,163] loss: 1.204645  [ 1440/ 4716]
[2022-08-08 01:09:24,506] loss: 1.009610  [ 1920/ 4716]
[2022-08-08 01:09:29,846] loss: 0.754099  [ 2400/ 4716]
[2022-08-08 01:09:35,188] loss: 0.474939  [ 2880/ 4716]
[2022-08-08 01:09:40,530] loss: 0.883453  [ 3360/ 4716]
[2022-08-08 01:09:45,870] loss: 0.217658  [ 3840/ 4716]
[2022-08-08 01:09:51,214] loss: 0.059313  [ 4320/ 4716]
[2022-08-08 01:10:13,658] Train Error: Accuracy: 93.872%, Avg loss: 0.191544
[2022-08-08 01:10:21,719] Test  Error: Accuracy: 93.130%, Avg loss: 0.196252
[2022-08-08 01:10:21,720] Epoch 2---------------
[2022-08-08 01:10:21,721] lr: 1.900000e-03
[2022-08-08 01:10:22,080] loss: 0.144344  [    0/ 4716]
[2022-08-08 01:10:27,422] loss: 0.044602  [  480/ 4716]
[2022-08-08 01:10:32,763] loss: 0.183204  [  960/ 4716]
[2022-08-08 01:10:38,105] loss: 1.394201  [ 1440/ 4716]
[2022-08-08 01:10:43,446] loss: 0.171914  [ 1920/ 4716]
[2022-08-08 01:10:48,788] loss: 0.065379  [ 2400/ 4716]
[2022-08-08 01:10:54,129] loss: 0.190068  [ 2880/ 4716]
[2022-08-08 01:10:59,471] loss: 0.087925  [ 3360/ 4716]
[2022-08-08 01:11:04,814] loss: 0.136858  [ 3840/ 4716]
[2022-08-08 01:11:10,158] loss: 0.066560  [ 4320/ 4716]
[2022-08-08 01:11:32,617] Train Error: Accuracy: 99.152%, Avg loss: 0.039233
[2022-08-08 01:11:40,675] Test  Error: Accuracy: 98.984%, Avg loss: 0.046574
[2022-08-08 01:11:40,676] Epoch 3---------------
[2022-08-08 01:11:40,677] lr: 1.805000e-03
[2022-08-08 01:11:41,035] loss: 0.023331  [    0/ 4716]
[2022-08-08 01:11:46,379] loss: 0.028381  [  480/ 4716]
[2022-08-08 01:11:51,722] loss: 0.016657  [  960/ 4716]
[2022-08-08 01:11:57,063] loss: 0.113837  [ 1440/ 4716]
[2022-08-08 01:12:02,404] loss: 0.043240  [ 1920/ 4716]
[2022-08-08 01:12:07,748] loss: 0.051529  [ 2400/ 4716]
[2022-08-08 01:12:13,090] loss: 0.138693  [ 2880/ 4716]
[2022-08-08 01:12:18,431] loss: 0.028420  [ 3360/ 4716]
[2022-08-08 01:12:23,775] loss: 0.117442  [ 3840/ 4716]
[2022-08-08 01:12:29,116] loss: 0.012685  [ 4320/ 4716]
[2022-08-08 01:12:51,547] Train Error: Accuracy: 99.152%, Avg loss: 0.034532
[2022-08-08 01:12:59,610] Test  Error: Accuracy: 98.984%, Avg loss: 0.044901
[2022-08-08 01:12:59,611] Epoch 4---------------
[2022-08-08 01:12:59,611] lr: 1.714750e-03
[2022-08-08 01:12:59,970] loss: 0.027669  [    0/ 4716]
[2022-08-08 01:13:05,312] loss: 0.025172  [  480/ 4716]
[2022-08-08 01:13:10,653] loss: 0.054376  [  960/ 4716]
[2022-08-08 01:13:15,998] loss: 0.082801  [ 1440/ 4716]
[2022-08-08 01:13:21,337] loss: 0.020144  [ 1920/ 4716]
[2022-08-08 01:13:26,683] loss: 0.008464  [ 2400/ 4716]
[2022-08-08 01:13:32,025] loss: 0.005649  [ 2880/ 4716]
[2022-08-08 01:13:37,366] loss: 0.008453  [ 3360/ 4716]
[2022-08-08 01:13:42,708] loss: 0.049805  [ 3840/ 4716]
[2022-08-08 01:13:48,050] loss: 0.088721  [ 4320/ 4716]
[2022-08-08 01:14:10,502] Train Error: Accuracy: 99.025%, Avg loss: 0.035661
[2022-08-08 01:14:18,563] Test  Error: Accuracy: 97.775%, Avg loss: 0.064935
[2022-08-08 01:14:18,564] Epoch 5---------------
[2022-08-08 01:14:18,565] lr: 1.197474e-03
[2022-08-08 01:14:18,922] loss: 0.042577  [    0/ 4716]
[2022-08-08 01:14:24,261] loss: 0.006139  [  480/ 4716]
[2022-08-08 01:14:29,603] loss: 0.031090  [  960/ 4716]
[2022-08-08 01:14:34,944] loss: 0.027662  [ 1440/ 4716]
[2022-08-08 01:14:40,286] loss: 0.002513  [ 1920/ 4716]
[2022-08-08 01:14:45,625] loss: 0.059129  [ 2400/ 4716]
[2022-08-08 01:14:50,966] loss: 0.015756  [ 2880/ 4716]
[2022-08-08 01:14:56,306] loss: 0.004161  [ 3360/ 4716]
[2022-08-08 01:15:01,647] loss: 0.048829  [ 3840/ 4716]
[2022-08-08 01:15:06,991] loss: 0.007304  [ 4320/ 4716]
[2022-08-08 01:15:29,442] Train Error: Accuracy: 99.873%, Avg loss: 0.008415
[2022-08-08 01:15:37,499] Test  Error: Accuracy: 99.661%, Avg loss: 0.013499
[2022-08-08 01:15:37,499] Epoch 6---------------
[2022-08-08 01:15:37,500] lr: 1.137600e-03
[2022-08-08 01:15:37,858] loss: 0.002091  [    0/ 4716]
[2022-08-08 01:15:43,202] loss: 0.010297  [  480/ 4716]
[2022-08-08 01:15:48,548] loss: 0.017721  [  960/ 4716]
[2022-08-08 01:15:53,891] loss: 0.001869  [ 1440/ 4716]
[2022-08-08 01:15:59,233] loss: 0.003320  [ 1920/ 4716]
[2022-08-08 01:16:04,574] loss: 0.003993  [ 2400/ 4716]
[2022-08-08 01:16:09,916] loss: 0.003223  [ 2880/ 4716]
[2022-08-08 01:16:15,258] loss: 0.001468  [ 3360/ 4716]
[2022-08-08 01:16:20,601] loss: 0.001132  [ 3840/ 4716]
[2022-08-08 01:16:25,943] loss: 0.002194  [ 4320/ 4716]
[2022-08-08 01:16:48,388] Train Error: Accuracy: 99.894%, Avg loss: 0.003871
[2022-08-08 01:16:56,447] Test  Error: Accuracy: 99.661%, Avg loss: 0.011775
[2022-08-08 01:16:56,448] Epoch 7---------------
[2022-08-08 01:16:56,449] lr: 1.080720e-03
[2022-08-08 01:16:56,806] loss: 0.001540  [    0/ 4716]
[2022-08-08 01:17:02,147] loss: 0.008065  [  480/ 4716]
[2022-08-08 01:17:07,488] loss: 0.000797  [  960/ 4716]
[2022-08-08 01:17:12,830] loss: 0.003338  [ 1440/ 4716]
[2022-08-08 01:17:18,170] loss: 0.000751  [ 1920/ 4716]
[2022-08-08 01:17:23,512] loss: 0.001473  [ 2400/ 4716]
[2022-08-08 01:17:28,854] loss: 0.001323  [ 2880/ 4716]
[2022-08-08 01:17:34,194] loss: 0.001109  [ 3360/ 4716]
[2022-08-08 01:17:39,536] loss: 0.001210  [ 3840/ 4716]
[2022-08-08 01:17:44,876] loss: 0.003672  [ 4320/ 4716]
[2022-08-08 01:18:07,323] Train Error: Accuracy: 99.979%, Avg loss: 0.002059
[2022-08-08 01:18:15,377] Test  Error: Accuracy: 99.952%, Avg loss: 0.005709
[2022-08-08 01:18:15,378] Epoch 8---------------
[2022-08-08 01:18:15,379] lr: 1.026684e-03
[2022-08-08 01:18:15,736] loss: 0.000900  [    0/ 4716]
[2022-08-08 01:18:21,080] loss: 0.013136  [  480/ 4716]
[2022-08-08 01:18:26,424] loss: 0.001588  [  960/ 4716]
[2022-08-08 01:18:31,767] loss: 0.000908  [ 1440/ 4716]
[2022-08-08 01:18:37,110] loss: 0.001434  [ 1920/ 4716]
[2022-08-08 01:18:42,454] loss: 0.005039  [ 2400/ 4716]
[2022-08-08 01:18:47,798] loss: 0.001467  [ 2880/ 4716]
[2022-08-08 01:18:53,141] loss: 0.001684  [ 3360/ 4716]
[2022-08-08 01:18:58,483] loss: 0.004716  [ 3840/ 4716]
[2022-08-08 01:19:03,827] loss: 0.001126  [ 4320/ 4716]
[2022-08-08 01:19:26,271] Train Error: Accuracy: 99.979%, Avg loss: 0.002891
[2022-08-08 01:19:34,330] Test  Error: Accuracy: 99.758%, Avg loss: 0.011376
[2022-08-08 01:19:34,331] Epoch 9---------------
[2022-08-08 01:19:34,332] lr: 7.169718e-04
[2022-08-08 01:19:34,690] loss: 0.001724  [    0/ 4716]
[2022-08-08 01:19:40,030] loss: 0.002417  [  480/ 4716]
[2022-08-08 01:19:45,374] loss: 0.000624  [  960/ 4716]
[2022-08-08 01:19:50,716] loss: 0.000970  [ 1440/ 4716]
[2022-08-08 01:19:56,057] loss: 0.000830  [ 1920/ 4716]
[2022-08-08 01:20:01,399] loss: 0.007108  [ 2400/ 4716]
[2022-08-08 01:20:06,740] loss: 0.002002  [ 2880/ 4716]
[2022-08-08 01:20:12,082] loss: 0.001217  [ 3360/ 4716]
[2022-08-08 01:20:17,425] loss: 0.000662  [ 3840/ 4716]
[2022-08-08 01:20:22,768] loss: 0.000697  [ 4320/ 4716]
[2022-08-08 01:20:45,234] Train Error: Accuracy: 99.936%, Avg loss: 0.002340
[2022-08-08 01:20:53,293] Test  Error: Accuracy: 99.952%, Avg loss: 0.003461
[2022-08-08 01:20:53,293] Epoch 10---------------
[2022-08-08 01:20:53,294] lr: 6.811233e-04
[2022-08-08 01:20:53,653] loss: 0.004248  [    0/ 4716]
[2022-08-08 01:20:58,997] loss: 0.000764  [  480/ 4716]
[2022-08-08 01:21:04,340] loss: 0.000421  [  960/ 4716]
[2022-08-08 01:21:09,685] loss: 0.000764  [ 1440/ 4716]
[2022-08-08 01:21:15,030] loss: 0.000207  [ 1920/ 4716]
[2022-08-08 01:21:20,371] loss: 0.001348  [ 2400/ 4716]
[2022-08-08 01:21:25,714] loss: 0.000601  [ 2880/ 4716]
[2022-08-08 01:21:31,059] loss: 0.000618  [ 3360/ 4716]
[2022-08-08 01:21:36,399] loss: 0.000903  [ 3840/ 4716]
[2022-08-08 01:21:41,742] loss: 0.000434  [ 4320/ 4716]
[2022-08-08 01:22:04,183] Train Error: Accuracy: 99.979%, Avg loss: 0.001130
[2022-08-08 01:22:12,240] Test  Error: Accuracy: 99.952%, Avg loss: 0.004447
[2022-08-08 01:22:12,241] Epoch 11---------------
[2022-08-08 01:22:12,242] lr: 4.756538e-04
[2022-08-08 01:22:12,600] loss: 0.001993  [    0/ 4716]
[2022-08-08 01:22:17,941] loss: 0.000582  [  480/ 4716]
[2022-08-08 01:22:23,282] loss: 0.000487  [  960/ 4716]
[2022-08-08 01:22:28,625] loss: 0.000734  [ 1440/ 4716]
[2022-08-08 01:22:33,966] loss: 0.002676  [ 1920/ 4716]
[2022-08-08 01:22:39,308] loss: 0.000558  [ 2400/ 4716]
[2022-08-08 01:22:44,653] loss: 0.001417  [ 2880/ 4716]
[2022-08-08 01:22:49,994] loss: 0.000456  [ 3360/ 4716]
[2022-08-08 01:22:55,336] loss: 0.000756  [ 3840/ 4716]
[2022-08-08 01:23:00,679] loss: 0.000429  [ 4320/ 4716]
[2022-08-08 01:23:23,107] Train Error: Accuracy: 100.000%, Avg loss: 0.000758
[2022-08-08 01:23:31,159] Test  Error: Accuracy: 99.855%, Avg loss: 0.007647
[2022-08-08 01:23:31,160] Epoch 12---------------
[2022-08-08 01:23:31,161] lr: 3.321668e-04
[2022-08-08 01:23:31,518] loss: 0.000440  [    0/ 4716]
[2022-08-08 01:23:36,863] loss: 0.001544  [  480/ 4716]
[2022-08-08 01:23:42,205] loss: 0.000593  [  960/ 4716]
[2022-08-08 01:23:47,548] loss: 0.001838  [ 1440/ 4716]
[2022-08-08 01:23:52,890] loss: 0.000499  [ 1920/ 4716]
[2022-08-08 01:23:58,233] loss: 0.000540  [ 2400/ 4716]
[2022-08-08 01:24:03,574] loss: 0.000260  [ 2880/ 4716]
[2022-08-08 01:24:08,915] loss: 0.000582  [ 3360/ 4716]
[2022-08-08 01:24:14,257] loss: 0.000434  [ 3840/ 4716]
[2022-08-08 01:24:19,597] loss: 0.000439  [ 4320/ 4716]
[2022-08-08 01:24:42,026] Train Error: Accuracy: 100.000%, Avg loss: 0.000762
[2022-08-08 01:24:50,078] Test  Error: Accuracy: 99.903%, Avg loss: 0.005418
[2022-08-08 01:24:50,079] Epoch 13---------------
[2022-08-08 01:24:50,080] lr: 3.155584e-04
[2022-08-08 01:24:50,438] loss: 0.000298  [    0/ 4716]
[2022-08-08 01:24:55,779] loss: 0.003368  [  480/ 4716]
[2022-08-08 01:25:01,121] loss: 0.000401  [  960/ 4716]
[2022-08-08 01:25:06,460] loss: 0.000454  [ 1440/ 4716]
[2022-08-08 01:25:11,802] loss: 0.000391  [ 1920/ 4716]
[2022-08-08 01:25:17,146] loss: 0.000484  [ 2400/ 4716]
[2022-08-08 01:25:22,489] loss: 0.000597  [ 2880/ 4716]
[2022-08-08 01:25:27,831] loss: 0.000476  [ 3360/ 4716]
[2022-08-08 01:25:33,172] loss: 0.000497  [ 3840/ 4716]
[2022-08-08 01:25:38,516] loss: 0.001783  [ 4320/ 4716]
[2022-08-08 01:26:00,952] Train Error: Accuracy: 99.936%, Avg loss: 0.002415
[2022-08-08 01:26:09,004] Test  Error: Accuracy: 99.903%, Avg loss: 0.003108
[2022-08-08 01:26:09,005] Epoch 14---------------
[2022-08-08 01:26:09,006] lr: 2.997805e-04
[2022-08-08 01:26:09,364] loss: 0.000707  [    0/ 4716]
[2022-08-08 01:26:14,704] loss: 0.000475  [  480/ 4716]
[2022-08-08 01:26:20,047] loss: 0.000338  [  960/ 4716]
[2022-08-08 01:26:25,393] loss: 0.000362  [ 1440/ 4716]
[2022-08-08 01:26:30,735] loss: 0.000413  [ 1920/ 4716]
[2022-08-08 01:26:36,079] loss: 0.000828  [ 2400/ 4716]
[2022-08-08 01:26:41,421] loss: 0.000747  [ 2880/ 4716]
[2022-08-08 01:26:46,765] loss: 0.003898  [ 3360/ 4716]
[2022-08-08 01:26:52,107] loss: 0.000309  [ 3840/ 4716]
[2022-08-08 01:26:57,450] loss: 0.000770  [ 4320/ 4716]
[2022-08-08 01:27:19,880] Train Error: Accuracy: 99.979%, Avg loss: 0.000959
[2022-08-08 01:27:27,931] Test  Error: Accuracy: 99.903%, Avg loss: 0.003915
[2022-08-08 01:27:27,931] Epoch 15---------------
[2022-08-08 01:27:27,932] lr: 2.093479e-04
[2022-08-08 01:27:28,289] loss: 0.000725  [    0/ 4716]
[2022-08-08 01:27:33,633] loss: 0.001989  [  480/ 4716]
[2022-08-08 01:27:38,978] loss: 0.000765  [  960/ 4716]
[2022-08-08 01:27:44,321] loss: 0.010293  [ 1440/ 4716]
[2022-08-08 01:27:49,663] loss: 0.001471  [ 1920/ 4716]
[2022-08-08 01:27:55,004] loss: 0.000335  [ 2400/ 4716]
[2022-08-08 01:28:00,346] loss: 0.000701  [ 2880/ 4716]
[2022-08-08 01:28:05,687] loss: 0.000697  [ 3360/ 4716]
[2022-08-08 01:28:11,028] loss: 0.000324  [ 3840/ 4716]
[2022-08-08 01:28:16,369] loss: 0.000760  [ 4320/ 4716]
[2022-08-08 01:28:38,801] Train Error: Accuracy: 100.000%, Avg loss: 0.000745
[2022-08-08 01:28:46,851] Test  Error: Accuracy: 99.855%, Avg loss: 0.004278
[2022-08-08 01:28:46,851] Done!
[2022-08-08 01:28:46,856] Number of parameters:3186442
[2022-08-08 01:28:46,856] ## end time: 2022-08-08 01:28:46.851760
[2022-08-08 01:28:46,856] ## used time: 0:19:44.460089
