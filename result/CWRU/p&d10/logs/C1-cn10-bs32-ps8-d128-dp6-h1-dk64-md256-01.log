[2022-08-08 14:49:35,778] ## start time: 2022-08-08 14:49:35.519394
[2022-08-08 14:49:35,778] Using cuda device
[2022-08-08 14:49:35,779] In train:p&d10.npy.
[2022-08-08 14:49:35,779] One Channel
[2022-08-08 14:49:35,779] With Normal data.
[2022-08-08 14:49:35,779] Nunber of classes:10.
[2022-08-08 14:49:35,779] Nunber of ViT channels:1.
[2022-08-08 14:49:36,895] Totol epochs: 15
[2022-08-08 14:49:36,898] Epoch 1---------------
[2022-08-08 14:49:36,898] lr: 2.000000e-03
[2022-08-08 14:49:40,119] loss: 2.444195  [    0/ 4852]
[2022-08-08 14:49:48,431] loss: 2.185298  [  480/ 4852]
[2022-08-08 14:49:56,714] loss: 1.629321  [  960/ 4852]
[2022-08-08 14:50:04,966] loss: 1.394628  [ 1440/ 4852]
[2022-08-08 14:50:13,254] loss: 1.137851  [ 1920/ 4852]
[2022-08-08 14:50:21,582] loss: 1.168105  [ 2400/ 4852]
[2022-08-08 14:50:29,866] loss: 1.125755  [ 2880/ 4852]
[2022-08-08 14:50:38,147] loss: 1.469069  [ 3360/ 4852]
[2022-08-08 14:50:46,425] loss: 1.004243  [ 3840/ 4852]
[2022-08-08 14:50:54,702] loss: 1.127261  [ 4320/ 4852]
[2022-08-08 14:51:02,984] loss: 1.159466  [ 4800/ 4852]
[2022-08-08 14:51:35,932] Train Error: Accuracy: 62.531%, Avg loss: 0.994027
[2022-08-08 14:51:48,924] Test  Error: Accuracy: 62.196%, Avg loss: 1.006234
[2022-08-08 14:51:48,924] Epoch 2---------------
[2022-08-08 14:51:48,924] lr: 1.900000e-03
[2022-08-08 14:51:49,481] loss: 0.805542  [    0/ 4852]
[2022-08-08 14:51:57,814] loss: 1.269068  [  480/ 4852]
[2022-08-08 14:52:06,145] loss: 0.859613  [  960/ 4852]
[2022-08-08 14:52:14,480] loss: 0.789929  [ 1440/ 4852]
[2022-08-08 14:52:22,812] loss: 1.503088  [ 1920/ 4852]
[2022-08-08 14:52:31,145] loss: 0.478488  [ 2400/ 4852]
[2022-08-08 14:52:39,477] loss: 0.846936  [ 2880/ 4852]
[2022-08-08 14:52:47,813] loss: 0.502036  [ 3360/ 4852]
[2022-08-08 14:52:56,152] loss: 0.862121  [ 3840/ 4852]
[2022-08-08 14:53:04,517] loss: 0.287613  [ 4320/ 4852]
[2022-08-08 14:53:12,908] loss: 0.732773  [ 4800/ 4852]
[2022-08-08 14:53:46,201] Train Error: Accuracy: 77.432%, Avg loss: 0.633060
[2022-08-08 14:53:59,274] Test  Error: Accuracy: 77.421%, Avg loss: 0.648331
[2022-08-08 14:53:59,274] Epoch 3---------------
[2022-08-08 14:53:59,275] lr: 1.805000e-03
[2022-08-08 14:53:59,839] loss: 0.537139  [    0/ 4852]
[2022-08-08 14:54:08,222] loss: 0.708302  [  480/ 4852]
[2022-08-08 14:54:16,604] loss: 0.464583  [  960/ 4852]
[2022-08-08 14:54:24,985] loss: 0.758360  [ 1440/ 4852]
[2022-08-08 14:54:33,366] loss: 0.973750  [ 1920/ 4852]
[2022-08-08 14:54:41,750] loss: 0.323228  [ 2400/ 4852]
[2022-08-08 14:54:50,132] loss: 0.609317  [ 2880/ 4852]
[2022-08-08 14:54:58,517] loss: 0.253392  [ 3360/ 4852]
[2022-08-08 14:55:06,898] loss: 0.774273  [ 3840/ 4852]
[2022-08-08 14:55:15,279] loss: 0.648989  [ 4320/ 4852]
[2022-08-08 14:55:23,662] loss: 0.644794  [ 4800/ 4852]
[2022-08-08 14:55:56,850] Train Error: Accuracy: 81.389%, Avg loss: 0.541316
[2022-08-08 14:56:09,918] Test  Error: Accuracy: 79.492%, Avg loss: 0.579489
[2022-08-08 14:56:09,919] Epoch 4---------------
[2022-08-08 14:56:09,920] lr: 1.714750e-03
[2022-08-08 14:56:10,481] loss: 0.601877  [    0/ 4852]
[2022-08-08 14:56:18,864] loss: 0.443929  [  480/ 4852]
[2022-08-08 14:56:27,246] loss: 0.286704  [  960/ 4852]
[2022-08-08 14:56:35,629] loss: 0.327304  [ 1440/ 4852]
[2022-08-08 14:56:44,012] loss: 0.447135  [ 1920/ 4852]
[2022-08-08 14:56:52,395] loss: 0.301981  [ 2400/ 4852]
[2022-08-08 14:57:00,777] loss: 0.728150  [ 2880/ 4852]
[2022-08-08 14:57:09,159] loss: 0.466134  [ 3360/ 4852]
[2022-08-08 14:57:17,540] loss: 0.560552  [ 3840/ 4852]
[2022-08-08 14:57:25,921] loss: 0.401522  [ 4320/ 4852]
[2022-08-08 14:57:34,304] loss: 0.240803  [ 4800/ 4852]
[2022-08-08 14:58:07,487] Train Error: Accuracy: 86.171%, Avg loss: 0.400226
[2022-08-08 14:58:20,559] Test  Error: Accuracy: 86.484%, Avg loss: 0.389756
[2022-08-08 14:58:20,560] Epoch 5---------------
[2022-08-08 14:58:20,561] lr: 1.629012e-03
[2022-08-08 14:58:21,122] loss: 0.405017  [    0/ 4852]
[2022-08-08 14:58:29,503] loss: 0.327493  [  480/ 4852]
[2022-08-08 14:58:37,887] loss: 0.329013  [  960/ 4852]
[2022-08-08 14:58:46,270] loss: 0.306668  [ 1440/ 4852]
[2022-08-08 14:58:54,650] loss: 0.419718  [ 1920/ 4852]
[2022-08-08 14:59:03,030] loss: 0.254185  [ 2400/ 4852]
[2022-08-08 14:59:11,411] loss: 0.224873  [ 2880/ 4852]
[2022-08-08 14:59:19,792] loss: 0.552546  [ 3360/ 4852]
[2022-08-08 14:59:28,175] loss: 0.614465  [ 3840/ 4852]
[2022-08-08 14:59:36,553] loss: 0.123692  [ 4320/ 4852]
[2022-08-08 14:59:44,932] loss: 0.201291  [ 4800/ 4852]
[2022-08-08 15:00:18,121] Train Error: Accuracy: 93.364%, Avg loss: 0.221063
[2022-08-08 15:00:31,187] Test  Error: Accuracy: 91.766%, Avg loss: 0.256489
[2022-08-08 15:00:31,188] Epoch 6---------------
[2022-08-08 15:00:31,189] lr: 1.547562e-03
[2022-08-08 15:00:31,751] loss: 0.333651  [    0/ 4852]
[2022-08-08 15:00:40,134] loss: 0.122032  [  480/ 4852]
[2022-08-08 15:00:48,514] loss: 0.144373  [  960/ 4852]
[2022-08-08 15:00:56,896] loss: 0.390823  [ 1440/ 4852]
[2022-08-08 15:01:05,278] loss: 0.292061  [ 1920/ 4852]
[2022-08-08 15:01:13,660] loss: 0.107110  [ 2400/ 4852]
[2022-08-08 15:01:22,044] loss: 0.179913  [ 2880/ 4852]
[2022-08-08 15:01:30,425] loss: 0.340213  [ 3360/ 4852]
[2022-08-08 15:01:38,806] loss: 0.390609  [ 3840/ 4852]
[2022-08-08 15:01:47,185] loss: 0.375895  [ 4320/ 4852]
[2022-08-08 15:01:55,564] loss: 0.501872  [ 4800/ 4852]
[2022-08-08 15:02:28,740] Train Error: Accuracy: 91.900%, Avg loss: 0.254173
[2022-08-08 15:02:41,808] Test  Error: Accuracy: 91.248%, Avg loss: 0.281214
[2022-08-08 15:02:41,809] Epoch 7---------------
[2022-08-08 15:02:41,810] lr: 1.326841e-03
[2022-08-08 15:02:42,369] loss: 0.239670  [    0/ 4852]
[2022-08-08 15:02:50,749] loss: 0.411905  [  480/ 4852]
[2022-08-08 15:02:59,130] loss: 0.097366  [  960/ 4852]
[2022-08-08 15:03:07,515] loss: 0.351615  [ 1440/ 4852]
[2022-08-08 15:03:15,899] loss: 0.143268  [ 1920/ 4852]
[2022-08-08 15:03:24,280] loss: 0.131212  [ 2400/ 4852]
[2022-08-08 15:03:32,658] loss: 0.318740  [ 2880/ 4852]
[2022-08-08 15:03:41,035] loss: 0.292074  [ 3360/ 4852]
[2022-08-08 15:03:49,416] loss: 0.085325  [ 3840/ 4852]
[2022-08-08 15:03:57,795] loss: 0.110144  [ 4320/ 4852]
[2022-08-08 15:04:06,174] loss: 0.671019  [ 4800/ 4852]
[2022-08-08 15:04:39,350] Train Error: Accuracy: 86.274%, Avg loss: 0.393484
[2022-08-08 15:04:52,408] Test  Error: Accuracy: 84.930%, Avg loss: 0.437544
[2022-08-08 15:04:52,409] Epoch 8---------------
[2022-08-08 15:04:52,410] lr: 9.265825e-04
[2022-08-08 15:04:52,971] loss: 0.557920  [    0/ 4852]
[2022-08-08 15:05:01,350] loss: 0.153356  [  480/ 4852]
[2022-08-08 15:05:09,729] loss: 0.135484  [  960/ 4852]
[2022-08-08 15:05:18,110] loss: 0.296223  [ 1440/ 4852]
[2022-08-08 15:05:26,489] loss: 0.100556  [ 1920/ 4852]
[2022-08-08 15:05:34,868] loss: 0.165129  [ 2400/ 4852]
[2022-08-08 15:05:43,247] loss: 0.139524  [ 2880/ 4852]
[2022-08-08 15:05:51,629] loss: 0.158912  [ 3360/ 4852]
[2022-08-08 15:06:00,011] loss: 0.206411  [ 3840/ 4852]
[2022-08-08 15:06:08,393] loss: 0.127923  [ 4320/ 4852]
[2022-08-08 15:06:16,805] loss: 0.182977  [ 4800/ 4852]
[2022-08-08 15:06:50,238] Train Error: Accuracy: 95.796%, Avg loss: 0.137609
[2022-08-08 15:07:03,390] Test  Error: Accuracy: 93.993%, Avg loss: 0.181498
[2022-08-08 15:07:03,390] Epoch 9---------------
[2022-08-08 15:07:03,391] lr: 8.802533e-04
[2022-08-08 15:07:03,956] loss: 0.089198  [    0/ 4852]
[2022-08-08 15:07:12,392] loss: 0.174294  [  480/ 4852]
[2022-08-08 15:07:20,829] loss: 0.214373  [  960/ 4852]
[2022-08-08 15:07:29,267] loss: 0.047623  [ 1440/ 4852]
[2022-08-08 15:07:37,701] loss: 0.351909  [ 1920/ 4852]
[2022-08-08 15:07:46,140] loss: 0.184020  [ 2400/ 4852]
[2022-08-08 15:07:54,576] loss: 0.060040  [ 2880/ 4852]
[2022-08-08 15:08:03,014] loss: 0.298901  [ 3360/ 4852]
[2022-08-08 15:08:11,450] loss: 0.334927  [ 3840/ 4852]
[2022-08-08 15:08:19,888] loss: 0.075233  [ 4320/ 4852]
[2022-08-08 15:08:28,325] loss: 0.087109  [ 4800/ 4852]
[2022-08-08 15:09:01,728] Train Error: Accuracy: 97.486%, Avg loss: 0.091964
[2022-08-08 15:09:14,878] Test  Error: Accuracy: 96.116%, Avg loss: 0.137996
[2022-08-08 15:09:14,879] Epoch 10---------------
[2022-08-08 15:09:14,880] lr: 8.362407e-04
[2022-08-08 15:09:15,444] loss: 0.063819  [    0/ 4852]
[2022-08-08 15:09:23,884] loss: 0.135327  [  480/ 4852]
[2022-08-08 15:09:32,321] loss: 0.084549  [  960/ 4852]
[2022-08-08 15:09:40,759] loss: 0.134404  [ 1440/ 4852]
[2022-08-08 15:09:49,196] loss: 0.055938  [ 1920/ 4852]
[2022-08-08 15:09:57,629] loss: 0.014663  [ 2400/ 4852]
[2022-08-08 15:10:06,064] loss: 0.227179  [ 2880/ 4852]
[2022-08-08 15:10:14,501] loss: 0.090832  [ 3360/ 4852]
[2022-08-08 15:10:22,942] loss: 0.029731  [ 3840/ 4852]
[2022-08-08 15:10:31,380] loss: 0.181264  [ 4320/ 4852]
[2022-08-08 15:10:39,818] loss: 0.068945  [ 4800/ 4852]
[2022-08-08 15:11:13,218] Train Error: Accuracy: 95.940%, Avg loss: 0.128048
[2022-08-08 15:11:26,370] Test  Error: Accuracy: 94.873%, Avg loss: 0.170143
[2022-08-08 15:11:26,371] Epoch 11---------------
[2022-08-08 15:11:26,372] lr: 6.470671e-04
[2022-08-08 15:11:26,936] loss: 0.079688  [    0/ 4852]
[2022-08-08 15:11:35,374] loss: 0.181853  [  480/ 4852]
[2022-08-08 15:11:43,811] loss: 0.056223  [  960/ 4852]
[2022-08-08 15:11:52,250] loss: 0.014593  [ 1440/ 4852]
[2022-08-08 15:12:00,687] loss: 0.047952  [ 1920/ 4852]
[2022-08-08 15:12:09,121] loss: 0.105412  [ 2400/ 4852]
[2022-08-08 15:12:17,558] loss: 0.044758  [ 2880/ 4852]
[2022-08-08 15:12:25,995] loss: 0.047031  [ 3360/ 4852]
[2022-08-08 15:12:34,432] loss: 0.325610  [ 3840/ 4852]
[2022-08-08 15:12:42,871] loss: 0.101964  [ 4320/ 4852]
[2022-08-08 15:12:51,310] loss: 0.012578  [ 4800/ 4852]
[2022-08-08 15:13:24,738] Train Error: Accuracy: 97.836%, Avg loss: 0.072118
[2022-08-08 15:13:37,893] Test  Error: Accuracy: 96.996%, Avg loss: 0.094586
[2022-08-08 15:13:37,893] Epoch 12---------------
[2022-08-08 15:13:37,894] lr: 6.147137e-04
[2022-08-08 15:13:38,458] loss: 0.137985  [    0/ 4852]
[2022-08-08 15:13:46,897] loss: 0.048445  [  480/ 4852]
[2022-08-08 15:13:55,333] loss: 0.153063  [  960/ 4852]
[2022-08-08 15:14:03,770] loss: 0.010948  [ 1440/ 4852]
[2022-08-08 15:14:12,205] loss: 0.101235  [ 1920/ 4852]
[2022-08-08 15:14:20,645] loss: 0.028731  [ 2400/ 4852]
[2022-08-08 15:14:29,083] loss: 0.066608  [ 2880/ 4852]
[2022-08-08 15:14:37,520] loss: 0.054765  [ 3360/ 4852]
[2022-08-08 15:14:45,959] loss: 0.092520  [ 3840/ 4852]
[2022-08-08 15:14:54,396] loss: 0.017283  [ 4320/ 4852]
[2022-08-08 15:15:02,833] loss: 0.024770  [ 4800/ 4852]
[2022-08-08 15:15:36,237] Train Error: Accuracy: 98.351%, Avg loss: 0.059904
[2022-08-08 15:15:49,388] Test  Error: Accuracy: 95.857%, Avg loss: 0.127086
[2022-08-08 15:15:49,388] Epoch 13---------------
[2022-08-08 15:15:49,389] lr: 4.292775e-04
[2022-08-08 15:15:49,954] loss: 0.104279  [    0/ 4852]
[2022-08-08 15:15:58,389] loss: 0.137398  [  480/ 4852]
[2022-08-08 15:16:06,826] loss: 0.017599  [  960/ 4852]
[2022-08-08 15:16:15,265] loss: 0.127958  [ 1440/ 4852]
[2022-08-08 15:16:23,700] loss: 0.042859  [ 1920/ 4852]
[2022-08-08 15:16:32,139] loss: 0.072346  [ 2400/ 4852]
[2022-08-08 15:16:40,577] loss: 0.125182  [ 2880/ 4852]
[2022-08-08 15:16:49,012] loss: 0.169514  [ 3360/ 4852]
[2022-08-08 15:16:57,450] loss: 0.129049  [ 3840/ 4852]
[2022-08-08 15:17:05,890] loss: 0.006734  [ 4320/ 4852]
[2022-08-08 15:17:14,328] loss: 0.080923  [ 4800/ 4852]
[2022-08-08 15:17:47,720] Train Error: Accuracy: 98.722%, Avg loss: 0.047554
[2022-08-08 15:18:00,878] Test  Error: Accuracy: 97.773%, Avg loss: 0.077727
[2022-08-08 15:18:00,878] Epoch 14---------------
[2022-08-08 15:18:00,879] lr: 4.078137e-04
[2022-08-08 15:18:01,443] loss: 0.014127  [    0/ 4852]
[2022-08-08 15:18:09,883] loss: 0.114348  [  480/ 4852]
[2022-08-08 15:18:18,318] loss: 0.028633  [  960/ 4852]
[2022-08-08 15:18:26,737] loss: 0.023905  [ 1440/ 4852]
[2022-08-08 15:18:35,156] loss: 0.072773  [ 1920/ 4852]
[2022-08-08 15:18:43,575] loss: 0.026298  [ 2400/ 4852]
[2022-08-08 15:18:51,994] loss: 0.013093  [ 2880/ 4852]
[2022-08-08 15:19:00,414] loss: 0.036162  [ 3360/ 4852]
[2022-08-08 15:19:08,831] loss: 0.018706  [ 3840/ 4852]
[2022-08-08 15:19:17,251] loss: 0.060819  [ 4320/ 4852]
[2022-08-08 15:19:25,670] loss: 0.010518  [ 4800/ 4852]
[2022-08-08 15:19:58,963] Train Error: Accuracy: 98.805%, Avg loss: 0.038511
[2022-08-08 15:20:12,072] Test  Error: Accuracy: 97.618%, Avg loss: 0.068104
[2022-08-08 15:20:12,072] Epoch 15---------------
[2022-08-08 15:20:12,073] lr: 3.874230e-04
[2022-08-08 15:20:12,636] loss: 0.010845  [    0/ 4852]
[2022-08-08 15:20:21,057] loss: 0.043376  [  480/ 4852]
[2022-08-08 15:20:29,476] loss: 0.139010  [  960/ 4852]
[2022-08-08 15:20:37,895] loss: 0.011372  [ 1440/ 4852]
[2022-08-08 15:20:46,315] loss: 0.005167  [ 1920/ 4852]
[2022-08-08 15:20:54,735] loss: 0.067997  [ 2400/ 4852]
[2022-08-08 15:21:03,153] loss: 0.056418  [ 2880/ 4852]
[2022-08-08 15:21:11,574] loss: 0.084251  [ 3360/ 4852]
[2022-08-08 15:21:19,992] loss: 0.039943  [ 3840/ 4852]
[2022-08-08 15:21:28,411] loss: 0.120155  [ 4320/ 4852]
[2022-08-08 15:21:36,831] loss: 0.054422  [ 4800/ 4852]
[2022-08-08 15:22:10,124] Train Error: Accuracy: 98.846%, Avg loss: 0.043144
[2022-08-08 15:22:23,232] Test  Error: Accuracy: 97.721%, Avg loss: 0.061278
[2022-08-08 15:22:23,232] Done!
[2022-08-08 15:22:23,237] Number of parameters:1263370
[2022-08-08 15:22:23,237] ## end time: 2022-08-08 15:22:23.232978
[2022-08-08 15:22:23,237] ## used time: 0:32:47.713584
