[2022-08-06 16:25:32,242] ## start time: 2022-08-06 16:25:32.103779
[2022-08-06 16:25:32,242] Using cuda device
[2022-08-06 16:25:32,243] In train:p&d10.npy.
[2022-08-06 16:25:32,244] One Channel
[2022-08-06 16:25:32,245] With Normal data.
[2022-08-06 16:25:32,245] Nunber of classes:10.
[2022-08-06 16:25:32,246] Nunber of ViT channels:1.
[2022-08-06 16:25:32,488] Totol epochs: 15
[2022-08-06 16:25:32,490] Epoch 1---------------
[2022-08-06 16:25:32,491] lr: 2.000000e-03
[2022-08-06 16:25:32,997] loss: 2.343711  [    0/ 4741]
[2022-08-06 16:25:40,566] loss: 1.678348  [  960/ 4741]
[2022-08-06 16:25:48,137] loss: 1.878334  [ 1920/ 4741]
[2022-08-06 16:25:55,707] loss: 1.665815  [ 2880/ 4741]
[2022-08-06 16:26:03,277] loss: 1.277749  [ 3840/ 4741]
[2022-08-06 16:26:24,415] Train Error: Accuracy: 47.332%, Avg loss: 1.595533
[2022-08-06 16:26:30,663] Test  Error: Accuracy: 46.768%, Avg loss: 1.560809
[2022-08-06 16:26:30,664] Epoch 2---------------
[2022-08-06 16:26:30,665] lr: 1.900000e-03
[2022-08-06 16:26:31,171] loss: 1.674594  [    0/ 4741]
[2022-08-06 16:26:38,741] loss: 0.541846  [  960/ 4741]
[2022-08-06 16:26:46,311] loss: 0.703377  [ 1920/ 4741]
[2022-08-06 16:26:53,881] loss: 0.383079  [ 2880/ 4741]
[2022-08-06 16:27:01,452] loss: 0.349186  [ 3840/ 4741]
[2022-08-06 16:27:22,594] Train Error: Accuracy: 80.194%, Avg loss: 0.548307
[2022-08-06 16:27:28,841] Test  Error: Accuracy: 77.669%, Avg loss: 0.590191
[2022-08-06 16:27:28,842] Epoch 3---------------
[2022-08-06 16:27:28,843] lr: 1.805000e-03
[2022-08-06 16:27:29,350] loss: 0.602794  [    0/ 4741]
[2022-08-06 16:27:36,920] loss: 0.254826  [  960/ 4741]
[2022-08-06 16:27:44,490] loss: 0.147957  [ 1920/ 4741]
[2022-08-06 16:27:52,060] loss: 0.281456  [ 2880/ 4741]
[2022-08-06 16:27:59,634] loss: 0.263443  [ 3840/ 4741]
[2022-08-06 16:28:20,778] Train Error: Accuracy: 83.210%, Avg loss: 0.583709
[2022-08-06 16:28:27,040] Test  Error: Accuracy: 80.803%, Avg loss: 0.657956
[2022-08-06 16:28:27,040] Epoch 4---------------
[2022-08-06 16:28:27,041] lr: 1.396675e-03
[2022-08-06 16:28:27,548] loss: 0.663989  [    0/ 4741]
[2022-08-06 16:28:35,119] loss: 0.043822  [  960/ 4741]
[2022-08-06 16:28:42,688] loss: 0.042369  [ 1920/ 4741]
[2022-08-06 16:28:50,258] loss: 0.042300  [ 2880/ 4741]
[2022-08-06 16:28:57,827] loss: 0.118145  [ 3840/ 4741]
[2022-08-06 16:29:18,969] Train Error: Accuracy: 96.920%, Avg loss: 0.101284
[2022-08-06 16:29:25,217] Test  Error: Accuracy: 95.054%, Avg loss: 0.147720
[2022-08-06 16:29:25,218] Epoch 5---------------
[2022-08-06 16:29:25,219] lr: 1.326841e-03
[2022-08-06 16:29:25,726] loss: 0.090047  [    0/ 4741]
[2022-08-06 16:29:33,297] loss: 0.106072  [  960/ 4741]
[2022-08-06 16:29:40,868] loss: 0.105263  [ 1920/ 4741]
[2022-08-06 16:29:48,439] loss: 0.053319  [ 2880/ 4741]
[2022-08-06 16:29:56,012] loss: 0.127222  [ 3840/ 4741]
[2022-08-06 16:30:17,154] Train Error: Accuracy: 97.806%, Avg loss: 0.074271
[2022-08-06 16:30:23,402] Test  Error: Accuracy: 96.523%, Avg loss: 0.127787
[2022-08-06 16:30:23,403] Epoch 6---------------
[2022-08-06 16:30:23,404] lr: 1.260499e-03
[2022-08-06 16:30:23,911] loss: 0.058209  [    0/ 4741]
[2022-08-06 16:30:31,481] loss: 0.027126  [  960/ 4741]
[2022-08-06 16:30:39,051] loss: 0.089411  [ 1920/ 4741]
[2022-08-06 16:30:46,621] loss: 0.049265  [ 2880/ 4741]
[2022-08-06 16:30:54,191] loss: 0.074738  [ 3840/ 4741]
[2022-08-06 16:31:15,328] Train Error: Accuracy: 88.209%, Avg loss: 0.388450
[2022-08-06 16:31:21,578] Test  Error: Accuracy: 86.043%, Avg loss: 0.443830
[2022-08-06 16:31:21,578] Epoch 7---------------
[2022-08-06 16:31:21,579] lr: 8.802533e-04
[2022-08-06 16:31:22,087] loss: 0.214497  [    0/ 4741]
[2022-08-06 16:31:29,657] loss: 0.054560  [  960/ 4741]
[2022-08-06 16:31:37,227] loss: 0.046073  [ 1920/ 4741]
[2022-08-06 16:31:44,799] loss: 0.054380  [ 2880/ 4741]
[2022-08-06 16:31:52,370] loss: 0.064908  [ 3840/ 4741]
[2022-08-06 16:32:13,514] Train Error: Accuracy: 99.093%, Avg loss: 0.035592
[2022-08-06 16:32:19,765] Test  Error: Accuracy: 97.453%, Avg loss: 0.075597
[2022-08-06 16:32:19,765] Epoch 8---------------
[2022-08-06 16:32:19,767] lr: 8.362407e-04
[2022-08-06 16:32:20,274] loss: 0.075790  [    0/ 4741]
[2022-08-06 16:32:27,844] loss: 0.027763  [  960/ 4741]
[2022-08-06 16:32:35,414] loss: 0.020408  [ 1920/ 4741]
[2022-08-06 16:32:42,984] loss: 0.004809  [ 2880/ 4741]
[2022-08-06 16:32:50,554] loss: 0.031662  [ 3840/ 4741]
[2022-08-06 16:33:11,699] Train Error: Accuracy: 58.806%, Avg loss: 2.862824
[2022-08-06 16:33:17,950] Test  Error: Accuracy: 58.521%, Avg loss: 2.847692
[2022-08-06 16:33:17,950] Epoch 9---------------
[2022-08-06 16:33:17,951] lr: 5.839780e-04
[2022-08-06 16:33:18,458] loss: 3.722860  [    0/ 4741]
[2022-08-06 16:33:26,027] loss: 0.111279  [  960/ 4741]
[2022-08-06 16:33:33,597] loss: 0.078791  [ 1920/ 4741]
[2022-08-06 16:33:41,168] loss: 0.090175  [ 2880/ 4741]
[2022-08-06 16:33:48,739] loss: 0.028592  [ 3840/ 4741]
[2022-08-06 16:34:09,885] Train Error: Accuracy: 99.536%, Avg loss: 0.022819
[2022-08-06 16:34:16,137] Test  Error: Accuracy: 98.531%, Avg loss: 0.058307
[2022-08-06 16:34:16,137] Epoch 10---------------
[2022-08-06 16:34:16,138] lr: 5.547791e-04
[2022-08-06 16:34:16,645] loss: 0.009919  [    0/ 4741]
[2022-08-06 16:34:24,216] loss: 0.011444  [  960/ 4741]
[2022-08-06 16:34:31,787] loss: 0.003868  [ 1920/ 4741]
[2022-08-06 16:34:39,359] loss: 0.016590  [ 2880/ 4741]
[2022-08-06 16:34:46,930] loss: 0.005466  [ 3840/ 4741]
[2022-08-06 16:35:08,070] Train Error: Accuracy: 99.473%, Avg loss: 0.021111
[2022-08-06 16:35:14,319] Test  Error: Accuracy: 98.433%, Avg loss: 0.059470
[2022-08-06 16:35:14,319] Epoch 11---------------
[2022-08-06 16:35:14,320] lr: 4.756538e-04
[2022-08-06 16:35:14,828] loss: 0.008044  [    0/ 4741]
[2022-08-06 16:35:22,398] loss: 0.008524  [  960/ 4741]
[2022-08-06 16:35:29,969] loss: 0.006871  [ 1920/ 4741]
[2022-08-06 16:35:37,538] loss: 0.003174  [ 2880/ 4741]
[2022-08-06 16:35:45,107] loss: 0.009007  [ 3840/ 4741]
[2022-08-06 16:36:06,247] Train Error: Accuracy: 99.684%, Avg loss: 0.013956
[2022-08-06 16:36:12,496] Test  Error: Accuracy: 98.874%, Avg loss: 0.042454
[2022-08-06 16:36:12,496] Epoch 12---------------
[2022-08-06 16:36:12,498] lr: 4.518711e-04
[2022-08-06 16:36:13,006] loss: 0.002797  [    0/ 4741]
[2022-08-06 16:36:20,578] loss: 0.011460  [  960/ 4741]
[2022-08-06 16:36:28,150] loss: 0.013106  [ 1920/ 4741]
[2022-08-06 16:36:35,721] loss: 0.005131  [ 2880/ 4741]
[2022-08-06 16:36:43,292] loss: 0.004523  [ 3840/ 4741]
[2022-08-06 16:37:04,437] Train Error: Accuracy: 99.789%, Avg loss: 0.009358
[2022-08-06 16:37:10,690] Test  Error: Accuracy: 99.021%, Avg loss: 0.037308
[2022-08-06 16:37:10,690] Epoch 13---------------
[2022-08-06 16:37:10,691] lr: 4.292775e-04
[2022-08-06 16:37:11,199] loss: 0.005831  [    0/ 4741]
[2022-08-06 16:37:18,771] loss: 0.001890  [  960/ 4741]
[2022-08-06 16:37:26,342] loss: 0.005144  [ 1920/ 4741]
[2022-08-06 16:37:33,912] loss: 0.008903  [ 2880/ 4741]
[2022-08-06 16:37:41,482] loss: 0.019123  [ 3840/ 4741]
[2022-08-06 16:38:02,620] Train Error: Accuracy: 99.852%, Avg loss: 0.008472
[2022-08-06 16:38:08,867] Test  Error: Accuracy: 98.874%, Avg loss: 0.037080
[2022-08-06 16:38:08,868] Epoch 14---------------
[2022-08-06 16:38:08,869] lr: 4.078137e-04
[2022-08-06 16:38:09,376] loss: 0.001652  [    0/ 4741]
[2022-08-06 16:38:16,946] loss: 0.005920  [  960/ 4741]
[2022-08-06 16:38:24,516] loss: 0.023590  [ 1920/ 4741]
[2022-08-06 16:38:32,085] loss: 0.001689  [ 2880/ 4741]
[2022-08-06 16:38:39,655] loss: 0.017066  [ 3840/ 4741]
[2022-08-06 16:39:00,799] Train Error: Accuracy: 99.852%, Avg loss: 0.007546
[2022-08-06 16:39:07,046] Test  Error: Accuracy: 99.167%, Avg loss: 0.028791
[2022-08-06 16:39:07,047] Epoch 15---------------
[2022-08-06 16:39:07,048] lr: 3.874230e-04
[2022-08-06 16:39:07,555] loss: 0.005068  [    0/ 4741]
[2022-08-06 16:39:15,126] loss: 0.001303  [  960/ 4741]
[2022-08-06 16:39:22,698] loss: 0.003520  [ 1920/ 4741]
[2022-08-06 16:39:30,269] loss: 0.021890  [ 2880/ 4741]
[2022-08-06 16:39:37,840] loss: 0.003356  [ 3840/ 4741]
[2022-08-06 16:39:58,982] Train Error: Accuracy: 99.852%, Avg loss: 0.007895
[2022-08-06 16:40:05,232] Test  Error: Accuracy: 98.923%, Avg loss: 0.036428
[2022-08-06 16:40:05,232] Done!
[2022-08-06 16:40:05,237] Number of parameters:1232650
[2022-08-06 16:40:05,237] ## end time: 2022-08-06 16:40:05.232165
[2022-08-06 16:40:05,237] ## used time: 0:14:33.128386
