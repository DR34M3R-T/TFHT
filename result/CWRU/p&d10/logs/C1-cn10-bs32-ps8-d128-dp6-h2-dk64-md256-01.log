[2022-08-08 16:26:46,167] ## start time: 2022-08-08 16:26:46.041034
[2022-08-08 16:26:46,167] Using cuda device
[2022-08-08 16:26:46,169] In train:p&d10.npy.
[2022-08-08 16:26:46,170] One Channel
[2022-08-08 16:26:46,170] With Normal data.
[2022-08-08 16:26:46,171] Nunber of classes:10.
[2022-08-08 16:26:46,171] Nunber of ViT channels:1.
[2022-08-08 16:26:46,393] Totol epochs: 15
[2022-08-08 16:26:46,396] Epoch 1---------------
[2022-08-08 16:26:46,396] lr: 2.000000e-03
[2022-08-08 16:26:47,225] loss: 2.249855  [    0/ 4656]
[2022-08-08 16:26:59,650] loss: 1.507901  [  480/ 4656]
[2022-08-08 16:27:12,075] loss: 1.867916  [  960/ 4656]
[2022-08-08 16:27:24,499] loss: 1.821298  [ 1440/ 4656]
[2022-08-08 16:27:36,925] loss: 1.420849  [ 1920/ 4656]
[2022-08-08 16:27:49,347] loss: 1.346411  [ 2400/ 4656]
[2022-08-08 16:28:01,773] loss: 1.374886  [ 2880/ 4656]
[2022-08-08 16:28:14,196] loss: 1.296555  [ 3360/ 4656]
[2022-08-08 16:28:26,620] loss: 0.983497  [ 3840/ 4656]
[2022-08-08 16:28:39,045] loss: 0.829169  [ 4320/ 4656]
[2022-08-08 16:29:32,041] Train Error: Accuracy: 65.314%, Avg loss: 0.995936
[2022-08-08 16:29:52,657] Test  Error: Accuracy: 65.115%, Avg loss: 0.982090
[2022-08-08 16:29:52,657] Epoch 2---------------
[2022-08-08 16:29:52,659] lr: 1.900000e-03
[2022-08-08 16:29:53,488] loss: 0.854072  [    0/ 4656]
[2022-08-08 16:30:05,913] loss: 0.651358  [  480/ 4656]
[2022-08-08 16:30:18,337] loss: 0.413108  [  960/ 4656]
[2022-08-08 16:30:30,761] loss: 0.655253  [ 1440/ 4656]
[2022-08-08 16:30:43,187] loss: 0.660008  [ 1920/ 4656]
[2022-08-08 16:30:55,608] loss: 0.483675  [ 2400/ 4656]
[2022-08-08 16:31:08,032] loss: 0.402701  [ 2880/ 4656]
[2022-08-08 16:31:20,456] loss: 0.618410  [ 3360/ 4656]
[2022-08-08 16:31:32,878] loss: 0.547180  [ 3840/ 4656]
[2022-08-08 16:31:45,303] loss: 0.354916  [ 4320/ 4656]
[2022-08-08 16:32:38,302] Train Error: Accuracy: 79.961%, Avg loss: 0.609836
[2022-08-08 16:32:58,918] Test  Error: Accuracy: 78.984%, Avg loss: 0.628918
[2022-08-08 16:32:58,919] Epoch 3---------------
[2022-08-08 16:32:58,920] lr: 1.805000e-03
[2022-08-08 16:32:59,750] loss: 0.453345  [    0/ 4656]
[2022-08-08 16:33:12,174] loss: 0.272128  [  480/ 4656]
[2022-08-08 16:33:24,599] loss: 0.250958  [  960/ 4656]
[2022-08-08 16:33:37,022] loss: 0.214131  [ 1440/ 4656]
[2022-08-08 16:33:49,446] loss: 0.601772  [ 1920/ 4656]
[2022-08-08 16:34:01,871] loss: 0.255829  [ 2400/ 4656]
[2022-08-08 16:34:14,297] loss: 0.368566  [ 2880/ 4656]
[2022-08-08 16:34:26,721] loss: 0.263135  [ 3360/ 4656]
[2022-08-08 16:34:39,146] loss: 0.364773  [ 3840/ 4656]
[2022-08-08 16:34:51,570] loss: 0.362022  [ 4320/ 4656]
[2022-08-08 16:35:44,567] Train Error: Accuracy: 90.893%, Avg loss: 0.274857
[2022-08-08 16:36:05,181] Test  Error: Accuracy: 89.516%, Avg loss: 0.318057
[2022-08-08 16:36:05,182] Epoch 4---------------
[2022-08-08 16:36:05,183] lr: 1.714750e-03
[2022-08-08 16:36:06,013] loss: 0.459618  [    0/ 4656]
[2022-08-08 16:36:18,437] loss: 0.255569  [  480/ 4656]
[2022-08-08 16:36:30,860] loss: 0.591992  [  960/ 4656]
[2022-08-08 16:36:43,285] loss: 0.437335  [ 1440/ 4656]
[2022-08-08 16:36:55,710] loss: 0.074473  [ 1920/ 4656]
[2022-08-08 16:37:08,135] loss: 0.179332  [ 2400/ 4656]
[2022-08-08 16:37:20,559] loss: 0.704804  [ 2880/ 4656]
[2022-08-08 16:37:32,982] loss: 0.518731  [ 3360/ 4656]
[2022-08-08 16:37:45,405] loss: 0.454614  [ 3840/ 4656]
[2022-08-08 16:37:57,829] loss: 0.394520  [ 4320/ 4656]
[2022-08-08 16:38:50,817] Train Error: Accuracy: 85.825%, Avg loss: 0.410780
[2022-08-08 16:39:11,428] Test  Error: Accuracy: 85.002%, Avg loss: 0.435627
[2022-08-08 16:39:11,428] Epoch 5---------------
[2022-08-08 16:39:11,429] lr: 1.197474e-03
[2022-08-08 16:39:12,260] loss: 0.530557  [    0/ 4656]
[2022-08-08 16:39:24,683] loss: 0.311979  [  480/ 4656]
[2022-08-08 16:39:37,105] loss: 0.249112  [  960/ 4656]
[2022-08-08 16:39:49,528] loss: 0.348053  [ 1440/ 4656]
[2022-08-08 16:40:01,952] loss: 0.133043  [ 1920/ 4656]
[2022-08-08 16:40:14,375] loss: 0.057615  [ 2400/ 4656]
[2022-08-08 16:40:26,800] loss: 0.220720  [ 2880/ 4656]
[2022-08-08 16:40:39,224] loss: 0.316617  [ 3360/ 4656]
[2022-08-08 16:40:51,650] loss: 0.213718  [ 3840/ 4656]
[2022-08-08 16:41:04,076] loss: 0.169496  [ 4320/ 4656]
[2022-08-08 16:41:57,061] Train Error: Accuracy: 93.578%, Avg loss: 0.204116
[2022-08-08 16:42:17,671] Test  Error: Accuracy: 91.866%, Avg loss: 0.254732
[2022-08-08 16:42:17,672] Epoch 6---------------
[2022-08-08 16:42:17,673] lr: 1.137600e-03
[2022-08-08 16:42:18,503] loss: 0.107511  [    0/ 4656]
[2022-08-08 16:42:30,927] loss: 0.171794  [  480/ 4656]
[2022-08-08 16:42:43,350] loss: 0.098478  [  960/ 4656]
[2022-08-08 16:42:55,773] loss: 0.081926  [ 1440/ 4656]
[2022-08-08 16:43:08,198] loss: 0.052669  [ 1920/ 4656]
[2022-08-08 16:43:20,622] loss: 0.098149  [ 2400/ 4656]
[2022-08-08 16:43:33,047] loss: 0.229422  [ 2880/ 4656]
[2022-08-08 16:43:45,471] loss: 0.105244  [ 3360/ 4656]
[2022-08-08 16:43:57,894] loss: 0.170367  [ 3840/ 4656]
[2022-08-08 16:44:10,319] loss: 0.096526  [ 4320/ 4656]
[2022-08-08 16:45:03,307] Train Error: Accuracy: 91.860%, Avg loss: 0.234761
[2022-08-08 16:45:23,916] Test  Error: Accuracy: 90.127%, Avg loss: 0.282866
[2022-08-08 16:45:23,916] Epoch 7---------------
[2022-08-08 16:45:23,917] lr: 9.753500e-04
[2022-08-08 16:45:24,748] loss: 0.179949  [    0/ 4656]
[2022-08-08 16:45:37,171] loss: 0.130270  [  480/ 4656]
[2022-08-08 16:45:49,598] loss: 0.096427  [  960/ 4656]
[2022-08-08 16:46:02,022] loss: 0.163269  [ 1440/ 4656]
[2022-08-08 16:46:14,444] loss: 0.283590  [ 1920/ 4656]
[2022-08-08 16:46:26,868] loss: 0.092621  [ 2400/ 4656]
[2022-08-08 16:46:39,294] loss: 0.076744  [ 2880/ 4656]
[2022-08-08 16:46:51,718] loss: 0.088386  [ 3360/ 4656]
[2022-08-08 16:47:04,141] loss: 0.086270  [ 3840/ 4656]
[2022-08-08 16:47:16,565] loss: 0.040587  [ 4320/ 4656]
[2022-08-08 16:48:09,552] Train Error: Accuracy: 93.986%, Avg loss: 0.165380
[2022-08-08 16:48:30,161] Test  Error: Accuracy: 93.089%, Avg loss: 0.199089
[2022-08-08 16:48:30,161] Epoch 8---------------
[2022-08-08 16:48:30,162] lr: 9.265825e-04
[2022-08-08 16:48:30,992] loss: 0.138148  [    0/ 4656]
[2022-08-08 16:48:43,418] loss: 0.027125  [  480/ 4656]
[2022-08-08 16:48:55,841] loss: 0.361383  [  960/ 4656]
[2022-08-08 16:49:08,266] loss: 0.211849  [ 1440/ 4656]
[2022-08-08 16:49:20,688] loss: 0.281472  [ 1920/ 4656]
[2022-08-08 16:49:33,113] loss: 0.086823  [ 2400/ 4656]
[2022-08-08 16:49:45,538] loss: 0.157494  [ 2880/ 4656]
[2022-08-08 16:49:57,962] loss: 0.043084  [ 3360/ 4656]
[2022-08-08 16:50:10,387] loss: 0.198863  [ 3840/ 4656]
[2022-08-08 16:50:22,812] loss: 0.366015  [ 4320/ 4656]
[2022-08-08 16:51:15,802] Train Error: Accuracy: 97.466%, Avg loss: 0.078873
[2022-08-08 16:51:36,412] Test  Error: Accuracy: 96.615%, Avg loss: 0.104688
[2022-08-08 16:51:36,413] Epoch 9---------------
[2022-08-08 16:51:36,414] lr: 8.802533e-04
[2022-08-08 16:51:37,245] loss: 0.093275  [    0/ 4656]
[2022-08-08 16:51:49,670] loss: 0.063897  [  480/ 4656]
[2022-08-08 16:52:02,094] loss: 0.018970  [  960/ 4656]
[2022-08-08 16:52:14,519] loss: 0.271115  [ 1440/ 4656]
[2022-08-08 16:52:26,944] loss: 0.070881  [ 1920/ 4656]
[2022-08-08 16:52:39,369] loss: 0.189254  [ 2400/ 4656]
[2022-08-08 16:52:51,794] loss: 0.043499  [ 2880/ 4656]
[2022-08-08 16:53:04,217] loss: 0.023723  [ 3360/ 4656]
[2022-08-08 16:53:16,640] loss: 0.077764  [ 3840/ 4656]
[2022-08-08 16:53:29,065] loss: 0.047583  [ 4320/ 4656]
[2022-08-08 16:54:22,040] Train Error: Accuracy: 97.680%, Avg loss: 0.074552
[2022-08-08 16:54:42,648] Test  Error: Accuracy: 96.568%, Avg loss: 0.115410
[2022-08-08 16:54:42,648] Epoch 10---------------
[2022-08-08 16:54:42,649] lr: 7.547072e-04
[2022-08-08 16:54:43,480] loss: 0.014902  [    0/ 4656]
[2022-08-08 16:54:55,904] loss: 0.190493  [  480/ 4656]
[2022-08-08 16:55:08,328] loss: 0.063209  [  960/ 4656]
[2022-08-08 16:55:20,752] loss: 0.051558  [ 1440/ 4656]
[2022-08-08 16:55:33,175] loss: 0.026149  [ 1920/ 4656]
[2022-08-08 16:55:45,601] loss: 0.109046  [ 2400/ 4656]
[2022-08-08 16:55:58,026] loss: 0.010806  [ 2880/ 4656]
[2022-08-08 16:56:10,452] loss: 0.012020  [ 3360/ 4656]
[2022-08-08 16:56:22,878] loss: 0.098799  [ 3840/ 4656]
[2022-08-08 16:56:35,302] loss: 0.238037  [ 4320/ 4656]
[2022-08-08 16:57:28,287] Train Error: Accuracy: 98.282%, Avg loss: 0.058624
[2022-08-08 16:57:48,896] Test  Error: Accuracy: 96.756%, Avg loss: 0.104395
[2022-08-08 16:57:48,896] Epoch 11---------------
[2022-08-08 16:57:48,897] lr: 7.169718e-04
[2022-08-08 16:57:49,726] loss: 0.065702  [    0/ 4656]
[2022-08-08 16:58:02,150] loss: 0.029668  [  480/ 4656]
[2022-08-08 16:58:14,574] loss: 0.028606  [  960/ 4656]
[2022-08-08 16:58:27,000] loss: 0.049401  [ 1440/ 4656]
[2022-08-08 16:58:39,424] loss: 0.263722  [ 1920/ 4656]
[2022-08-08 16:58:51,845] loss: 0.092323  [ 2400/ 4656]
[2022-08-08 16:59:04,269] loss: 0.027319  [ 2880/ 4656]
[2022-08-08 16:59:16,695] loss: 0.039989  [ 3360/ 4656]
[2022-08-08 16:59:29,119] loss: 0.063029  [ 3840/ 4656]
[2022-08-08 16:59:41,544] loss: 0.011766  [ 4320/ 4656]
[2022-08-08 17:00:34,521] Train Error: Accuracy: 98.497%, Avg loss: 0.041849
[2022-08-08 17:00:55,127] Test  Error: Accuracy: 97.132%, Avg loss: 0.086397
[2022-08-08 17:00:55,127] Epoch 12---------------
[2022-08-08 17:00:55,128] lr: 6.811233e-04
[2022-08-08 17:00:55,958] loss: 0.015485  [    0/ 4656]
[2022-08-08 17:01:08,381] loss: 0.002016  [  480/ 4656]
[2022-08-08 17:01:20,808] loss: 0.009189  [  960/ 4656]
[2022-08-08 17:01:33,234] loss: 0.034900  [ 1440/ 4656]
[2022-08-08 17:01:45,659] loss: 0.009735  [ 1920/ 4656]
[2022-08-08 17:01:58,082] loss: 0.090748  [ 2400/ 4656]
[2022-08-08 17:02:10,506] loss: 0.029321  [ 2880/ 4656]
[2022-08-08 17:02:22,931] loss: 0.013347  [ 3360/ 4656]
[2022-08-08 17:02:35,355] loss: 0.008863  [ 3840/ 4656]
[2022-08-08 17:02:47,781] loss: 0.019283  [ 4320/ 4656]
[2022-08-08 17:03:40,768] Train Error: Accuracy: 99.055%, Avg loss: 0.034400
[2022-08-08 17:04:01,377] Test  Error: Accuracy: 97.790%, Avg loss: 0.070351
[2022-08-08 17:04:01,377] Epoch 13---------------
[2022-08-08 17:04:01,378] lr: 6.470671e-04
[2022-08-08 17:04:02,209] loss: 0.020394  [    0/ 4656]
[2022-08-08 17:04:14,634] loss: 0.016325  [  480/ 4656]
[2022-08-08 17:04:27,057] loss: 0.070702  [  960/ 4656]
[2022-08-08 17:04:39,480] loss: 0.014787  [ 1440/ 4656]
[2022-08-08 17:04:51,906] loss: 0.008804  [ 1920/ 4656]
[2022-08-08 17:05:04,329] loss: 0.009365  [ 2400/ 4656]
[2022-08-08 17:05:16,755] loss: 0.029800  [ 2880/ 4656]
[2022-08-08 17:05:29,179] loss: 0.179410  [ 3360/ 4656]
[2022-08-08 17:05:41,602] loss: 0.006159  [ 3840/ 4656]
[2022-08-08 17:05:54,028] loss: 0.006867  [ 4320/ 4656]
[2022-08-08 17:06:47,012] Train Error: Accuracy: 98.819%, Avg loss: 0.032132
[2022-08-08 17:07:07,622] Test  Error: Accuracy: 97.743%, Avg loss: 0.077349
[2022-08-08 17:07:07,623] Epoch 14---------------
[2022-08-08 17:07:07,624] lr: 5.547791e-04
[2022-08-08 17:07:08,454] loss: 0.013427  [    0/ 4656]
[2022-08-08 17:07:20,879] loss: 0.093971  [  480/ 4656]
[2022-08-08 17:07:33,306] loss: 0.014150  [  960/ 4656]
[2022-08-08 17:07:45,731] loss: 0.033727  [ 1440/ 4656]
[2022-08-08 17:07:58,154] loss: 0.037149  [ 1920/ 4656]
[2022-08-08 17:08:10,579] loss: 0.068040  [ 2400/ 4656]
[2022-08-08 17:08:23,003] loss: 0.237033  [ 2880/ 4656]
[2022-08-08 17:08:35,426] loss: 0.060803  [ 3360/ 4656]
[2022-08-08 17:08:47,850] loss: 0.084762  [ 3840/ 4656]
[2022-08-08 17:09:00,274] loss: 0.062150  [ 4320/ 4656]
[2022-08-08 17:09:53,258] Train Error: Accuracy: 98.325%, Avg loss: 0.053348
[2022-08-08 17:10:13,870] Test  Error: Accuracy: 96.991%, Avg loss: 0.097088
[2022-08-08 17:10:13,870] Epoch 15---------------
[2022-08-08 17:10:13,871] lr: 3.874230e-04
[2022-08-08 17:10:14,702] loss: 0.124781  [    0/ 4656]
[2022-08-08 17:10:27,126] loss: 0.009586  [  480/ 4656]
[2022-08-08 17:10:39,551] loss: 0.032305  [  960/ 4656]
[2022-08-08 17:10:51,976] loss: 0.067717  [ 1440/ 4656]
[2022-08-08 17:11:04,400] loss: 0.021177  [ 1920/ 4656]
[2022-08-08 17:11:16,823] loss: 0.029400  [ 2400/ 4656]
[2022-08-08 17:11:29,249] loss: 0.003995  [ 2880/ 4656]
[2022-08-08 17:11:41,673] loss: 0.014871  [ 3360/ 4656]
[2022-08-08 17:11:54,098] loss: 0.141699  [ 3840/ 4656]
[2022-08-08 17:12:06,522] loss: 0.010936  [ 4320/ 4656]
[2022-08-08 17:12:59,497] Train Error: Accuracy: 99.485%, Avg loss: 0.021863
[2022-08-08 17:13:20,109] Test  Error: Accuracy: 98.213%, Avg loss: 0.060230
[2022-08-08 17:13:20,109] Done!
[2022-08-08 17:13:20,113] Number of parameters:1656586
[2022-08-08 17:13:20,114] ## end time: 2022-08-08 17:13:20.109624
[2022-08-08 17:13:20,114] ## used time: 0:46:34.068590
