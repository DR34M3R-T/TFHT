[2022-08-06 15:29:23,265] ## start time: 2022-08-06 15:29:23.141638
[2022-08-06 15:29:23,266] Using cuda device
[2022-08-06 15:29:23,266] In train:p&d10.npy.
[2022-08-06 15:29:23,266] One Channel
[2022-08-06 15:29:23,266] With Normal data.
[2022-08-06 15:29:23,267] Nunber of classes:10.
[2022-08-06 15:29:23,267] Nunber of ViT channels:1.
[2022-08-06 15:29:24,279] Totol epochs: 10
[2022-08-06 15:29:24,280] Epoch 1---------------
[2022-08-06 15:29:24,280] lr: 2.000000e-03
[2022-08-06 15:29:25,767] loss: 2.421052  [    0/ 4689]
[2022-08-06 15:29:25,919] loss: 2.125293  [  960/ 4689]
[2022-08-06 15:29:26,070] loss: 1.750692  [ 1920/ 4689]
[2022-08-06 15:29:26,225] loss: 1.280933  [ 2880/ 4689]
[2022-08-06 15:29:26,375] loss: 1.031130  [ 3840/ 4689]
[2022-08-06 15:29:26,773] Train Error: Accuracy: 82.299%, Avg loss: 0.750676
[2022-08-06 15:29:26,894] Test  Error: Accuracy: 81.519%, Avg loss: 0.767854
[2022-08-06 15:29:26,895] Epoch 2---------------
[2022-08-06 15:29:26,895] lr: 1.900000e-03
[2022-08-06 15:29:26,905] loss: 0.667368  [    0/ 4689]
[2022-08-06 15:29:27,056] loss: 0.581211  [  960/ 4689]
[2022-08-06 15:29:27,209] loss: 0.466461  [ 1920/ 4689]
[2022-08-06 15:29:27,361] loss: 0.361759  [ 2880/ 4689]
[2022-08-06 15:29:27,512] loss: 0.262136  [ 3840/ 4689]
[2022-08-06 15:29:27,910] Train Error: Accuracy: 93.602%, Avg loss: 0.274149
[2022-08-06 15:29:28,031] Test  Error: Accuracy: 92.455%, Avg loss: 0.302012
[2022-08-06 15:29:28,031] Epoch 3---------------
[2022-08-06 15:29:28,032] lr: 1.805000e-03
[2022-08-06 15:29:28,044] loss: 0.257796  [    0/ 4689]
[2022-08-06 15:29:28,196] loss: 0.304934  [  960/ 4689]
[2022-08-06 15:29:28,347] loss: 0.203659  [ 1920/ 4689]
[2022-08-06 15:29:28,497] loss: 0.164910  [ 2880/ 4689]
[2022-08-06 15:29:28,648] loss: 0.129623  [ 3840/ 4689]
[2022-08-06 15:29:29,050] Train Error: Accuracy: 96.502%, Avg loss: 0.161598
[2022-08-06 15:29:29,170] Test  Error: Accuracy: 95.893%, Avg loss: 0.177424
[2022-08-06 15:29:29,170] Epoch 4---------------
[2022-08-06 15:29:29,171] lr: 1.714750e-03
[2022-08-06 15:29:29,184] loss: 0.203217  [    0/ 4689]
[2022-08-06 15:29:29,335] loss: 0.151589  [  960/ 4689]
[2022-08-06 15:29:29,489] loss: 0.095707  [ 1920/ 4689]
[2022-08-06 15:29:29,640] loss: 0.094889  [ 2880/ 4689]
[2022-08-06 15:29:29,791] loss: 0.033607  [ 3840/ 4689]
[2022-08-06 15:29:30,190] Train Error: Accuracy: 97.569%, Avg loss: 0.108821
[2022-08-06 15:29:30,309] Test  Error: Accuracy: 97.135%, Avg loss: 0.121391
[2022-08-06 15:29:30,310] Epoch 5---------------
[2022-08-06 15:29:30,311] lr: 1.629012e-03
[2022-08-06 15:29:30,323] loss: 0.081201  [    0/ 4689]
[2022-08-06 15:29:30,476] loss: 0.047297  [  960/ 4689]
[2022-08-06 15:29:30,629] loss: 0.107976  [ 1920/ 4689]
[2022-08-06 15:29:30,782] loss: 0.068485  [ 2880/ 4689]
[2022-08-06 15:29:30,932] loss: 0.124800  [ 3840/ 4689]
[2022-08-06 15:29:31,334] Train Error: Accuracy: 98.187%, Avg loss: 0.081753
[2022-08-06 15:29:31,453] Test  Error: Accuracy: 97.564%, Avg loss: 0.097651
[2022-08-06 15:29:31,454] Epoch 6---------------
[2022-08-06 15:29:31,455] lr: 1.547562e-03
[2022-08-06 15:29:31,466] loss: 0.125190  [    0/ 4689]
[2022-08-06 15:29:31,621] loss: 0.059970  [  960/ 4689]
[2022-08-06 15:29:31,772] loss: 0.043611  [ 1920/ 4689]
[2022-08-06 15:29:31,923] loss: 0.038013  [ 2880/ 4689]
[2022-08-06 15:29:32,073] loss: 0.030660  [ 3840/ 4689]
[2022-08-06 15:29:32,471] Train Error: Accuracy: 99.552%, Avg loss: 0.036267
[2022-08-06 15:29:32,592] Test  Error: Accuracy: 98.902%, Avg loss: 0.051467
[2022-08-06 15:29:32,593] Epoch 7---------------
[2022-08-06 15:29:32,595] lr: 1.470184e-03
[2022-08-06 15:29:32,607] loss: 0.019831  [    0/ 4689]
[2022-08-06 15:29:32,759] loss: 0.030699  [  960/ 4689]
[2022-08-06 15:29:32,909] loss: 0.022383  [ 1920/ 4689]
[2022-08-06 15:29:33,062] loss: 0.022310  [ 2880/ 4689]
[2022-08-06 15:29:33,211] loss: 0.019404  [ 3840/ 4689]
[2022-08-06 15:29:33,603] Train Error: Accuracy: 99.488%, Avg loss: 0.032166
[2022-08-06 15:29:33,730] Test  Error: Accuracy: 98.902%, Avg loss: 0.045171
[2022-08-06 15:29:33,731] Epoch 8---------------
[2022-08-06 15:29:33,733] lr: 1.396675e-03
[2022-08-06 15:29:33,744] loss: 0.021193  [    0/ 4689]
[2022-08-06 15:29:33,894] loss: 0.023360  [  960/ 4689]
[2022-08-06 15:29:34,047] loss: 0.035122  [ 1920/ 4689]
[2022-08-06 15:29:34,204] loss: 0.011899  [ 2880/ 4689]
[2022-08-06 15:29:34,357] loss: 0.019732  [ 3840/ 4689]
[2022-08-06 15:29:34,758] Train Error: Accuracy: 99.296%, Avg loss: 0.030920
[2022-08-06 15:29:34,875] Test  Error: Accuracy: 99.140%, Avg loss: 0.036302
[2022-08-06 15:29:34,875] Epoch 9---------------
[2022-08-06 15:29:34,876] lr: 1.326841e-03
[2022-08-06 15:29:34,889] loss: 0.068917  [    0/ 4689]
[2022-08-06 15:29:35,045] loss: 0.031031  [  960/ 4689]
[2022-08-06 15:29:35,199] loss: 0.021038  [ 1920/ 4689]
[2022-08-06 15:29:35,352] loss: 0.014735  [ 2880/ 4689]
[2022-08-06 15:29:35,504] loss: 0.012810  [ 3840/ 4689]
[2022-08-06 15:29:35,905] Train Error: Accuracy: 83.387%, Avg loss: 0.503129
[2022-08-06 15:29:36,021] Test  Error: Accuracy: 81.901%, Avg loss: 0.567380
[2022-08-06 15:29:36,022] Epoch 10---------------
[2022-08-06 15:29:36,023] lr: 9.265825e-04
[2022-08-06 15:29:36,034] loss: 0.414093  [    0/ 4689]
[2022-08-06 15:29:36,187] loss: 0.023728  [  960/ 4689]
[2022-08-06 15:29:36,337] loss: 0.016031  [ 1920/ 4689]
[2022-08-06 15:29:36,486] loss: 0.013362  [ 2880/ 4689]
[2022-08-06 15:29:36,638] loss: 0.012397  [ 3840/ 4689]
[2022-08-06 15:29:37,038] Train Error: Accuracy: 99.531%, Avg loss: 0.022390
[2022-08-06 15:29:37,155] Test  Error: Accuracy: 99.093%, Avg loss: 0.033138
[2022-08-06 15:29:37,155] Done!
[2022-08-06 15:29:37,158] Number of parameters:27626
[2022-08-06 15:29:37,158] ## end time: 2022-08-06 15:29:37.155031
[2022-08-06 15:29:37,158] ## used time: 0:00:14.013393
