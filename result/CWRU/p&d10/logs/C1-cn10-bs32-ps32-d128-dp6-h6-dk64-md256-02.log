[2022-08-08 00:49:01,144] ## start time: 2022-08-08 00:49:01.018034
[2022-08-08 00:49:01,145] Using cuda device
[2022-08-08 00:49:01,146] In train:p&d10.npy.
[2022-08-08 00:49:01,147] One Channel
[2022-08-08 00:49:01,147] With Normal data.
[2022-08-08 00:49:01,148] Nunber of classes:10.
[2022-08-08 00:49:01,148] Nunber of ViT channels:1.
[2022-08-08 00:49:01,393] Totol epochs: 15
[2022-08-08 00:49:01,396] Epoch 1---------------
[2022-08-08 00:49:01,396] lr: 2.000000e-03
[2022-08-08 00:49:01,753] loss: 2.434197  [    0/ 4818]
[2022-08-08 00:49:07,094] loss: 1.756254  [  480/ 4818]
[2022-08-08 00:49:12,436] loss: 1.444124  [  960/ 4818]
[2022-08-08 00:49:17,777] loss: 1.213794  [ 1440/ 4818]
[2022-08-08 00:49:23,121] loss: 0.908988  [ 1920/ 4818]
[2022-08-08 00:49:28,465] loss: 0.228333  [ 2400/ 4818]
[2022-08-08 00:49:33,808] loss: 0.674545  [ 2880/ 4818]
[2022-08-08 00:49:39,150] loss: 0.155968  [ 3360/ 4818]
[2022-08-08 00:49:44,491] loss: 0.162727  [ 3840/ 4818]
[2022-08-08 00:49:49,832] loss: 0.161827  [ 4320/ 4818]
[2022-08-08 00:49:55,033] loss: 0.060189  [ 4800/ 4818]
[2022-08-08 00:50:13,816] Train Error: Accuracy: 94.977%, Avg loss: 0.170451
[2022-08-08 00:50:21,480] Test  Error: Accuracy: 93.333%, Avg loss: 0.197463
[2022-08-08 00:50:21,481] Epoch 2---------------
[2022-08-08 00:50:21,484] lr: 1.900000e-03
[2022-08-08 00:50:21,841] loss: 0.026918  [    0/ 4818]
[2022-08-08 00:50:27,183] loss: 0.014260  [  480/ 4818]
[2022-08-08 00:50:32,525] loss: 0.019867  [  960/ 4818]
[2022-08-08 00:50:37,869] loss: 0.011485  [ 1440/ 4818]
[2022-08-08 00:50:43,211] loss: 0.010657  [ 1920/ 4818]
[2022-08-08 00:50:48,552] loss: 0.367320  [ 2400/ 4818]
[2022-08-08 00:50:53,893] loss: 0.037070  [ 2880/ 4818]
[2022-08-08 00:50:59,238] loss: 0.345851  [ 3360/ 4818]
[2022-08-08 00:51:04,580] loss: 0.238419  [ 3840/ 4818]
[2022-08-08 00:51:09,922] loss: 0.015885  [ 4320/ 4818]
[2022-08-08 00:51:15,122] loss: 0.176574  [ 4800/ 4818]
[2022-08-08 00:51:33,906] Train Error: Accuracy: 95.787%, Avg loss: 0.137552
[2022-08-08 00:51:41,580] Test  Error: Accuracy: 95.318%, Avg loss: 0.157907
[2022-08-08 00:51:41,581] Epoch 3---------------
[2022-08-08 00:51:41,582] lr: 1.805000e-03
[2022-08-08 00:51:41,940] loss: 0.073535  [    0/ 4818]
[2022-08-08 00:51:47,283] loss: 0.596006  [  480/ 4818]
[2022-08-08 00:51:52,627] loss: 0.055891  [  960/ 4818]
[2022-08-08 00:51:57,970] loss: 0.177514  [ 1440/ 4818]
[2022-08-08 00:52:03,311] loss: 0.121291  [ 1920/ 4818]
[2022-08-08 00:52:08,654] loss: 0.120906  [ 2400/ 4818]
[2022-08-08 00:52:13,997] loss: 0.054397  [ 2880/ 4818]
[2022-08-08 00:52:19,338] loss: 0.013596  [ 3360/ 4818]
[2022-08-08 00:52:24,681] loss: 0.020202  [ 3840/ 4818]
[2022-08-08 00:52:30,022] loss: 0.031869  [ 4320/ 4818]
[2022-08-08 00:52:35,220] loss: 0.005424  [ 4800/ 4818]
[2022-08-08 00:52:53,984] Train Error: Accuracy: 99.253%, Avg loss: 0.034833
[2022-08-08 00:53:01,638] Test  Error: Accuracy: 99.084%, Avg loss: 0.031981
[2022-08-08 00:53:01,638] Epoch 4---------------
[2022-08-08 00:53:01,640] lr: 1.714750e-03
[2022-08-08 00:53:01,998] loss: 0.027561  [    0/ 4818]
[2022-08-08 00:53:07,342] loss: 0.047517  [  480/ 4818]
[2022-08-08 00:53:12,684] loss: 0.002528  [  960/ 4818]
[2022-08-08 00:53:18,025] loss: 0.015209  [ 1440/ 4818]
[2022-08-08 00:53:23,364] loss: 0.082812  [ 1920/ 4818]
[2022-08-08 00:53:28,709] loss: 0.014713  [ 2400/ 4818]
[2022-08-08 00:53:34,049] loss: 0.013371  [ 2880/ 4818]
[2022-08-08 00:53:39,392] loss: 0.003082  [ 3360/ 4818]
[2022-08-08 00:53:44,736] loss: 0.016194  [ 3840/ 4818]
[2022-08-08 00:53:50,079] loss: 0.022160  [ 4320/ 4818]
[2022-08-08 00:53:55,279] loss: 0.117217  [ 4800/ 4818]
[2022-08-08 00:54:14,039] Train Error: Accuracy: 98.111%, Avg loss: 0.065670
[2022-08-08 00:54:21,698] Test  Error: Accuracy: 97.913%, Avg loss: 0.072568
[2022-08-08 00:54:21,699] Epoch 5---------------
[2022-08-08 00:54:21,700] lr: 1.197474e-03
[2022-08-08 00:54:22,057] loss: 0.029916  [    0/ 4818]
[2022-08-08 00:54:27,398] loss: 0.241488  [  480/ 4818]
[2022-08-08 00:54:32,741] loss: 0.025983  [  960/ 4818]
[2022-08-08 00:54:38,082] loss: 0.007625  [ 1440/ 4818]
[2022-08-08 00:54:43,426] loss: 0.031776  [ 1920/ 4818]
[2022-08-08 00:54:48,768] loss: 0.078096  [ 2400/ 4818]
[2022-08-08 00:54:54,109] loss: 0.003014  [ 2880/ 4818]
[2022-08-08 00:54:59,447] loss: 0.002896  [ 3360/ 4818]
[2022-08-08 00:55:04,791] loss: 0.002488  [ 3840/ 4818]
[2022-08-08 00:55:10,132] loss: 0.029488  [ 4320/ 4818]
[2022-08-08 00:55:15,330] loss: 0.058295  [ 4800/ 4818]
[2022-08-08 00:55:34,095] Train Error: Accuracy: 98.589%, Avg loss: 0.041235
[2022-08-08 00:55:41,750] Test  Error: Accuracy: 98.219%, Avg loss: 0.059799
[2022-08-08 00:55:41,751] Epoch 6---------------
[2022-08-08 00:55:41,751] lr: 1.137600e-03
[2022-08-08 00:55:42,110] loss: 0.011156  [    0/ 4818]
[2022-08-08 00:55:47,453] loss: 0.005962  [  480/ 4818]
[2022-08-08 00:55:52,796] loss: 0.003940  [  960/ 4818]
[2022-08-08 00:55:58,137] loss: 0.003646  [ 1440/ 4818]
[2022-08-08 00:56:03,480] loss: 0.030522  [ 1920/ 4818]
[2022-08-08 00:56:08,822] loss: 0.001809  [ 2400/ 4818]
[2022-08-08 00:56:14,164] loss: 0.004702  [ 2880/ 4818]
[2022-08-08 00:56:19,505] loss: 0.002128  [ 3360/ 4818]
[2022-08-08 00:56:24,848] loss: 0.001397  [ 3840/ 4818]
[2022-08-08 00:56:30,190] loss: 0.002309  [ 4320/ 4818]
[2022-08-08 00:56:35,387] loss: 0.001133  [ 4800/ 4818]
[2022-08-08 00:56:54,153] Train Error: Accuracy: 99.979%, Avg loss: 0.002363
[2022-08-08 00:57:01,808] Test  Error: Accuracy: 99.898%, Avg loss: 0.004955
[2022-08-08 00:57:01,808] Epoch 7---------------
[2022-08-08 00:57:01,809] lr: 1.080720e-03
[2022-08-08 00:57:02,166] loss: 0.002954  [    0/ 4818]
[2022-08-08 00:57:07,511] loss: 0.000452  [  480/ 4818]
[2022-08-08 00:57:12,855] loss: 0.001061  [  960/ 4818]
[2022-08-08 00:57:18,197] loss: 0.000774  [ 1440/ 4818]
[2022-08-08 00:57:23,539] loss: 0.001087  [ 1920/ 4818]
[2022-08-08 00:57:28,882] loss: 0.000515  [ 2400/ 4818]
[2022-08-08 00:57:34,223] loss: 0.001788  [ 2880/ 4818]
[2022-08-08 00:57:39,567] loss: 0.000707  [ 3360/ 4818]
[2022-08-08 00:57:44,912] loss: 0.001061  [ 3840/ 4818]
[2022-08-08 00:57:50,253] loss: 0.000360  [ 4320/ 4818]
[2022-08-08 00:57:55,451] loss: 0.000593  [ 4800/ 4818]
[2022-08-08 00:58:14,213] Train Error: Accuracy: 100.000%, Avg loss: 0.001100
[2022-08-08 00:58:21,871] Test  Error: Accuracy: 99.898%, Avg loss: 0.003038
[2022-08-08 00:58:21,871] Epoch 8---------------
[2022-08-08 00:58:21,872] lr: 1.026684e-03
[2022-08-08 00:58:22,230] loss: 0.000774  [    0/ 4818]
[2022-08-08 00:58:27,573] loss: 0.000390  [  480/ 4818]
[2022-08-08 00:58:32,915] loss: 0.000978  [  960/ 4818]
[2022-08-08 00:58:38,259] loss: 0.000730  [ 1440/ 4818]
[2022-08-08 00:58:43,601] loss: 0.001316  [ 1920/ 4818]
[2022-08-08 00:58:48,945] loss: 0.000456  [ 2400/ 4818]
[2022-08-08 00:58:54,286] loss: 0.001203  [ 2880/ 4818]
[2022-08-08 00:58:59,629] loss: 0.001250  [ 3360/ 4818]
[2022-08-08 00:59:04,970] loss: 0.002427  [ 3840/ 4818]
[2022-08-08 00:59:10,308] loss: 0.000910  [ 4320/ 4818]
[2022-08-08 00:59:15,507] loss: 0.000403  [ 4800/ 4818]
[2022-08-08 00:59:34,276] Train Error: Accuracy: 99.958%, Avg loss: 0.001771
[2022-08-08 00:59:41,933] Test  Error: Accuracy: 99.898%, Avg loss: 0.002617
[2022-08-08 00:59:41,933] Epoch 9---------------
[2022-08-08 00:59:41,934] lr: 9.753500e-04
[2022-08-08 00:59:42,292] loss: 0.000524  [    0/ 4818]
[2022-08-08 00:59:47,634] loss: 0.000369  [  480/ 4818]
[2022-08-08 00:59:52,975] loss: 0.000571  [  960/ 4818]
[2022-08-08 00:59:58,317] loss: 0.000458  [ 1440/ 4818]
[2022-08-08 01:00:03,659] loss: 0.000256  [ 1920/ 4818]
[2022-08-08 01:00:09,001] loss: 0.000251  [ 2400/ 4818]
[2022-08-08 01:00:14,341] loss: 0.000847  [ 2880/ 4818]
[2022-08-08 01:00:19,685] loss: 0.006326  [ 3360/ 4818]
[2022-08-08 01:00:25,028] loss: 0.000544  [ 3840/ 4818]
[2022-08-08 01:00:30,370] loss: 0.000601  [ 4320/ 4818]
[2022-08-08 01:00:35,570] loss: 0.001873  [ 4800/ 4818]
[2022-08-08 01:00:54,329] Train Error: Accuracy: 99.958%, Avg loss: 0.001437
[2022-08-08 01:01:01,986] Test  Error: Accuracy: 100.000%, Avg loss: 0.001962
[2022-08-08 01:01:01,986] Epoch 10---------------
[2022-08-08 01:01:01,987] lr: 9.265825e-04
[2022-08-08 01:01:02,345] loss: 0.000417  [    0/ 4818]
[2022-08-08 01:01:07,688] loss: 0.000237  [  480/ 4818]
[2022-08-08 01:01:13,030] loss: 0.000738  [  960/ 4818]
[2022-08-08 01:01:18,373] loss: 0.005419  [ 1440/ 4818]
[2022-08-08 01:01:23,712] loss: 0.003979  [ 1920/ 4818]
[2022-08-08 01:01:29,052] loss: 0.002546  [ 2400/ 4818]
[2022-08-08 01:01:34,396] loss: 0.000862  [ 2880/ 4818]
[2022-08-08 01:01:39,739] loss: 0.000531  [ 3360/ 4818]
[2022-08-08 01:01:45,082] loss: 0.000256  [ 3840/ 4818]
[2022-08-08 01:01:50,422] loss: 0.000355  [ 4320/ 4818]
[2022-08-08 01:01:55,620] loss: 0.328991  [ 4800/ 4818]
[2022-08-08 01:02:14,385] Train Error: Accuracy: 95.683%, Avg loss: 0.126011
[2022-08-08 01:02:22,044] Test  Error: Accuracy: 95.216%, Avg loss: 0.154270
[2022-08-08 01:02:22,044] Epoch 11---------------
[2022-08-08 01:02:22,045] lr: 6.470671e-04
[2022-08-08 01:02:22,404] loss: 0.308052  [    0/ 4818]
[2022-08-08 01:02:27,745] loss: 0.000800  [  480/ 4818]
[2022-08-08 01:02:33,088] loss: 0.002948  [  960/ 4818]
[2022-08-08 01:02:38,432] loss: 0.002275  [ 1440/ 4818]
[2022-08-08 01:02:43,775] loss: 0.000584  [ 1920/ 4818]
[2022-08-08 01:02:49,115] loss: 0.000993  [ 2400/ 4818]
[2022-08-08 01:02:54,459] loss: 0.001096  [ 2880/ 4818]
[2022-08-08 01:02:59,803] loss: 0.002264  [ 3360/ 4818]
[2022-08-08 01:03:05,145] loss: 0.001375  [ 3840/ 4818]
[2022-08-08 01:03:10,491] loss: 0.000376  [ 4320/ 4818]
[2022-08-08 01:03:15,689] loss: 0.001248  [ 4800/ 4818]
[2022-08-08 01:03:34,447] Train Error: Accuracy: 100.000%, Avg loss: 0.001078
[2022-08-08 01:03:42,102] Test  Error: Accuracy: 99.898%, Avg loss: 0.004156
[2022-08-08 01:03:42,103] Epoch 12---------------
[2022-08-08 01:03:42,104] lr: 6.147137e-04
[2022-08-08 01:03:42,461] loss: 0.000369  [    0/ 4818]
[2022-08-08 01:03:47,805] loss: 0.000599  [  480/ 4818]
[2022-08-08 01:03:53,148] loss: 0.000644  [  960/ 4818]
[2022-08-08 01:03:58,489] loss: 0.000522  [ 1440/ 4818]
[2022-08-08 01:04:03,833] loss: 0.000355  [ 1920/ 4818]
[2022-08-08 01:04:09,175] loss: 0.000523  [ 2400/ 4818]
[2022-08-08 01:04:14,516] loss: 0.000309  [ 2880/ 4818]
[2022-08-08 01:04:19,859] loss: 0.000346  [ 3360/ 4818]
[2022-08-08 01:04:25,201] loss: 0.000412  [ 3840/ 4818]
[2022-08-08 01:04:30,543] loss: 0.000454  [ 4320/ 4818]
[2022-08-08 01:04:35,742] loss: 0.000591  [ 4800/ 4818]
[2022-08-08 01:04:54,501] Train Error: Accuracy: 100.000%, Avg loss: 0.000677
[2022-08-08 01:05:02,158] Test  Error: Accuracy: 100.000%, Avg loss: 0.001527
[2022-08-08 01:05:02,158] Epoch 13---------------
[2022-08-08 01:05:02,159] lr: 5.839780e-04
[2022-08-08 01:05:02,518] loss: 0.000287  [    0/ 4818]
[2022-08-08 01:05:07,862] loss: 0.009487  [  480/ 4818]
[2022-08-08 01:05:13,204] loss: 0.000469  [  960/ 4818]
[2022-08-08 01:05:18,545] loss: 0.000225  [ 1440/ 4818]
[2022-08-08 01:05:23,887] loss: 0.000736  [ 1920/ 4818]
[2022-08-08 01:05:29,229] loss: 0.001329  [ 2400/ 4818]
[2022-08-08 01:05:34,572] loss: 0.000381  [ 2880/ 4818]
[2022-08-08 01:05:39,914] loss: 0.000558  [ 3360/ 4818]
[2022-08-08 01:05:45,256] loss: 0.003835  [ 3840/ 4818]
[2022-08-08 01:05:50,597] loss: 0.000871  [ 4320/ 4818]
[2022-08-08 01:05:55,796] loss: 0.000804  [ 4800/ 4818]
[2022-08-08 01:06:14,553] Train Error: Accuracy: 100.000%, Avg loss: 0.000749
[2022-08-08 01:06:22,214] Test  Error: Accuracy: 99.949%, Avg loss: 0.001991
[2022-08-08 01:06:22,214] Epoch 14---------------
[2022-08-08 01:06:22,215] lr: 4.078137e-04
[2022-08-08 01:06:22,574] loss: 0.000556  [    0/ 4818]
[2022-08-08 01:06:27,913] loss: 0.000406  [  480/ 4818]
[2022-08-08 01:06:33,259] loss: 0.000179  [  960/ 4818]
[2022-08-08 01:06:38,600] loss: 0.000591  [ 1440/ 4818]
[2022-08-08 01:06:43,943] loss: 0.003809  [ 1920/ 4818]
[2022-08-08 01:06:49,283] loss: 0.002357  [ 2400/ 4818]
[2022-08-08 01:06:54,626] loss: 0.001561  [ 2880/ 4818]
[2022-08-08 01:06:59,969] loss: 0.000303  [ 3360/ 4818]
[2022-08-08 01:07:05,311] loss: 0.000385  [ 3840/ 4818]
[2022-08-08 01:07:10,652] loss: 0.000367  [ 4320/ 4818]
[2022-08-08 01:07:15,851] loss: 0.000338  [ 4800/ 4818]
[2022-08-08 01:07:34,607] Train Error: Accuracy: 100.000%, Avg loss: 0.000640
[2022-08-08 01:07:42,263] Test  Error: Accuracy: 99.949%, Avg loss: 0.002337
[2022-08-08 01:07:42,264] Epoch 15---------------
[2022-08-08 01:07:42,264] lr: 3.155584e-04
[2022-08-08 01:07:42,622] loss: 0.000534  [    0/ 4818]
[2022-08-08 01:07:47,964] loss: 0.000392  [  480/ 4818]
[2022-08-08 01:07:53,305] loss: 0.000335  [  960/ 4818]
[2022-08-08 01:07:58,649] loss: 0.005159  [ 1440/ 4818]
[2022-08-08 01:08:03,992] loss: 0.000535  [ 1920/ 4818]
[2022-08-08 01:08:09,333] loss: 0.000756  [ 2400/ 4818]
[2022-08-08 01:08:14,676] loss: 0.000558  [ 2880/ 4818]
[2022-08-08 01:08:20,018] loss: 0.000762  [ 3360/ 4818]
[2022-08-08 01:08:25,361] loss: 0.000227  [ 3840/ 4818]
[2022-08-08 01:08:30,702] loss: 0.000475  [ 4320/ 4818]
[2022-08-08 01:08:35,900] loss: 0.000446  [ 4800/ 4818]
[2022-08-08 01:08:54,665] Train Error: Accuracy: 100.000%, Avg loss: 0.000682
[2022-08-08 01:09:02,324] Test  Error: Accuracy: 99.847%, Avg loss: 0.004227
[2022-08-08 01:09:02,325] Done!
[2022-08-08 01:09:02,329] Number of parameters:3186442
[2022-08-08 01:09:02,329] ## end time: 2022-08-08 01:09:02.325655
[2022-08-08 01:09:02,330] ## used time: 0:20:01.307621
