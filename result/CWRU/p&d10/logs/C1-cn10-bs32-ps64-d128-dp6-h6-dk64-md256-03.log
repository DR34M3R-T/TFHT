[2022-08-08 01:48:46,605] ## start time: 2022-08-08 01:48:46.463674
[2022-08-08 01:48:46,606] Using cuda device
[2022-08-08 01:48:46,607] In train:p&d10.npy.
[2022-08-08 01:48:46,608] One Channel
[2022-08-08 01:48:46,609] With Normal data.
[2022-08-08 01:48:46,609] Nunber of classes:10.
[2022-08-08 01:48:46,610] Nunber of ViT channels:1.
[2022-08-08 01:48:46,952] Totol epochs: 15
[2022-08-08 01:48:46,956] Epoch 1---------------
[2022-08-08 01:48:46,956] lr: 2.000000e-03
[2022-08-08 01:48:47,139] loss: 2.481065  [    0/ 4738]
[2022-08-08 01:48:49,866] loss: 1.880512  [  480/ 4738]
[2022-08-08 01:48:52,591] loss: 1.449063  [  960/ 4738]
[2022-08-08 01:48:55,318] loss: 0.784748  [ 1440/ 4738]
[2022-08-08 01:48:58,043] loss: 0.375952  [ 1920/ 4738]
[2022-08-08 01:49:00,770] loss: 0.175111  [ 2400/ 4738]
[2022-08-08 01:49:03,495] loss: 0.072706  [ 2880/ 4738]
[2022-08-08 01:49:06,218] loss: 0.068811  [ 3360/ 4738]
[2022-08-08 01:49:08,942] loss: 0.123219  [ 3840/ 4738]
[2022-08-08 01:49:11,667] loss: 0.106054  [ 4320/ 4738]
[2022-08-08 01:49:22,920] Train Error: Accuracy: 93.499%, Avg loss: 0.191976
[2022-08-08 01:49:26,821] Test  Error: Accuracy: 92.127%, Avg loss: 0.213559
[2022-08-08 01:49:26,822] Epoch 2---------------
[2022-08-08 01:49:26,822] lr: 1.900000e-03
[2022-08-08 01:49:27,006] loss: 0.289349  [    0/ 4738]
[2022-08-08 01:49:29,731] loss: 0.025940  [  480/ 4738]
[2022-08-08 01:49:32,456] loss: 0.009764  [  960/ 4738]
[2022-08-08 01:49:35,181] loss: 0.064110  [ 1440/ 4738]
[2022-08-08 01:49:37,909] loss: 0.065549  [ 1920/ 4738]
[2022-08-08 01:49:40,633] loss: 0.018274  [ 2400/ 4738]
[2022-08-08 01:49:43,356] loss: 0.010392  [ 2880/ 4738]
[2022-08-08 01:49:46,081] loss: 0.008964  [ 3360/ 4738]
[2022-08-08 01:49:48,807] loss: 0.005105  [ 3840/ 4738]
[2022-08-08 01:49:51,533] loss: 0.006759  [ 4320/ 4738]
[2022-08-08 01:50:02,803] Train Error: Accuracy: 99.536%, Avg loss: 0.020719
[2022-08-08 01:50:06,707] Test  Error: Accuracy: 99.315%, Avg loss: 0.030820
[2022-08-08 01:50:06,708] Epoch 3---------------
[2022-08-08 01:50:06,709] lr: 1.805000e-03
[2022-08-08 01:50:06,892] loss: 0.004852  [    0/ 4738]
[2022-08-08 01:50:09,620] loss: 0.004721  [  480/ 4738]
[2022-08-08 01:50:12,344] loss: 0.128663  [  960/ 4738]
[2022-08-08 01:50:15,069] loss: 0.898772  [ 1440/ 4738]
[2022-08-08 01:50:17,795] loss: 0.046041  [ 1920/ 4738]
[2022-08-08 01:50:20,521] loss: 0.037191  [ 2400/ 4738]
[2022-08-08 01:50:23,244] loss: 0.041731  [ 2880/ 4738]
[2022-08-08 01:50:25,971] loss: 0.098216  [ 3360/ 4738]
[2022-08-08 01:50:28,697] loss: 0.034362  [ 3840/ 4738]
[2022-08-08 01:50:31,423] loss: 0.007345  [ 4320/ 4738]
[2022-08-08 01:50:42,688] Train Error: Accuracy: 99.768%, Avg loss: 0.012816
[2022-08-08 01:50:46,588] Test  Error: Accuracy: 99.413%, Avg loss: 0.018708
[2022-08-08 01:50:46,588] Epoch 4---------------
[2022-08-08 01:50:46,590] lr: 1.714750e-03
[2022-08-08 01:50:46,774] loss: 0.004102  [    0/ 4738]
[2022-08-08 01:50:49,501] loss: 0.001597  [  480/ 4738]
[2022-08-08 01:50:52,227] loss: 0.001795  [  960/ 4738]
[2022-08-08 01:50:54,951] loss: 0.002430  [ 1440/ 4738]
[2022-08-08 01:50:57,678] loss: 0.001967  [ 1920/ 4738]
[2022-08-08 01:51:00,405] loss: 0.003340  [ 2400/ 4738]
[2022-08-08 01:51:03,131] loss: 0.005064  [ 2880/ 4738]
[2022-08-08 01:51:05,856] loss: 0.009393  [ 3360/ 4738]
[2022-08-08 01:51:08,581] loss: 0.005470  [ 3840/ 4738]
[2022-08-08 01:51:11,306] loss: 0.003512  [ 4320/ 4738]
[2022-08-08 01:51:22,569] Train Error: Accuracy: 99.515%, Avg loss: 0.015121
[2022-08-08 01:51:26,472] Test  Error: Accuracy: 99.364%, Avg loss: 0.023265
[2022-08-08 01:51:26,472] Epoch 5---------------
[2022-08-08 01:51:26,474] lr: 1.326841e-03
[2022-08-08 01:51:26,656] loss: 0.003345  [    0/ 4738]
[2022-08-08 01:51:29,381] loss: 0.002088  [  480/ 4738]
[2022-08-08 01:51:32,106] loss: 0.023954  [  960/ 4738]
[2022-08-08 01:51:34,834] loss: 0.003117  [ 1440/ 4738]
[2022-08-08 01:51:37,561] loss: 0.001538  [ 1920/ 4738]
[2022-08-08 01:51:40,285] loss: 0.010685  [ 2400/ 4738]
[2022-08-08 01:51:43,010] loss: 0.015760  [ 2880/ 4738]
[2022-08-08 01:51:45,736] loss: 0.001279  [ 3360/ 4738]
[2022-08-08 01:51:48,462] loss: 0.008544  [ 3840/ 4738]
[2022-08-08 01:51:51,189] loss: 0.004000  [ 4320/ 4738]
[2022-08-08 01:52:02,459] Train Error: Accuracy: 99.894%, Avg loss: 0.005185
[2022-08-08 01:52:06,362] Test  Error: Accuracy: 99.853%, Avg loss: 0.011501
[2022-08-08 01:52:06,363] Epoch 6---------------
[2022-08-08 01:52:06,364] lr: 1.260499e-03
[2022-08-08 01:52:06,548] loss: 0.001564  [    0/ 4738]
[2022-08-08 01:52:09,273] loss: 0.032119  [  480/ 4738]
[2022-08-08 01:52:11,998] loss: 0.032446  [  960/ 4738]
[2022-08-08 01:52:14,723] loss: 0.004922  [ 1440/ 4738]
[2022-08-08 01:52:17,449] loss: 0.005566  [ 1920/ 4738]
[2022-08-08 01:52:20,174] loss: 0.001085  [ 2400/ 4738]
[2022-08-08 01:52:22,901] loss: 0.001871  [ 2880/ 4738]
[2022-08-08 01:52:25,625] loss: 0.001083  [ 3360/ 4738]
[2022-08-08 01:52:28,349] loss: 0.000870  [ 3840/ 4738]
[2022-08-08 01:52:31,073] loss: 0.001535  [ 4320/ 4738]
[2022-08-08 01:52:42,314] Train Error: Accuracy: 99.979%, Avg loss: 0.001574
[2022-08-08 01:52:46,207] Test  Error: Accuracy: 99.853%, Avg loss: 0.004833
[2022-08-08 01:52:46,207] Epoch 7---------------
[2022-08-08 01:52:46,208] lr: 1.197474e-03
[2022-08-08 01:52:46,393] loss: 0.000736  [    0/ 4738]
[2022-08-08 01:52:49,117] loss: 0.002006  [  480/ 4738]
[2022-08-08 01:52:51,843] loss: 0.001436  [  960/ 4738]
[2022-08-08 01:52:54,570] loss: 0.000247  [ 1440/ 4738]
[2022-08-08 01:52:57,293] loss: 0.000503  [ 1920/ 4738]
[2022-08-08 01:53:00,020] loss: 0.000890  [ 2400/ 4738]
[2022-08-08 01:53:02,746] loss: 0.000660  [ 2880/ 4738]
[2022-08-08 01:53:05,468] loss: 0.000987  [ 3360/ 4738]
[2022-08-08 01:53:08,196] loss: 0.001161  [ 3840/ 4738]
[2022-08-08 01:53:10,919] loss: 0.001516  [ 4320/ 4738]
[2022-08-08 01:53:22,166] Train Error: Accuracy: 99.958%, Avg loss: 0.002190
[2022-08-08 01:53:26,060] Test  Error: Accuracy: 99.756%, Avg loss: 0.009874
[2022-08-08 01:53:26,061] Epoch 8---------------
[2022-08-08 01:53:26,062] lr: 8.362407e-04
[2022-08-08 01:53:26,246] loss: 0.001733  [    0/ 4738]
[2022-08-08 01:53:28,970] loss: 0.000392  [  480/ 4738]
[2022-08-08 01:53:31,696] loss: 0.003102  [  960/ 4738]
[2022-08-08 01:53:34,421] loss: 0.000389  [ 1440/ 4738]
[2022-08-08 01:53:37,146] loss: 0.000471  [ 1920/ 4738]
[2022-08-08 01:53:39,871] loss: 0.002957  [ 2400/ 4738]
[2022-08-08 01:53:42,596] loss: 0.008056  [ 2880/ 4738]
[2022-08-08 01:53:45,320] loss: 0.000749  [ 3360/ 4738]
[2022-08-08 01:53:48,045] loss: 0.000955  [ 3840/ 4738]
[2022-08-08 01:53:50,771] loss: 0.000357  [ 4320/ 4738]
[2022-08-08 01:54:02,016] Train Error: Accuracy: 100.000%, Avg loss: 0.000695
[2022-08-08 01:54:05,909] Test  Error: Accuracy: 99.951%, Avg loss: 0.003867
[2022-08-08 01:54:05,910] Epoch 9---------------
[2022-08-08 01:54:05,911] lr: 7.944286e-04
[2022-08-08 01:54:06,094] loss: 0.000493  [    0/ 4738]
[2022-08-08 01:54:08,822] loss: 0.000418  [  480/ 4738]
[2022-08-08 01:54:11,546] loss: 0.000351  [  960/ 4738]
[2022-08-08 01:54:14,269] loss: 0.001228  [ 1440/ 4738]
[2022-08-08 01:54:16,995] loss: 0.000691  [ 1920/ 4738]
[2022-08-08 01:54:19,723] loss: 0.000417  [ 2400/ 4738]
[2022-08-08 01:54:22,448] loss: 0.001118  [ 2880/ 4738]
[2022-08-08 01:54:25,174] loss: 0.000352  [ 3360/ 4738]
[2022-08-08 01:54:27,898] loss: 0.000390  [ 3840/ 4738]
[2022-08-08 01:54:30,624] loss: 0.000316  [ 4320/ 4738]
[2022-08-08 01:54:41,869] Train Error: Accuracy: 100.000%, Avg loss: 0.000670
[2022-08-08 01:54:45,760] Test  Error: Accuracy: 99.902%, Avg loss: 0.002840
[2022-08-08 01:54:45,761] Epoch 10---------------
[2022-08-08 01:54:45,762] lr: 7.547072e-04
[2022-08-08 01:54:45,945] loss: 0.000413  [    0/ 4738]
[2022-08-08 01:54:48,669] loss: 0.000409  [  480/ 4738]
[2022-08-08 01:54:51,393] loss: 0.000558  [  960/ 4738]
[2022-08-08 01:54:54,119] loss: 0.000199  [ 1440/ 4738]
[2022-08-08 01:54:56,845] loss: 0.000302  [ 1920/ 4738]
[2022-08-08 01:54:59,568] loss: 0.000184  [ 2400/ 4738]
[2022-08-08 01:55:02,293] loss: 0.000226  [ 2880/ 4738]
[2022-08-08 01:55:05,016] loss: 0.000330  [ 3360/ 4738]
[2022-08-08 01:55:07,740] loss: 0.000316  [ 3840/ 4738]
[2022-08-08 01:55:10,462] loss: 0.000298  [ 4320/ 4738]
[2022-08-08 01:55:21,707] Train Error: Accuracy: 100.000%, Avg loss: 0.000540
[2022-08-08 01:55:25,596] Test  Error: Accuracy: 99.951%, Avg loss: 0.001863
[2022-08-08 01:55:25,596] Epoch 11---------------
[2022-08-08 01:55:25,597] lr: 7.169718e-04
[2022-08-08 01:55:25,781] loss: 0.000368  [    0/ 4738]
[2022-08-08 01:55:28,507] loss: 0.000367  [  480/ 4738]
[2022-08-08 01:55:31,231] loss: 0.000220  [  960/ 4738]
[2022-08-08 01:55:33,957] loss: 0.000805  [ 1440/ 4738]
[2022-08-08 01:55:36,683] loss: 0.000212  [ 1920/ 4738]
[2022-08-08 01:55:39,410] loss: 0.000203  [ 2400/ 4738]
[2022-08-08 01:55:42,134] loss: 0.000318  [ 2880/ 4738]
[2022-08-08 01:55:44,861] loss: 0.000271  [ 3360/ 4738]
[2022-08-08 01:55:47,586] loss: 0.000551  [ 3840/ 4738]
[2022-08-08 01:55:50,312] loss: 0.000183  [ 4320/ 4738]
[2022-08-08 01:56:01,556] Train Error: Accuracy: 100.000%, Avg loss: 0.000445
[2022-08-08 01:56:05,449] Test  Error: Accuracy: 99.951%, Avg loss: 0.004778
[2022-08-08 01:56:05,450] Epoch 12---------------
[2022-08-08 01:56:05,450] lr: 5.006882e-04
[2022-08-08 01:56:05,634] loss: 0.000548  [    0/ 4738]
[2022-08-08 01:56:08,359] loss: 0.000367  [  480/ 4738]
[2022-08-08 01:56:11,084] loss: 0.000468  [  960/ 4738]
[2022-08-08 01:56:13,808] loss: 0.000289  [ 1440/ 4738]
[2022-08-08 01:56:16,534] loss: 0.000344  [ 1920/ 4738]
[2022-08-08 01:56:19,257] loss: 0.000336  [ 2400/ 4738]
[2022-08-08 01:56:21,982] loss: 0.000278  [ 2880/ 4738]
[2022-08-08 01:56:24,709] loss: 0.000920  [ 3360/ 4738]
[2022-08-08 01:56:27,434] loss: 0.000194  [ 3840/ 4738]
[2022-08-08 01:56:30,161] loss: 0.000261  [ 4320/ 4738]
[2022-08-08 01:56:41,407] Train Error: Accuracy: 100.000%, Avg loss: 0.000361
[2022-08-08 01:56:45,299] Test  Error: Accuracy: 99.951%, Avg loss: 0.002041
[2022-08-08 01:56:45,299] Epoch 13---------------
[2022-08-08 01:56:45,300] lr: 4.756538e-04
[2022-08-08 01:56:45,483] loss: 0.000682  [    0/ 4738]
[2022-08-08 01:56:48,209] loss: 0.000424  [  480/ 4738]
[2022-08-08 01:56:50,935] loss: 0.000340  [  960/ 4738]
[2022-08-08 01:56:53,662] loss: 0.000210  [ 1440/ 4738]
[2022-08-08 01:56:56,387] loss: 0.000248  [ 1920/ 4738]
[2022-08-08 01:56:59,113] loss: 0.000311  [ 2400/ 4738]
[2022-08-08 01:57:01,837] loss: 0.000333  [ 2880/ 4738]
[2022-08-08 01:57:04,559] loss: 0.000347  [ 3360/ 4738]
[2022-08-08 01:57:07,284] loss: 0.000299  [ 3840/ 4738]
[2022-08-08 01:57:10,011] loss: 0.000257  [ 4320/ 4738]
[2022-08-08 01:57:21,258] Train Error: Accuracy: 100.000%, Avg loss: 0.000323
[2022-08-08 01:57:25,150] Test  Error: Accuracy: 99.951%, Avg loss: 0.001659
[2022-08-08 01:57:25,150] Epoch 14---------------
[2022-08-08 01:57:25,152] lr: 4.518711e-04
[2022-08-08 01:57:25,335] loss: 0.000266  [    0/ 4738]
[2022-08-08 01:57:28,061] loss: 0.000584  [  480/ 4738]
[2022-08-08 01:57:30,783] loss: 0.000417  [  960/ 4738]
[2022-08-08 01:57:33,508] loss: 0.000319  [ 1440/ 4738]
[2022-08-08 01:57:36,234] loss: 0.000285  [ 1920/ 4738]
[2022-08-08 01:57:38,960] loss: 0.000399  [ 2400/ 4738]
[2022-08-08 01:57:41,684] loss: 0.000249  [ 2880/ 4738]
[2022-08-08 01:57:44,410] loss: 0.000299  [ 3360/ 4738]
[2022-08-08 01:57:47,134] loss: 0.001130  [ 3840/ 4738]
[2022-08-08 01:57:49,861] loss: 0.000365  [ 4320/ 4738]
[2022-08-08 01:58:01,103] Train Error: Accuracy: 100.000%, Avg loss: 0.000377
[2022-08-08 01:58:04,995] Test  Error: Accuracy: 100.000%, Avg loss: 0.000992
[2022-08-08 01:58:04,996] Epoch 15---------------
[2022-08-08 01:58:04,996] lr: 4.292775e-04
[2022-08-08 01:58:05,180] loss: 0.000324  [    0/ 4738]
[2022-08-08 01:58:07,906] loss: 0.000254  [  480/ 4738]
[2022-08-08 01:58:10,632] loss: 0.000261  [  960/ 4738]
[2022-08-08 01:58:13,356] loss: 0.000233  [ 1440/ 4738]
[2022-08-08 01:58:16,082] loss: 0.000311  [ 1920/ 4738]
[2022-08-08 01:58:18,807] loss: 0.000512  [ 2400/ 4738]
[2022-08-08 01:58:21,534] loss: 0.000648  [ 2880/ 4738]
[2022-08-08 01:58:24,258] loss: 0.000297  [ 3360/ 4738]
[2022-08-08 01:58:26,984] loss: 0.000517  [ 3840/ 4738]
[2022-08-08 01:58:29,709] loss: 0.000418  [ 4320/ 4738]
[2022-08-08 01:58:40,958] Train Error: Accuracy: 99.979%, Avg loss: 0.001126
[2022-08-08 01:58:44,852] Test  Error: Accuracy: 99.853%, Avg loss: 0.004769
[2022-08-08 01:58:44,852] Done!
[2022-08-08 01:58:44,857] Number of parameters:3186442
[2022-08-08 01:58:44,857] ## end time: 2022-08-08 01:58:44.852012
[2022-08-08 01:58:44,858] ## used time: 0:09:58.388338
