[2022-08-06 15:22:47,610] ## start time: 2022-08-06 15:22:47.486832
[2022-08-06 15:22:47,611] Using cuda device
[2022-08-06 15:22:47,612] In train:p&d10.npy.
[2022-08-06 15:22:47,613] One Channel
[2022-08-06 15:22:47,613] With Normal data.
[2022-08-06 15:22:47,614] Nunber of classes:10.
[2022-08-06 15:22:47,614] Nunber of ViT channels:1.
[2022-08-06 15:22:47,801] Totol epochs: 10
[2022-08-06 15:22:47,802] Epoch 1---------------
[2022-08-06 15:22:47,803] lr: 2.000000e-03
[2022-08-06 15:22:47,817] loss: 2.253568  [    0/ 4772]
[2022-08-06 15:22:47,977] loss: 1.953463  [  960/ 4772]
[2022-08-06 15:22:48,139] loss: 1.370282  [ 1920/ 4772]
[2022-08-06 15:22:48,293] loss: 0.910451  [ 2880/ 4772]
[2022-08-06 15:22:48,445] loss: 0.525398  [ 3840/ 4772]
[2022-08-06 15:22:48,863] Train Error: Accuracy: 92.645%, Avg loss: 0.375176
[2022-08-06 15:22:48,981] Test  Error: Accuracy: 91.944%, Avg loss: 0.379704
[2022-08-06 15:22:48,982] Epoch 2---------------
[2022-08-06 15:22:48,983] lr: 1.900000e-03
[2022-08-06 15:22:48,995] loss: 0.277055  [    0/ 4772]
[2022-08-06 15:22:49,146] loss: 0.295781  [  960/ 4772]
[2022-08-06 15:22:49,294] loss: 0.182165  [ 1920/ 4772]
[2022-08-06 15:22:49,444] loss: 0.164642  [ 2880/ 4772]
[2022-08-06 15:22:49,595] loss: 0.151449  [ 3840/ 4772]
[2022-08-06 15:22:50,016] Train Error: Accuracy: 96.291%, Avg loss: 0.155277
[2022-08-06 15:22:50,135] Test  Error: Accuracy: 95.624%, Avg loss: 0.185946
[2022-08-06 15:22:50,135] Epoch 3---------------
[2022-08-06 15:22:50,136] lr: 1.805000e-03
[2022-08-06 15:22:50,149] loss: 0.213098  [    0/ 4772]
[2022-08-06 15:22:50,299] loss: 0.094041  [  960/ 4772]
[2022-08-06 15:22:50,449] loss: 0.094288  [ 1920/ 4772]
[2022-08-06 15:22:50,599] loss: 0.067155  [ 2880/ 4772]
[2022-08-06 15:22:50,752] loss: 0.049228  [ 3840/ 4772]
[2022-08-06 15:22:51,181] Train Error: Accuracy: 92.666%, Avg loss: 0.198602
[2022-08-06 15:22:51,302] Test  Error: Accuracy: 90.701%, Avg loss: 0.229916
[2022-08-06 15:22:51,302] Epoch 4---------------
[2022-08-06 15:22:51,303] lr: 1.396675e-03
[2022-08-06 15:22:51,315] loss: 0.068106  [    0/ 4772]
[2022-08-06 15:22:51,465] loss: 0.036424  [  960/ 4772]
[2022-08-06 15:22:51,617] loss: 0.032019  [ 1920/ 4772]
[2022-08-06 15:22:51,767] loss: 0.018884  [ 2880/ 4772]
[2022-08-06 15:22:51,919] loss: 0.033772  [ 3840/ 4772]
[2022-08-06 15:22:52,342] Train Error: Accuracy: 99.015%, Avg loss: 0.048561
[2022-08-06 15:22:52,463] Test  Error: Accuracy: 97.911%, Avg loss: 0.074226
[2022-08-06 15:22:52,463] Epoch 5---------------
[2022-08-06 15:22:52,464] lr: 1.326841e-03
[2022-08-06 15:22:52,475] loss: 0.055042  [    0/ 4772]
[2022-08-06 15:22:52,626] loss: 0.027541  [  960/ 4772]
[2022-08-06 15:22:52,777] loss: 0.013676  [ 1920/ 4772]
[2022-08-06 15:22:52,928] loss: 0.017687  [ 2880/ 4772]
[2022-08-06 15:22:53,081] loss: 0.020242  [ 3840/ 4772]
[2022-08-06 15:22:53,500] Train Error: Accuracy: 99.769%, Avg loss: 0.018505
[2022-08-06 15:22:53,620] Test  Error: Accuracy: 99.503%, Avg loss: 0.026803
[2022-08-06 15:22:53,620] Epoch 6---------------
[2022-08-06 15:22:53,622] lr: 1.260499e-03
[2022-08-06 15:22:53,633] loss: 0.011766  [    0/ 4772]
[2022-08-06 15:22:53,785] loss: 0.038818  [  960/ 4772]
[2022-08-06 15:22:53,938] loss: 0.012093  [ 1920/ 4772]
[2022-08-06 15:22:54,089] loss: 0.019133  [ 2880/ 4772]
[2022-08-06 15:22:54,240] loss: 0.024162  [ 3840/ 4772]
[2022-08-06 15:22:54,658] Train Error: Accuracy: 99.665%, Avg loss: 0.021167
[2022-08-06 15:22:54,776] Test  Error: Accuracy: 99.005%, Avg loss: 0.039145
[2022-08-06 15:22:54,777] Epoch 7---------------
[2022-08-06 15:22:54,778] lr: 8.802533e-04
[2022-08-06 15:22:54,789] loss: 0.024672  [    0/ 4772]
[2022-08-06 15:22:54,940] loss: 0.013381  [  960/ 4772]
[2022-08-06 15:22:55,094] loss: 0.017416  [ 1920/ 4772]
[2022-08-06 15:22:55,247] loss: 0.019583  [ 2880/ 4772]
[2022-08-06 15:22:55,397] loss: 0.014450  [ 3840/ 4772]
[2022-08-06 15:22:55,812] Train Error: Accuracy: 99.874%, Avg loss: 0.011645
[2022-08-06 15:22:55,931] Test  Error: Accuracy: 99.503%, Avg loss: 0.022055
[2022-08-06 15:22:55,931] Epoch 8---------------
[2022-08-06 15:22:55,932] lr: 8.362407e-04
[2022-08-06 15:22:55,944] loss: 0.005803  [    0/ 4772]
[2022-08-06 15:22:56,096] loss: 0.014418  [  960/ 4772]
[2022-08-06 15:22:56,245] loss: 0.031977  [ 1920/ 4772]
[2022-08-06 15:22:56,399] loss: 0.015637  [ 2880/ 4772]
[2022-08-06 15:22:56,549] loss: 0.014340  [ 3840/ 4772]
[2022-08-06 15:22:56,968] Train Error: Accuracy: 98.596%, Avg loss: 0.050835
[2022-08-06 15:22:57,087] Test  Error: Accuracy: 97.215%, Avg loss: 0.090690
[2022-08-06 15:22:57,087] Epoch 9---------------
[2022-08-06 15:22:57,088] lr: 5.839780e-04
[2022-08-06 15:22:57,100] loss: 0.035735  [    0/ 4772]
[2022-08-06 15:22:57,252] loss: 0.007621  [  960/ 4772]
[2022-08-06 15:22:57,403] loss: 0.010342  [ 1920/ 4772]
[2022-08-06 15:22:57,553] loss: 0.004683  [ 2880/ 4772]
[2022-08-06 15:22:57,702] loss: 0.008448  [ 3840/ 4772]
[2022-08-06 15:22:58,118] Train Error: Accuracy: 99.916%, Avg loss: 0.010539
[2022-08-06 15:22:58,236] Test  Error: Accuracy: 99.254%, Avg loss: 0.025754
[2022-08-06 15:22:58,236] Epoch 10---------------
[2022-08-06 15:22:58,237] lr: 5.547791e-04
[2022-08-06 15:22:58,249] loss: 0.005369  [    0/ 4772]
[2022-08-06 15:22:58,400] loss: 0.008672  [  960/ 4772]
[2022-08-06 15:22:58,553] loss: 0.005426  [ 1920/ 4772]
[2022-08-06 15:22:58,704] loss: 0.018662  [ 2880/ 4772]
[2022-08-06 15:22:58,855] loss: 0.005808  [ 3840/ 4772]
[2022-08-06 15:22:59,273] Train Error: Accuracy: 99.769%, Avg loss: 0.012159
[2022-08-06 15:22:59,394] Test  Error: Accuracy: 99.503%, Avg loss: 0.021767
[2022-08-06 15:22:59,395] Done!
[2022-08-06 15:22:59,398] Number of parameters:92106
[2022-08-06 15:22:59,398] ## end time: 2022-08-06 15:22:59.394909
[2022-08-06 15:22:59,398] ## used time: 0:00:11.908077
